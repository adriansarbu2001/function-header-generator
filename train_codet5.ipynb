{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import ast\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarbu\\miniconda3\\envs\\machine_learning_py38\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for code_search_net contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/code_search_net\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 8.44k/8.44k [00:00<00:00, 8.42MB/s]\n",
      "Downloading readme: 100%|██████████| 12.9k/12.9k [00:00<00:00, 12.9MB/s]\n",
      "Downloading data: 100%|██████████| 941M/941M [00:31<00:00, 30.2MB/s] \n",
      "Generating train split: 100%|██████████| 412178/412178 [05:08<00:00, 1336.65 examples/s]\n",
      "Generating test split: 100%|██████████| 22176/22176 [00:16<00:00, 1365.78 examples/s]\n",
      "Generating validation split: 100%|██████████| 23107/23107 [00:17<00:00, 1303.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load CodeSearchNet dataset\n",
    "dataset = load_dataset(\"code_search_net\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 412178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 22176\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 23107\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# View the structure of the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function to extract function headers and docstrings\n",
    "def preprocess_function(sample):\n",
    "    # Extract function and docstring to form the header-comment pair\n",
    "    # Adjusted for the actual dataset keys: 'func_documentation_string' and 'func_code_string'\n",
    "    docstring = sample.get('func_documentation_string')\n",
    "    code_string = sample.get('func_code_string')\n",
    "    \n",
    "    if docstring and code_string:\n",
    "        function_header = code_string.split(')')[0] + ')'\n",
    "        if \"def \" in function_header:\n",
    "            # print({\n",
    "            #     \"input\": docstring,\n",
    "            #     \"output\": function_header\n",
    "            # })\n",
    "            return {\n",
    "                \"input\": docstring,\n",
    "                \"output\": function_header\n",
    "            }\n",
    "    return {\n",
    "        \"input\": \"\",\n",
    "        \"output\": \"\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 390/412178 [00:00<01:50, 3714.07 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Trains a k-nearest neighbors classifier for face recognition.\\n\\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\\n\\n     (View in source code to see train_dir example tree structure)\\n\\n     Structure:\\n        <train_dir>/\\n        ├── <person1>/\\n        │   ├── <somename1>.jpeg\\n        │   ├── <somename2>.jpeg\\n        │   ├── ...\\n        ├── <person2>/\\n        │   ├── <somename1>.jpeg\\n        │   └── <somename2>.jpeg\\n        └── ...\\n\\n    :param model_save_path: (optional) path to save model on disk\\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\\n    :param verbose: verbosity of training\\n    :return: returns knn classifier that was trained on the given data.', 'output': \"def train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False)\"}\n",
      "{'input': \"Recognizes faces in given image using a trained KNN classifier\\n\\n    :param X_img_path: path to image to be recognized\\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\\n           of mis-classifying an unknown person as a known one.\\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\\n        For faces of unrecognized persons, the name 'unknown' will be returned.\", 'output': 'def predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6)'}\n",
      "{'input': 'Shows the face recognition results visually.\\n\\n    :param img_path: path to image to be recognized\\n    :param predictions: results of the predict function\\n    :return:', 'output': 'def show_prediction_labels_on_image(img_path, predictions)'}\n",
      "{'input': \"Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\\n\\n    :param rect: a dlib 'rect' object\\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\", 'output': 'def _rect_to_css(rect)'}\n",
      "{'input': 'Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\\n\\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\\n    :param image_shape: numpy shape of the image array\\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order', 'output': 'def _trim_css_to_bounds(css, image_shape)'}\n",
      "{'input': \"Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\\n    for each comparison face. The distance tells you how similar the faces are.\\n\\n    :param faces: List of face encodings to compare\\n    :param face_to_compare: A face encoding to compare against\\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\", 'output': 'def face_distance(face_encodings, face_to_compare)'}\n",
      "{'input': \"Loads an image file (.jpg, .png, etc) into a numpy array\\n\\n    :param file: image file name or file object to load\\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\\n    :return: image contents as numpy array\", 'output': \"def load_image_file(file, mode='RGB')\"}\n",
      "{'input': 'Returns an array of bounding boxes of human faces in a image\\n\\n    :param img: An image (as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\\n    :return: A list of dlib \\'rect\\' objects of found face locations', 'output': 'def _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\")'}\n",
      "{'input': 'Returns an array of bounding boxes of human faces in a image\\n\\n    :param img: An image (as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order', 'output': 'def face_locations(img, number_of_times_to_upsample=1, model=\"hog\")'}\n",
      "{'input': \"Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\\n    If you are using a GPU, this can give you much faster results since the GPU\\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\\n\\n    :param img: A list of images (each as a numpy array)\\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\\n    :param batch_size: How many images to include in each GPU processing batch.\\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\", 'output': 'def batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128)'}\n",
      "{'input': 'Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\\n\\n    :param face_image: image to search\\n    :param face_locations: Optionally provide a list of face locations to check.\\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\\n    :return: A list of dicts of face feature locations (eyes, nose, etc)', 'output': 'def face_landmarks(face_image, face_locations=None, model=\"large\")'}\n",
      "{'input': 'Given an image, return the 128-dimension face encoding for each face in the image.\\n\\n    :param face_image: The image that contains one or more faces\\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\\n    :return: A list of 128-dimensional face encodings (one for each face in the image)', 'output': 'def face_encodings(face_image, known_face_locations=None, num_jitters=1)'}\n",
      "{'input': 'Parses the given data type string to a :class:`DataType`. The data type string format equals\\n    to :class:`DataType.simpleString`, except that top level struct type can omit\\n    the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use ``byte`` instead\\n    of ``tinyint`` for :class:`ByteType`. We can also use ``int`` as a short name\\n    for :class:`IntegerType`. Since Spark 2.3, this also supports a schema in a DDL-formatted\\n    string and case-insensitive strings.\\n\\n    >>> _parse_datatype_string(\"int \")\\n    IntegerType\\n    >>> _parse_datatype_string(\"INT \")\\n    IntegerType\\n    >>> _parse_datatype_string(\"a: byte, b: decimal(  16 , 8   ) \")\\n    StructType(List(StructField(a,ByteType,true),StructField(b,DecimalType(16,8),true)))\\n    >>> _parse_datatype_string(\"a DOUBLE, b STRING\")\\n    StructType(List(StructField(a,DoubleType,true),StructField(b,StringType,true)))\\n    >>> _parse_datatype_string(\"a: array< short>\")\\n    StructType(List(StructField(a,ArrayType(ShortType,true),true)))\\n    >>> _parse_datatype_string(\" map<string , string > \")\\n    MapType(StringType,StringType,true)\\n\\n    >>> # Error cases\\n    >>> _parse_datatype_string(\"blabla\") # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ParseException:...\\n    >>> _parse_datatype_string(\"a: int,\") # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ParseException:...\\n    >>> _parse_datatype_string(\"array<int\") # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ParseException:...\\n    >>> _parse_datatype_string(\"map<int, boolean>>\") # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ParseException:...', 'output': 'def _parse_datatype_string(s)'}\n",
      "{'input': 'Return the Catalyst datatype from the size of integers.', 'output': 'def _int_size_to_type(size)'}\n",
      "{'input': 'Infer the DataType from obj', 'output': 'def _infer_type(obj)'}\n",
      "{'input': 'Infer the schema from dict/namedtuple/object', 'output': 'def _infer_schema(row, names=None)'}\n",
      "{'input': 'Return whether there is NullType in `dt` or not', 'output': 'def _has_nulltype(dt)'}\n",
      "{'input': 'Create a converter to drop the names of fields in obj', 'output': 'def _create_converter(dataType)'}\n",
      "{'input': 'Make a verifier that checks the type of obj against dataType and raises a TypeError if they do\\n    not match.\\n\\n    This verifier also checks the value of obj against datatype and raises a ValueError if it\\'s not\\n    within the allowed range, e.g. using 128 as ByteType will overflow. Note that, Python float is\\n    not checked, so it will become infinity when cast to Java float if it overflows.\\n\\n    >>> _make_type_verifier(StructType([]))(None)\\n    >>> _make_type_verifier(StringType())(\"\")\\n    >>> _make_type_verifier(LongType())(0)\\n    >>> _make_type_verifier(ArrayType(ShortType()))(list(range(3)))\\n    >>> _make_type_verifier(ArrayType(StringType()))(set()) # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    TypeError:...\\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({})\\n    >>> _make_type_verifier(StructType([]))(())\\n    >>> _make_type_verifier(StructType([]))([])\\n    >>> _make_type_verifier(StructType([]))([1]) # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ValueError:...\\n    >>> # Check if numeric values are within the allowed range.\\n    >>> _make_type_verifier(ByteType())(12)\\n    >>> _make_type_verifier(ByteType())(1234) # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ValueError:...\\n    >>> _make_type_verifier(ByteType(), False)(None) # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ValueError:...\\n    >>> _make_type_verifier(\\n    ...     ArrayType(ShortType(), False))([1, None]) # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ValueError:...\\n    >>> _make_type_verifier(MapType(StringType(), IntegerType()))({None: 1})\\n    Traceback (most recent call last):\\n        ...\\n    ValueError:...\\n    >>> schema = StructType().add(\"a\", IntegerType()).add(\"b\", StringType(), False)\\n    >>> _make_type_verifier(schema)((1, None)) # doctest: +IGNORE_EXCEPTION_DETAIL\\n    Traceback (most recent call last):\\n        ...\\n    ValueError:...', 'output': 'def _make_type_verifier(dataType, nullable=True, name=None)'}\n",
      "{'input': 'Convert Spark data type to pyarrow type', 'output': 'def to_arrow_type(dt)'}\n",
      "{'input': 'Convert a schema from Spark to Arrow', 'output': 'def to_arrow_schema(schema)'}\n",
      "{'input': 'Convert pyarrow type to Spark data type.', 'output': 'def from_arrow_type(at)'}\n",
      "{'input': 'Convert schema from Arrow to Spark.', 'output': 'def from_arrow_schema(arrow_schema)'}\n",
      "{'input': 'Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone.\\n\\n    If the input series is not a timestamp series, then the same series is returned. If the input\\n    series is a timestamp series, then a converted series is returned.\\n\\n    :param s: pandas.Series\\n    :param timezone: the timezone to convert. if None then use local timezone\\n    :return pandas.Series that have been converted to tz-naive', 'output': 'def _check_series_localize_timestamps(s, timezone)'}\n",
      "{'input': 'Convert timezone aware timestamps to timezone-naive in the specified timezone or local timezone\\n\\n    :param pdf: pandas.DataFrame\\n    :param timezone: the timezone to convert. if None then use local timezone\\n    :return pandas.DataFrame where any timezone aware columns have been converted to tz-naive', 'output': 'def _check_dataframe_localize_timestamps(pdf, timezone)'}\n",
      "{'input': 'Convert a tz-naive timestamp in the specified timezone or local timezone to UTC normalized for\\n    Spark internal storage\\n\\n    :param s: a pandas.Series\\n    :param timezone: the timezone to convert. if None then use local timezone\\n    :return pandas.Series where if it is a timestamp, has been UTC normalized without a time zone', 'output': 'def _check_series_convert_timestamps_internal(s, timezone)'}\n",
      "{'input': 'Convert timestamp to timezone-naive in the specified timezone or local timezone\\n\\n    :param s: a pandas.Series\\n    :param from_timezone: the timezone to convert from. if None then use local timezone\\n    :param to_timezone: the timezone to convert to. if None then use local timezone\\n    :return pandas.Series where if it is a timestamp, has been converted to tz-naive', 'output': 'def _check_series_convert_timestamps_localize(s, from_timezone, to_timezone)'}\n",
      "{'input': 'Construct a StructType by adding new elements to it to define the schema. The method accepts\\n        either:\\n\\n            a) A single parameter which is a StructField object.\\n            b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\\n               metadata(optional). The data_type parameter may be either a String or a\\n               DataType object.\\n\\n        >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\\\\\\\\n        ...     StructField(\"f2\", StringType(), True, None)])\\n        >>> struct1 == struct2\\n        True\\n        >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\\n        >>> struct1 == struct2\\n        True\\n        >>> struct1 = StructType().add(\"f1\", \"string\", True)\\n        >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\\n        >>> struct1 == struct2\\n        True\\n\\n        :param field: Either the name of the field or a StructField object\\n        :param data_type: If present, the DataType of the StructField to create\\n        :param nullable: Whether the field to add should be nullable (default True)\\n        :param metadata: Any additional metadata (default None)\\n        :return: a new updated StructType', 'output': 'def add(self, field, data_type=None, nullable=True, metadata=None)'}\n",
      "{'input': \"Cache the sqlType() into class, because it's heavy used in `toInternal`.\", 'output': 'def _cachedSqlType(cls)'}\n",
      "{'input': 'Return as an dict\\n\\n        :param recursive: turns the nested Row as dict (default: False).\\n\\n        >>> Row(name=\"Alice\", age=11).asDict() == {\\'name\\': \\'Alice\\', \\'age\\': 11}\\n        True\\n        >>> row = Row(key=1, value=Row(name=\\'a\\', age=2))\\n        >>> row.asDict() == {\\'key\\': 1, \\'value\\': Row(age=2, name=\\'a\\')}\\n        True\\n        >>> row.asDict(True) == {\\'key\\': 1, \\'value\\': {\\'name\\': \\'a\\', \\'age\\': 2}}\\n        True', 'output': 'def asDict(self, recursive=False)'}\n",
      "{'input': 'Gets summary (e.g. residuals, mse, r-squared ) of model on\\n        training set. An exception is thrown if\\n        `trainingSummary is None`.', 'output': 'def summary(self)'}\n",
      "{'input': 'Evaluates the model on a test dataset.\\n\\n        :param dataset:\\n          Test dataset to evaluate model on, where dataset is an\\n          instance of :py:class:`pyspark.sql.DataFrame`', 'output': 'def evaluate(self, dataset)'}\n",
      "{'input': 'Gets summary (e.g. residuals, deviance, pValues) of model on\\n        training set. An exception is thrown if\\n        `trainingSummary is None`.', 'output': 'def summary(self)'}\n",
      "{'input': 'Evaluates the model on a test dataset.\\n\\n        :param dataset:\\n          Test dataset to evaluate model on, where dataset is an\\n          instance of :py:class:`pyspark.sql.DataFrame`', 'output': 'def evaluate(self, dataset)'}\n",
      "{'input': 'Get all the directories', 'output': 'def _get_local_dirs(sub)'}\n",
      "{'input': 'Choose one directory for spill by number n', 'output': 'def _get_spill_dir(self, n)'}\n",
      "{'input': 'Combine the items by creator and combiner', 'output': 'def mergeValues(self, iterator)'}\n",
      "{'input': 'Merge (K,V) pair by mergeCombiner', 'output': 'def mergeCombiners(self, iterator, limit=None)'}\n",
      "{'input': 'dump already partitioned data into disks.\\n\\n        It will dump the data in batch for better performance.', 'output': 'def _spill(self)'}\n",
      "{'input': 'Return all merged items as iterator', 'output': 'def items(self)'}\n",
      "{'input': 'Return all partitioned items as iterator', 'output': 'def _external_items(self)'}\n",
      "{'input': 'merge the partitioned items and return the as iterator\\n\\n        If one partition can not be fit in memory, then them will be\\n        partitioned and merged recursively.', 'output': 'def _recursive_merged_items(self, index)'}\n",
      "{'input': 'Choose one directory for spill by number n', 'output': 'def _get_path(self, n)'}\n",
      "{'input': 'Sort the elements in iterator, do external sort when the memory\\n        goes above the limit.', 'output': 'def sorted(self, iterator, key=None, reverse=False)'}\n",
      "{'input': 'dump the values into disk', 'output': 'def _spill(self)'}\n",
      "{'input': 'dump already partitioned data into disks.', 'output': 'def _spill(self)'}\n",
      "{'input': 'load a partition from disk, then sort and group by key', 'output': 'def _merge_sorted_items(self, index)'}\n",
      "{'input': 'Called by a worker process after the fork().', 'output': 'def worker(sock, authenticated)'}\n",
      "{'input': 'This function returns consistent hash code for builtin types, especially\\n    for None and tuple with None.\\n\\n    The algorithm is similar to that one used by CPython 2.7\\n\\n    >>> portable_hash(None)\\n    0\\n    >>> portable_hash((None, 1)) & 0xffffffff\\n    219750521', 'output': 'def portable_hash(x)'}\n",
      "{'input': 'Parse a memory string in the format supported by Java (e.g. 1g, 200m) and\\n    return the value in MiB\\n\\n    >>> _parse_memory(\"256m\")\\n    256\\n    >>> _parse_memory(\"2g\")\\n    2048', 'output': 'def _parse_memory(s)'}\n",
      "{'input': \"Ignore the 'u' prefix of string in doc tests, to make it works\\n    in both python 2 and 3\", 'output': 'def ignore_unicode_prefix(f)'}\n",
      "{'input': 'Persist this RDD with the default storage level (C{MEMORY_ONLY}).', 'output': 'def cache(self)'}\n",
      "{'input': 'Set this RDD\\'s storage level to persist its values across operations\\n        after the first time it is computed. This can only be used to assign\\n        a new storage level if the RDD does not have a storage level set yet.\\n        If no storage level is specified defaults to (C{MEMORY_ONLY}).\\n\\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\\n        >>> rdd.persist().is_cached\\n        True', 'output': 'def persist(self, storageLevel=StorageLevel.MEMORY_ONLY)'}\n",
      "{'input': 'Mark the RDD as non-persistent, and remove all blocks for it from\\n        memory and disk.\\n\\n        .. versionchanged:: 3.0.0\\n           Added optional argument `blocking` to specify whether to block until all\\n           blocks are deleted.', 'output': 'def unpersist(self, blocking=False)'}\n",
      "{'input': 'Gets the name of the file to which this RDD was checkpointed\\n\\n        Not defined if RDD is checkpointed locally.', 'output': 'def getCheckpointFile(self)'}\n",
      "{'input': 'Return a new RDD by applying a function to each element of this RDD.\\n\\n        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\\n        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\\n        [(\\'a\\', 1), (\\'b\\', 1), (\\'c\\', 1)]', 'output': 'def map(self, f, preservesPartitioning=False)'}\n",
      "{'input': 'Return a new RDD by first applying a function to all elements of this\\n        RDD, and then flattening the results.\\n\\n        >>> rdd = sc.parallelize([2, 3, 4])\\n        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\\n        [1, 1, 1, 2, 2, 3]\\n        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\\n        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]', 'output': 'def flatMap(self, f, preservesPartitioning=False)'}\n",
      "{'input': 'Return a new RDD by applying a function to each partition of this RDD.\\n\\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\\n        >>> def f(iterator): yield sum(iterator)\\n        >>> rdd.mapPartitions(f).collect()\\n        [3, 7]', 'output': 'def mapPartitions(self, f, preservesPartitioning=False)'}\n",
      "{'input': 'Deprecated: use mapPartitionsWithIndex instead.\\n\\n        Return a new RDD by applying a function to each partition of this RDD,\\n        while tracking the index of the original partition.\\n\\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\\n        >>> def f(splitIndex, iterator): yield splitIndex\\n        >>> rdd.mapPartitionsWithSplit(f).sum()\\n        6', 'output': 'def mapPartitionsWithSplit(self, f, preservesPartitioning=False)'}\n",
      "{'input': 'Return a new RDD containing the distinct elements in this RDD.\\n\\n        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\\n        [1, 2, 3]', 'output': 'def distinct(self, numPartitions=None)'}\n",
      "{'input': \"Return a sampled subset of this RDD.\\n\\n        :param withReplacement: can elements be sampled multiple times (replaced when sampled out)\\n        :param fraction: expected size of the sample as a fraction of this RDD's size\\n            without replacement: probability that each element is chosen; fraction must be [0, 1]\\n            with replacement: expected number of times each element is chosen; fraction must be >= 0\\n        :param seed: seed for the random number generator\\n\\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\\n            count of the given :class:`DataFrame`.\\n\\n        >>> rdd = sc.parallelize(range(100), 4)\\n        >>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\\n        True\", 'output': 'def sample(self, withReplacement, fraction, seed=None)'}\n",
      "{'input': \"Randomly splits this RDD with the provided weights.\\n\\n        :param weights: weights for splits, will be normalized if they don't sum to 1\\n        :param seed: random seed\\n        :return: split RDDs in a list\\n\\n        >>> rdd = sc.parallelize(range(500), 1)\\n        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\\n        >>> len(rdd1.collect() + rdd2.collect())\\n        500\\n        >>> 150 < rdd1.count() < 250\\n        True\\n        >>> 250 < rdd2.count() < 350\\n        True\", 'output': 'def randomSplit(self, weights, seed=None)'}\n",
      "{'input': \"Return a fixed-size sampled subset of this RDD.\\n\\n        .. note:: This method should only be used if the resulting array is expected\\n            to be small, as all the data is loaded into the driver's memory.\\n\\n        >>> rdd = sc.parallelize(range(0, 10))\\n        >>> len(rdd.takeSample(True, 20, 1))\\n        20\\n        >>> len(rdd.takeSample(False, 5, 2))\\n        5\\n        >>> len(rdd.takeSample(False, 15, 3))\\n        10\", 'output': 'def takeSample(self, withReplacement, num, seed=None)'}\n",
      "{'input': \"Returns a sampling rate that guarantees a sample of\\n        size >= sampleSizeLowerBound 99.99% of the time.\\n\\n        How the sampling rate is determined:\\n        Let p = num / total, where num is the sample size and total is the\\n        total number of data points in the RDD. We're trying to compute\\n        q > p such that\\n          - when sampling with replacement, we're drawing each data point\\n            with prob_i ~ Pois(q), where we want to guarantee\\n            Pr[s < num] < 0.0001 for s = sum(prob_i for i from 0 to\\n            total), i.e. the failure rate of not having a sufficiently large\\n            sample < 0.0001. Setting q = p + 5 * sqrt(p/total) is sufficient\\n            to guarantee 0.9999 success rate for num > 12, but we need a\\n            slightly larger q (9 empirically determined).\\n          - when sampling without replacement, we're drawing each data point\\n            with prob_i ~ Binomial(total, fraction) and our choice of q\\n            guarantees 1-delta, or 0.9999 success rate, where success rate is\\n            defined the same as in sampling with replacement.\", 'output': 'def _computeFractionForSampleSize(sampleSizeLowerBound, total, withReplacement)'}\n",
      "{'input': 'Return the union of this RDD and another one.\\n\\n        >>> rdd = sc.parallelize([1, 1, 2, 3])\\n        >>> rdd.union(rdd).collect()\\n        [1, 1, 2, 3, 1, 1, 2, 3]', 'output': 'def union(self, other)'}\n",
      "{'input': 'Return the intersection of this RDD and another one. The output will\\n        not contain any duplicate elements, even if the input RDDs did.\\n\\n        .. note:: This method performs a shuffle internally.\\n\\n        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\\n        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\\n        >>> rdd1.intersection(rdd2).collect()\\n        [1, 2, 3]', 'output': 'def intersection(self, other)'}\n",
      "{'input': 'Repartition the RDD according to the given partitioner and, within each resulting partition,\\n        sort records by their keys.\\n\\n        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\\n        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, True)\\n        >>> rdd2.glom().collect()\\n        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]', 'output': 'def repartitionAndSortWithinPartitions(self, numPartitions=None, partitionFunc=portable_hash,\\n                                           ascending=True, keyfunc=lambda x: x)'}\n",
      "{'input': \"Sorts this RDD, which is assumed to consist of (key, value) pairs.\\n\\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\\n        >>> sc.parallelize(tmp).sortByKey().first()\\n        ('1', 3)\\n        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\\n        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\\n        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\\n        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\\n        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\\n        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\", 'output': 'def sortByKey(self, ascending=True, numPartitions=None, keyfunc=lambda x: x)'}\n",
      "{'input': \"Sorts this RDD by the given keyfunc\\n\\n        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\\n        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\\n        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\\n        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\", 'output': 'def sortBy(self, keyfunc, ascending=True, numPartitions=None)'}\n",
      "{'input': 'Return the Cartesian product of this RDD and another one, that is, the\\n        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\\n        C{b} is in C{other}.\\n\\n        >>> rdd = sc.parallelize([1, 2])\\n        >>> sorted(rdd.cartesian(rdd).collect())\\n        [(1, 1), (1, 2), (2, 1), (2, 2)]', 'output': 'def cartesian(self, other)'}\n",
      "{'input': 'Return an RDD of grouped items.\\n\\n        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\\n        >>> result = rdd.groupBy(lambda x: x % 2).collect()\\n        >>> sorted([(x, sorted(y)) for (x, y) in result])\\n        [(0, [2, 8]), (1, [1, 1, 3, 5])]', 'output': 'def groupBy(self, f, numPartitions=None, partitionFunc=portable_hash)'}\n",
      "{'input': \"Return an RDD created by piping elements to a forked external process.\\n\\n        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\\n        [u'1', u'2', u'', u'3']\\n\\n        :param checkCode: whether or not to check the return value of the shell command.\", 'output': 'def pipe(self, command, env=None, checkCode=False)'}\n",
      "{'input': 'Applies a function to all elements of this RDD.\\n\\n        >>> def f(x): print(x)\\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)', 'output': 'def foreach(self, f)'}\n",
      "{'input': 'Applies a function to each partition of this RDD.\\n\\n        >>> def f(iterator):\\n        ...     for x in iterator:\\n        ...          print(x)\\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)', 'output': 'def foreachPartition(self, f)'}\n",
      "{'input': \"Return a list that contains all of the elements in this RDD.\\n\\n        .. note:: This method should only be used if the resulting array is expected\\n            to be small, as all the data is loaded into the driver's memory.\", 'output': 'def collect(self)'}\n",
      "{'input': 'Reduces the elements of this RDD using the specified commutative and\\n        associative binary operator. Currently reduces partitions locally.\\n\\n        >>> from operator import add\\n        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\\n        15\\n        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\\n        10\\n        >>> sc.parallelize([]).reduce(add)\\n        Traceback (most recent call last):\\n            ...\\n        ValueError: Can not reduce() empty RDD', 'output': 'def reduce(self, f)'}\n",
      "{'input': 'Reduces the elements of this RDD in a multi-level tree pattern.\\n\\n        :param depth: suggested depth of the tree (default: 2)\\n\\n        >>> add = lambda x, y: x + y\\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\\n        >>> rdd.treeReduce(add)\\n        -5\\n        >>> rdd.treeReduce(add, 1)\\n        -5\\n        >>> rdd.treeReduce(add, 2)\\n        -5\\n        >>> rdd.treeReduce(add, 5)\\n        -5\\n        >>> rdd.treeReduce(add, 10)\\n        -5', 'output': 'def treeReduce(self, f, depth=2)'}\n",
      "{'input': 'Aggregate the elements of each partition, and then the results for all\\n        the partitions, using a given associative function and a neutral \"zero value.\"\\n\\n        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\\n        as its result value to avoid object allocation; however, it should not\\n        modify C{t2}.\\n\\n        This behaves somewhat differently from fold operations implemented\\n        for non-distributed collections in functional languages like Scala.\\n        This fold operation may be applied to partitions individually, and then\\n        fold those results into the final result, rather than apply the fold\\n        to each element sequentially in some defined ordering. For functions\\n        that are not commutative, the result may differ from that of a fold\\n        applied to a non-distributed collection.\\n\\n        >>> from operator import add\\n        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\\n        15', 'output': 'def fold(self, zeroValue, op)'}\n",
      "{'input': 'Aggregate the elements of each partition, and then the results for all\\n        the partitions, using a given combine functions and a neutral \"zero\\n        value.\"\\n\\n        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\\n        as its result value to avoid object allocation; however, it should not\\n        modify C{t2}.\\n\\n        The first function (seqOp) can return a different result type, U, than\\n        the type of this RDD. Thus, we need one operation for merging a T into\\n        an U and one operation for merging two U\\n\\n        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\\n        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\\n        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\\n        (10, 4)\\n        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\\n        (0, 0)', 'output': 'def aggregate(self, zeroValue, seqOp, combOp)'}\n",
      "{'input': 'Aggregates the elements of this RDD in a multi-level tree\\n        pattern.\\n\\n        :param depth: suggested depth of the tree (default: 2)\\n\\n        >>> add = lambda x, y: x + y\\n        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\\n        >>> rdd.treeAggregate(0, add, add)\\n        -5\\n        >>> rdd.treeAggregate(0, add, add, 1)\\n        -5\\n        >>> rdd.treeAggregate(0, add, add, 2)\\n        -5\\n        >>> rdd.treeAggregate(0, add, add, 5)\\n        -5\\n        >>> rdd.treeAggregate(0, add, add, 10)\\n        -5', 'output': 'def treeAggregate(self, zeroValue, seqOp, combOp, depth=2)'}\n",
      "{'input': 'Find the maximum item in this RDD.\\n\\n        :param key: A function used to generate key for comparing\\n\\n        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\\n        >>> rdd.max()\\n        43.0\\n        >>> rdd.max(key=str)\\n        5.0', 'output': 'def max(self, key=None)'}\n",
      "{'input': 'Find the minimum item in this RDD.\\n\\n        :param key: A function used to generate key for comparing\\n\\n        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\\n        >>> rdd.min()\\n        2.0\\n        >>> rdd.min(key=str)\\n        10.0', 'output': 'def min(self, key=None)'}\n",
      "{'input': 'Add up the elements in this RDD.\\n\\n        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\\n        6.0', 'output': 'def sum(self)'}\n",
      "{'input': \"Return a L{StatCounter} object that captures the mean, variance\\n        and count of the RDD's elements in one operation.\", 'output': 'def stats(self)'}\n",
      "{'input': 'Compute a histogram using the provided buckets. The buckets\\n        are all open to the right except for the last which is closed.\\n        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\\n        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\\n        and 50 we would have a histogram of 1,0,1.\\n\\n        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\\n        this can be switched from an O(log n) inseration to O(1) per\\n        element (where n is the number of buckets).\\n\\n        Buckets must be sorted, not contain any duplicates, and have\\n        at least two elements.\\n\\n        If `buckets` is a number, it will generate buckets which are\\n        evenly spaced between the minimum and maximum of the RDD. For\\n        example, if the min value is 0 and the max is 100, given `buckets`\\n        as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\\n        be at least 1. An exception is raised if the RDD contains infinity.\\n        If the elements in the RDD do not vary (max == min), a single bucket\\n        will be used.\\n\\n        The return value is a tuple of buckets and histogram.\\n\\n        >>> rdd = sc.parallelize(range(51))\\n        >>> rdd.histogram(2)\\n        ([0, 25, 50], [25, 26])\\n        >>> rdd.histogram([0, 5, 25, 50])\\n        ([0, 5, 25, 50], [5, 20, 26])\\n        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\\n        ([0, 15, 30, 45, 60], [15, 15, 15, 6])\\n        >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\\n        >>> rdd.histogram((\"a\", \"b\", \"c\"))\\n        ((\\'a\\', \\'b\\', \\'c\\'), [2, 2])', 'output': 'def histogram(self, buckets)'}\n",
      "{'input': 'Return the count of each unique value in this RDD as a dictionary of\\n        (value, count) pairs.\\n\\n        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\\n        [(1, 2), (2, 3)]', 'output': 'def countByValue(self)'}\n",
      "{'input': \"Get the top N elements from an RDD.\\n\\n        .. note:: This method should only be used if the resulting array is expected\\n            to be small, as all the data is loaded into the driver's memory.\\n\\n        .. note:: It returns the list sorted in descending order.\\n\\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\\n        [12]\\n        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\\n        [6, 5]\\n        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\\n        [4, 3, 2]\", 'output': 'def top(self, num, key=None)'}\n",
      "{'input': \"Get the N elements from an RDD ordered in ascending order or as\\n        specified by the optional key function.\\n\\n        .. note:: this method should only be used if the resulting array is expected\\n            to be small, as all the data is loaded into the driver's memory.\\n\\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\\n        [1, 2, 3, 4, 5, 6]\\n        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\\n        [10, 9, 7, 6, 5, 4]\", 'output': 'def takeOrdered(self, num, key=None)'}\n",
      "{'input': \"Take the first num elements of the RDD.\\n\\n        It works by first scanning one partition, and use the results from\\n        that partition to estimate the number of additional partitions needed\\n        to satisfy the limit.\\n\\n        Translated from the Scala implementation in RDD#take().\\n\\n        .. note:: this method should only be used if the resulting array is expected\\n            to be small, as all the data is loaded into the driver's memory.\\n\\n        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\\n        [2, 3]\\n        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\\n        [2, 3, 4, 5, 6]\\n        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\\n        [91, 92, 93]\", 'output': 'def take(self, num)'}\n",
      "{'input': 'Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\\n        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\\n        converted for output using either user specified converters or, by default,\\n        L{org.apache.spark.api.python.JavaToWritableConverter}.\\n\\n        :param conf: Hadoop job configuration, passed in as a dict\\n        :param keyConverter: (None by default)\\n        :param valueConverter: (None by default)', 'output': 'def saveAsNewAPIHadoopDataset(self, conf, keyConverter=None, valueConverter=None)'}\n",
      "{'input': 'Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\\n        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\\n        will be inferred if not specified. Keys and values are converted for output using either\\n        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\\n        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\\n        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\\n\\n        :param path: path to Hadoop file\\n        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\\n               (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\\n        :param keyClass: fully qualified classname of key Writable class\\n               (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\\n        :param valueClass: fully qualified classname of value Writable class\\n               (e.g. \"org.apache.hadoop.io.Text\", None by default)\\n        :param keyConverter: (None by default)\\n        :param valueConverter: (None by default)\\n        :param conf: Hadoop job configuration, passed in as a dict (None by default)', 'output': 'def saveAsNewAPIHadoopFile(self, path, outputFormatClass, keyClass=None, valueClass=None,\\n                               keyConverter=None, valueConverter=None, conf=None)'}\n",
      "{'input': \"Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\\n        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\\n        RDD's key and value types. The mechanism is as follows:\\n\\n            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\\n            2. Keys and values of this Java RDD are converted to Writables and written out.\\n\\n        :param path: path to sequence file\\n        :param compressionCodecClass: (None by default)\", 'output': 'def saveAsSequenceFile(self, path, compressionCodecClass=None)'}\n",
      "{'input': \"Save this RDD as a SequenceFile of serialized objects. The serializer\\n        used is L{pyspark.serializers.PickleSerializer}, default batch size\\n        is 10.\\n\\n        >>> tmpFile = NamedTemporaryFile(delete=True)\\n        >>> tmpFile.close()\\n        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\\n        >>> sorted(sc.pickleFile(tmpFile.name, 5).map(str).collect())\\n        ['1', '2', 'rdd', 'spark']\", 'output': 'def saveAsPickleFile(self, path, batchSize=10)'}\n",
      "{'input': 'Save this RDD as a text file, using string representations of elements.\\n\\n        @param path: path to text file\\n        @param compressionCodecClass: (None by default) string i.e.\\n            \"org.apache.hadoop.io.compress.GzipCodec\"\\n\\n        >>> tempFile = NamedTemporaryFile(delete=True)\\n        >>> tempFile.close()\\n        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\\n        >>> from fileinput import input\\n        >>> from glob import glob\\n        >>> \\'\\'.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\\n        \\'0\\\\\\\\n1\\\\\\\\n2\\\\\\\\n3\\\\\\\\n4\\\\\\\\n5\\\\\\\\n6\\\\\\\\n7\\\\\\\\n8\\\\\\\\n9\\\\\\\\n\\'\\n\\n        Empty lines are tolerated when saving to text files.\\n\\n        >>> tempFile2 = NamedTemporaryFile(delete=True)\\n        >>> tempFile2.close()\\n        >>> sc.parallelize([\\'\\', \\'foo\\', \\'\\', \\'bar\\', \\'\\']).saveAsTextFile(tempFile2.name)\\n        >>> \\'\\'.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\\n        \\'\\\\\\\\n\\\\\\\\n\\\\\\\\nbar\\\\\\\\nfoo\\\\\\\\n\\'\\n\\n        Using compressionCodecClass\\n\\n        >>> tempFile3 = NamedTemporaryFile(delete=True)\\n        >>> tempFile3.close()\\n        >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\\n        >>> sc.parallelize([\\'foo\\', \\'bar\\']).saveAsTextFile(tempFile3.name, codec)\\n        >>> from fileinput import input, hook_compressed\\n        >>> result = sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed))\\n        >>> b\\'\\'.join(result).decode(\\'utf-8\\')\\n        u\\'bar\\\\\\\\nfoo\\\\\\\\n\\'', 'output': 'def saveAsTextFile(self, path, compressionCodecClass=None)'}\n",
      "{'input': 'Merge the values for each key using an associative and commutative reduce function.\\n\\n        This will also perform the merging locally on each mapper before\\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\\n\\n        Output will be partitioned with C{numPartitions} partitions, or\\n        the default parallelism level if C{numPartitions} is not specified.\\n        Default partitioner is hash-partition.\\n\\n        >>> from operator import add\\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\\n        >>> sorted(rdd.reduceByKey(add).collect())\\n        [(\\'a\\', 2), (\\'b\\', 1)]', 'output': 'def reduceByKey(self, func, numPartitions=None, partitionFunc=portable_hash)'}\n",
      "{'input': 'Merge the values for each key using an associative and commutative reduce function, but\\n        return the results immediately to the master as a dictionary.\\n\\n        This will also perform the merging locally on each mapper before\\n        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\\n\\n        >>> from operator import add\\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\\n        >>> sorted(rdd.reduceByKeyLocally(add).items())\\n        [(\\'a\\', 2), (\\'b\\', 1)]', 'output': 'def reduceByKeyLocally(self, func)'}\n",
      "{'input': 'Return a copy of the RDD partitioned using the specified partitioner.\\n\\n        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\\n        >>> sets = pairs.partitionBy(2).glom().collect()\\n        >>> len(set(sets[0]).intersection(set(sets[1])))\\n        0', 'output': 'def partitionBy(self, numPartitions, partitionFunc=portable_hash)'}\n",
      "{'input': 'Generic function to combine the elements for each key using a custom\\n        set of aggregation functions.\\n\\n        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\\n        type\" C.\\n\\n        Users provide three functions:\\n\\n            - C{createCombiner}, which turns a V into a C (e.g., creates\\n              a one-element list)\\n            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\\n              a list)\\n            - C{mergeCombiners}, to combine two C\\'s into a single one (e.g., merges\\n              the lists)\\n\\n        To avoid memory allocation, both mergeValue and mergeCombiners are allowed to\\n        modify and return their first argument instead of creating a new C.\\n\\n        In addition, users can control the partitioning of the output RDD.\\n\\n        .. note:: V and C can be different -- for example, one might group an RDD of type\\n            (Int, Int) into an RDD of type (Int, List[Int]).\\n\\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\\n        >>> def to_list(a):\\n        ...     return [a]\\n        ...\\n        >>> def append(a, b):\\n        ...     a.append(b)\\n        ...     return a\\n        ...\\n        >>> def extend(a, b):\\n        ...     a.extend(b)\\n        ...     return a\\n        ...\\n        >>> sorted(x.combineByKey(to_list, append, extend).collect())\\n        [(\\'a\\', [1, 2]), (\\'b\\', [1])]', 'output': 'def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\\n                     numPartitions=None, partitionFunc=portable_hash)'}\n",
      "{'input': 'Aggregate the values of each key, using given combine functions and a neutral\\n        \"zero value\". This function can return a different result type, U, than the type\\n        of the values in this RDD, V. Thus, we need one operation for merging a V into\\n        a U and one operation for merging two U\\'s, The former operation is used for merging\\n        values within a partition, and the latter is used for merging values between\\n        partitions. To avoid memory allocation, both of these functions are\\n        allowed to modify and return their first argument instead of creating a new U.', 'output': 'def aggregateByKey(self, zeroValue, seqFunc, combFunc, numPartitions=None,\\n                       partitionFunc=portable_hash)'}\n",
      "{'input': 'Merge the values for each key using an associative function \"func\"\\n        and a neutral \"zeroValue\" which may be added to the result an\\n        arbitrary number of times, and must not change the result\\n        (e.g., 0 for addition, or 1 for multiplication.).\\n\\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\\n        >>> from operator import add\\n        >>> sorted(rdd.foldByKey(0, add).collect())\\n        [(\\'a\\', 2), (\\'b\\', 1)]', 'output': 'def foldByKey(self, zeroValue, func, numPartitions=None, partitionFunc=portable_hash)'}\n",
      "{'input': 'Group the values for each key in the RDD into a single sequence.\\n        Hash-partitions the resulting RDD with numPartitions partitions.\\n\\n        .. note:: If you are grouping in order to perform an aggregation (such as a\\n            sum or average) over each key, using reduceByKey or aggregateByKey will\\n            provide much better performance.\\n\\n        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\\n        >>> sorted(rdd.groupByKey().mapValues(len).collect())\\n        [(\\'a\\', 2), (\\'b\\', 1)]\\n        >>> sorted(rdd.groupByKey().mapValues(list).collect())\\n        [(\\'a\\', [1, 1]), (\\'b\\', [1])]', 'output': 'def groupByKey(self, numPartitions=None, partitionFunc=portable_hash)'}\n",
      "{'input': 'Pass each value in the key-value pair RDD through a flatMap function\\n        without changing the keys; this also retains the original RDD\\'s\\n        partitioning.\\n\\n        >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\\n        >>> def f(x): return x\\n        >>> x.flatMapValues(f).collect()\\n        [(\\'a\\', \\'x\\'), (\\'a\\', \\'y\\'), (\\'a\\', \\'z\\'), (\\'b\\', \\'p\\'), (\\'b\\', \\'r\\')]', 'output': 'def flatMapValues(self, f)'}\n",
      "{'input': 'Pass each value in the key-value pair RDD through a map function\\n        without changing the keys; this also retains the original RDD\\'s\\n        partitioning.\\n\\n        >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\\n        >>> def f(x): return len(x)\\n        >>> x.mapValues(f).collect()\\n        [(\\'a\\', 3), (\\'b\\', 1)]', 'output': 'def mapValues(self, f)'}\n",
      "{'input': 'Return a subset of this RDD sampled by key (via stratified sampling).\\n        Create a sample of this RDD using variable sampling rates for\\n        different keys as specified by fractions, a key to sampling rate map.\\n\\n        >>> fractions = {\"a\": 0.2, \"b\": 0.1}\\n        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\\n        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\\n        >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\\n        True\\n        >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\\n        True\\n        >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\\n        True', 'output': 'def sampleByKey(self, withReplacement, fractions, seed=None)'}\n",
      "{'input': 'Return each (key, value) pair in C{self} that has no pair with matching\\n        key in C{other}.\\n\\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\\n        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\\n        >>> sorted(x.subtractByKey(y).collect())\\n        [(\\'b\\', 4), (\\'b\\', 5)]', 'output': 'def subtractByKey(self, other, numPartitions=None)'}\n",
      "{'input': 'Return each value in C{self} that is not contained in C{other}.\\n\\n        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\\n        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\\n        >>> sorted(x.subtract(y).collect())\\n        [(\\'a\\', 1), (\\'b\\', 4), (\\'b\\', 5)]', 'output': 'def subtract(self, other, numPartitions=None)'}\n",
      "{'input': 'Return a new RDD that is reduced into `numPartitions` partitions.\\n\\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\\n        [[1], [2, 3], [4, 5]]\\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\\n        [[1, 2, 3, 4, 5]]', 'output': 'def coalesce(self, numPartitions, shuffle=False)'}\n",
      "{'input': 'Zips this RDD with another one, returning key-value pairs with the\\n        first element in each RDD second element in each RDD, etc. Assumes\\n        that the two RDDs have the same number of partitions and the same\\n        number of elements in each partition (e.g. one was made through\\n        a map on the other).\\n\\n        >>> x = sc.parallelize(range(0,5))\\n        >>> y = sc.parallelize(range(1000, 1005))\\n        >>> x.zip(y).collect()\\n        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]', 'output': 'def zip(self, other)'}\n",
      "{'input': 'Zips this RDD with its element indices.\\n\\n        The ordering is first based on the partition index and then the\\n        ordering of items within each partition. So the first item in\\n        the first partition gets index 0, and the last item in the last\\n        partition receives the largest index.\\n\\n        This method needs to trigger a spark job when this RDD contains\\n        more than one partitions.\\n\\n        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\\n        [(\\'a\\', 0), (\\'b\\', 1), (\\'c\\', 2), (\\'d\\', 3)]', 'output': 'def zipWithIndex(self)'}\n",
      "{'input': 'Zips this RDD with generated unique Long ids.\\n\\n        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\\n        n is the number of partitions. So there may exist gaps, but this\\n        method won\\'t trigger a spark job, which is different from\\n        L{zipWithIndex}\\n\\n        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\\n        [(\\'a\\', 0), (\\'b\\', 1), (\\'c\\', 4), (\\'d\\', 2), (\\'e\\', 5)]', 'output': 'def zipWithUniqueId(self)'}\n",
      "{'input': \"Get the RDD's current storage level.\\n\\n        >>> rdd1 = sc.parallelize([1,2])\\n        >>> rdd1.getStorageLevel()\\n        StorageLevel(False, False, False, False, 1)\\n        >>> print(rdd1.getStorageLevel())\\n        Serialized 1x Replicated\", 'output': 'def getStorageLevel(self)'}\n",
      "{'input': \"Returns the default number of partitions to use during reduce tasks (e.g., groupBy).\\n        If spark.default.parallelism is set, then we'll use the value from SparkContext\\n        defaultParallelism, otherwise we'll use the number of partitions in this RDD.\\n\\n        This mirrors the behavior of the Scala Partitioner#defaultPartitioner, intended to reduce\\n        the likelihood of OOMs. Once PySpark adopts Partitioner-based APIs, this behavior will\\n        be inherent.\", 'output': 'def _defaultReducePartitions(self)'}\n",
      "{'input': \"Return the list of values in the RDD for key `key`. This operation\\n        is done efficiently if the RDD has a known partitioner by only\\n        searching the partition that the key maps to.\\n\\n        >>> l = range(1000)\\n        >>> rdd = sc.parallelize(zip(l, l), 10)\\n        >>> rdd.lookup(42)  # slow\\n        [42]\\n        >>> sorted = rdd.sortByKey()\\n        >>> sorted.lookup(42)  # fast\\n        [42]\\n        >>> sorted.lookup(1024)\\n        []\\n        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\\n        >>> list(rdd2.lookup(('a', 'b'))[0])\\n        ['c']\", 'output': 'def lookup(self, key)'}\n",
      "{'input': 'Return a JavaRDD of Object by unpickling\\n\\n        It will convert each Python object into Java object by Pyrolite, whenever the\\n        RDD is serialized in batch or not.', 'output': 'def _to_java_object_rdd(self)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Approximate version of count() that returns a potentially incomplete\\n        result within a timeout, even if not all tasks have finished.\\n\\n        >>> rdd = sc.parallelize(range(1000), 10)\\n        >>> rdd.countApprox(1000, 1.0)\\n        1000', 'output': 'def countApprox(self, timeout, confidence=0.95)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Approximate operation to return the sum within a timeout\\n        or meet the confidence.\\n\\n        >>> rdd = sc.parallelize(range(1000), 10)\\n        >>> r = sum(range(1000))\\n        >>> abs(rdd.sumApprox(1000) - r) / r < 0.05\\n        True', 'output': 'def sumApprox(self, timeout, confidence=0.95)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Approximate operation to return the mean within a timeout\\n        or meet the confidence.\\n\\n        >>> rdd = sc.parallelize(range(1000), 10)\\n        >>> r = sum(range(1000)) / 1000.0\\n        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\\n        True', 'output': 'def meanApprox(self, timeout, confidence=0.95)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Return approximate number of distinct elements in the RDD.\\n\\n        The algorithm used is based on streamlib\\'s implementation of\\n        `\"HyperLogLog in Practice: Algorithmic Engineering of a State\\n        of The Art Cardinality Estimation Algorithm\", available here\\n        <https://doi.org/10.1145/2452376.2452456>`_.\\n\\n        :param relativeSD: Relative accuracy. Smaller values create\\n                           counters that require more space.\\n                           It must be greater than 0.000017.\\n\\n        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\\n        >>> 900 < n < 1100\\n        True\\n        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\\n        >>> 16 < n < 24\\n        True', 'output': 'def countApproxDistinct(self, relativeSD=0.05)'}\n",
      "{'input': 'Return an iterator that contains all of the elements in this RDD.\\n        The iterator will consume as much memory as the largest partition in this RDD.\\n\\n        >>> rdd = sc.parallelize(range(10))\\n        >>> [x for x in rdd.toLocalIterator()]\\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]', 'output': 'def toLocalIterator(self)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Returns a new RDD by applying a function to each partition of the wrapped RDD,\\n        where tasks are launched together in a barrier stage.\\n        The interface is the same as :func:`RDD.mapPartitions`.\\n        Please see the API doc there.\\n\\n        .. versionadded:: 2.4.0', 'output': 'def mapPartitions(self, f, preservesPartitioning=False)'}\n",
      "{'input': 'Convert a list of Column (or names) into a JVM Seq of Column.\\n\\n    An optional `converter` could be used to convert items in `cols`\\n    into JVM Column objects.', 'output': 'def _to_seq(sc, cols, converter=None)'}\n",
      "{'input': 'Convert a list of Column (or names) into a JVM (Scala) List of Column.\\n\\n    An optional `converter` could be used to convert items in `cols`\\n    into JVM Column objects.', 'output': 'def _to_list(sc, cols, converter=None)'}\n",
      "{'input': 'Create a method for given unary operator', 'output': 'def _unary_op(name, doc=\"unary operator\")'}\n",
      "{'input': 'Create a method for given binary operator', 'output': 'def _bin_op(name, doc=\"binary operator\")'}\n",
      "{'input': 'Create a method for binary operator (this object is on right side)', 'output': 'def _reverse_op(name, doc=\"binary operator\")'}\n",
      "{'input': 'Return a :class:`Column` which is a substring of the column.\\n\\n        :param startPos: start position (int or Column)\\n        :param length:  length of the substring (int or Column)\\n\\n        >>> df.select(df.name.substr(1, 3).alias(\"col\")).collect()\\n        [Row(col=u\\'Ali\\'), Row(col=u\\'Bob\\')]', 'output': 'def substr(self, startPos, length)'}\n",
      "{'input': 'A boolean expression that is evaluated to true if the value of this\\n        expression is contained by the evaluated values of the arguments.\\n\\n        >>> df[df.name.isin(\"Bob\", \"Mike\")].collect()\\n        [Row(age=5, name=u\\'Bob\\')]\\n        >>> df[df.age.isin([1, 2, 3])].collect()\\n        [Row(age=2, name=u\\'Alice\\')]', 'output': 'def isin(self, *cols)'}\n",
      "{'input': 'Returns this column aliased with a new name or names (in the case of expressions that\\n        return more than one column, such as explode).\\n\\n        :param alias: strings of desired column names (collects all positional arguments passed)\\n        :param metadata: a dict of information to be stored in ``metadata`` attribute of the\\n            corresponding :class: `StructField` (optional, keyword only argument)\\n\\n        .. versionchanged:: 2.2\\n           Added optional ``metadata`` argument.\\n\\n        >>> df.select(df.age.alias(\"age2\")).collect()\\n        [Row(age2=2), Row(age2=5)]\\n        >>> df.select(df.age.alias(\"age3\", metadata={\\'max\\': 99})).schema[\\'age3\\'].metadata[\\'max\\']\\n        99', 'output': 'def alias(self, *alias, **kwargs)'}\n",
      "{'input': 'Convert the column into type ``dataType``.\\n\\n        >>> df.select(df.age.cast(\"string\").alias(\\'ages\\')).collect()\\n        [Row(ages=u\\'2\\'), Row(ages=u\\'5\\')]\\n        >>> df.select(df.age.cast(StringType()).alias(\\'ages\\')).collect()\\n        [Row(ages=u\\'2\\'), Row(ages=u\\'5\\')]', 'output': 'def cast(self, dataType)'}\n",
      "{'input': 'Evaluates a list of conditions and returns one of multiple possible result expressions.\\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\\n\\n        See :func:`pyspark.sql.functions.when` for example usage.\\n\\n        :param condition: a boolean :class:`Column` expression.\\n        :param value: a literal value, or a :class:`Column` expression.\\n\\n        >>> from pyspark.sql import functions as F\\n        >>> df.select(df.name, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0)).show()\\n        +-----+------------------------------------------------------------+\\n        | name|CASE WHEN (age > 4) THEN 1 WHEN (age < 3) THEN -1 ELSE 0 END|\\n        +-----+------------------------------------------------------------+\\n        |Alice|                                                          -1|\\n        |  Bob|                                                           1|\\n        +-----+------------------------------------------------------------+', 'output': 'def when(self, condition, value)'}\n",
      "{'input': 'Evaluates a list of conditions and returns one of multiple possible result expressions.\\n        If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\\n\\n        See :func:`pyspark.sql.functions.when` for example usage.\\n\\n        :param value: a literal value, or a :class:`Column` expression.\\n\\n        >>> from pyspark.sql import functions as F\\n        >>> df.select(df.name, F.when(df.age > 3, 1).otherwise(0)).show()\\n        +-----+-------------------------------------+\\n        | name|CASE WHEN (age > 3) THEN 1 ELSE 0 END|\\n        +-----+-------------------------------------+\\n        |Alice|                                    0|\\n        |  Bob|                                    1|\\n        +-----+-------------------------------------+', 'output': 'def otherwise(self, value)'}\n",
      "{'input': 'Define a windowing column.\\n\\n        :param window: a :class:`WindowSpec`\\n        :return: a Column\\n\\n        >>> from pyspark.sql import Window\\n        >>> window = Window.partitionBy(\"name\").orderBy(\"age\").rowsBetween(-1, 1)\\n        >>> from pyspark.sql.functions import rank, min\\n        >>> # df.select(rank().over(window), min(\\'age\\').over(window))', 'output': 'def over(self, window)'}\n",
      "{'input': 'Applies transformation on a vector or an RDD[Vector].\\n\\n        .. note:: In Python, transform cannot currently be used within\\n            an RDD transformation or action.\\n            Call transform directly on the RDD instead.\\n\\n        :param vector: Vector or RDD of Vector to be transformed.', 'output': 'def transform(self, vector)'}\n",
      "{'input': 'Computes the mean and variance and stores as a model to be used\\n        for later scaling.\\n\\n        :param dataset: The data used to compute the mean and variance\\n                     to build the transformation model.\\n        :return: a StandardScalarModel', 'output': 'def fit(self, dataset)'}\n",
      "{'input': 'Returns a ChiSquared feature selector.\\n\\n        :param data: an `RDD[LabeledPoint]` containing the labeled dataset\\n                     with categorical features. Real-valued features will be\\n                     treated as categorical for each distinct value.\\n                     Apply feature discretizer before using this function.', 'output': 'def fit(self, data)'}\n",
      "{'input': 'Computes a [[PCAModel]] that contains the principal components of the input vectors.\\n        :param data: source vectors', 'output': 'def fit(self, data)'}\n",
      "{'input': 'Transforms the input document (list of terms) to term frequency\\n        vectors, or transform the RDD of document to RDD of term\\n        frequency vectors.', 'output': 'def transform(self, document)'}\n",
      "{'input': 'Computes the inverse document frequency.\\n\\n        :param dataset: an RDD of term frequency vectors', 'output': 'def fit(self, dataset)'}\n",
      "{'input': 'Find synonyms of a word\\n\\n        :param word: a word or a vector representation of word\\n        :param num: number of synonyms to find\\n        :return: array of (word, cosineSimilarity)\\n\\n        .. note:: Local use only', 'output': 'def findSynonyms(self, word, num)'}\n",
      "{'input': 'Load a model from the given path.', 'output': 'def load(cls, sc, path)'}\n",
      "{'input': 'Computes the Hadamard product of the vector.', 'output': 'def transform(self, vector)'}\n",
      "{'input': 'Predict values for a single data point or an RDD of points using\\n        the model trained.\\n\\n        .. note:: In Python, predict cannot currently be used within an RDD\\n            transformation or action.\\n            Call predict directly on the RDD instead.', 'output': 'def predict(self, x)'}\n",
      "{'input': 'Train a decision tree model for classification.\\n\\n        :param data:\\n          Training data: RDD of LabeledPoint. Labels should take values\\n          {0, 1, ..., numClasses-1}.\\n        :param numClasses:\\n          Number of classes for classification.\\n        :param categoricalFeaturesInfo:\\n          Map storing arity of categorical features. An entry (n -> k)\\n          indicates that feature n is categorical with k categories\\n          indexed from 0: {0, 1, ..., k-1}.\\n        :param impurity:\\n          Criterion used for information gain calculation.\\n          Supported values: \"gini\" or \"entropy\".\\n          (default: \"gini\")\\n        :param maxDepth:\\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\\n          means 1 internal node + 2 leaf nodes).\\n          (default: 5)\\n        :param maxBins:\\n          Number of bins used for finding splits at each node.\\n          (default: 32)\\n        :param minInstancesPerNode:\\n          Minimum number of instances required at child nodes to create\\n          the parent split.\\n          (default: 1)\\n        :param minInfoGain:\\n          Minimum info gain required to create a split.\\n          (default: 0.0)\\n        :return:\\n          DecisionTreeModel.\\n\\n        Example usage:\\n\\n        >>> from numpy import array\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> from pyspark.mllib.tree import DecisionTree\\n        >>>\\n        >>> data = [\\n        ...     LabeledPoint(0.0, [0.0]),\\n        ...     LabeledPoint(1.0, [1.0]),\\n        ...     LabeledPoint(1.0, [2.0]),\\n        ...     LabeledPoint(1.0, [3.0])\\n        ... ]\\n        >>> model = DecisionTree.trainClassifier(sc.parallelize(data), 2, {})\\n        >>> print(model)\\n        DecisionTreeModel classifier of depth 1 with 3 nodes\\n\\n        >>> print(model.toDebugString())\\n        DecisionTreeModel classifier of depth 1 with 3 nodes\\n          If (feature 0 <= 0.5)\\n           Predict: 0.0\\n          Else (feature 0 > 0.5)\\n           Predict: 1.0\\n        <BLANKLINE>\\n        >>> model.predict(array([1.0]))\\n        1.0\\n        >>> model.predict(array([0.0]))\\n        0.0\\n        >>> rdd = sc.parallelize([[1.0], [0.0]])\\n        >>> model.predict(rdd).collect()\\n        [1.0, 0.0]', 'output': 'def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo,\\n                        impurity=\"gini\", maxDepth=5, maxBins=32, minInstancesPerNode=1,\\n                        minInfoGain=0.0)'}\n",
      "{'input': 'Train a decision tree model for regression.\\n\\n        :param data:\\n          Training data: RDD of LabeledPoint. Labels are real numbers.\\n        :param categoricalFeaturesInfo:\\n          Map storing arity of categorical features. An entry (n -> k)\\n          indicates that feature n is categorical with k categories\\n          indexed from 0: {0, 1, ..., k-1}.\\n        :param impurity:\\n          Criterion used for information gain calculation.\\n          The only supported value for regression is \"variance\".\\n          (default: \"variance\")\\n        :param maxDepth:\\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\\n          means 1 internal node + 2 leaf nodes).\\n          (default: 5)\\n        :param maxBins:\\n          Number of bins used for finding splits at each node.\\n          (default: 32)\\n        :param minInstancesPerNode:\\n          Minimum number of instances required at child nodes to create\\n          the parent split.\\n          (default: 1)\\n        :param minInfoGain:\\n          Minimum info gain required to create a split.\\n          (default: 0.0)\\n        :return:\\n          DecisionTreeModel.\\n\\n        Example usage:\\n\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> from pyspark.mllib.tree import DecisionTree\\n        >>> from pyspark.mllib.linalg import SparseVector\\n        >>>\\n        >>> sparse_data = [\\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 0.0})),\\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\\n        ... ]\\n        >>>\\n        >>> model = DecisionTree.trainRegressor(sc.parallelize(sparse_data), {})\\n        >>> model.predict(SparseVector(2, {1: 1.0}))\\n        1.0\\n        >>> model.predict(SparseVector(2, {1: 0.0}))\\n        0.0\\n        >>> rdd = sc.parallelize([[0.0, 1.0], [0.0, 0.0]])\\n        >>> model.predict(rdd).collect()\\n        [1.0, 0.0]', 'output': 'def trainRegressor(cls, data, categoricalFeaturesInfo,\\n                       impurity=\"variance\", maxDepth=5, maxBins=32, minInstancesPerNode=1,\\n                       minInfoGain=0.0)'}\n",
      "{'input': 'Train a random forest model for binary or multiclass\\n        classification.\\n\\n        :param data:\\n          Training dataset: RDD of LabeledPoint. Labels should take values\\n          {0, 1, ..., numClasses-1}.\\n        :param numClasses:\\n          Number of classes for classification.\\n        :param categoricalFeaturesInfo:\\n          Map storing arity of categorical features. An entry (n -> k)\\n          indicates that feature n is categorical with k categories\\n          indexed from 0: {0, 1, ..., k-1}.\\n        :param numTrees:\\n          Number of trees in the random forest.\\n        :param featureSubsetStrategy:\\n          Number of features to consider for splits at each node.\\n          Supported values: \"auto\", \"all\", \"sqrt\", \"log2\", \"onethird\".\\n          If \"auto\" is set, this parameter is set based on numTrees:\\n          if numTrees == 1, set to \"all\";\\n          if numTrees > 1 (forest) set to \"sqrt\".\\n          (default: \"auto\")\\n        :param impurity:\\n          Criterion used for information gain calculation.\\n          Supported values: \"gini\" or \"entropy\".\\n          (default: \"gini\")\\n        :param maxDepth:\\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\\n          means 1 internal node + 2 leaf nodes).\\n          (default: 4)\\n        :param maxBins:\\n          Maximum number of bins used for splitting features.\\n          (default: 32)\\n        :param seed:\\n          Random seed for bootstrapping and choosing feature subsets.\\n          Set as None to generate seed based on system time.\\n          (default: None)\\n        :return:\\n          RandomForestModel that can be used for prediction.\\n\\n        Example usage:\\n\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> from pyspark.mllib.tree import RandomForest\\n        >>>\\n        >>> data = [\\n        ...     LabeledPoint(0.0, [0.0]),\\n        ...     LabeledPoint(0.0, [1.0]),\\n        ...     LabeledPoint(1.0, [2.0]),\\n        ...     LabeledPoint(1.0, [3.0])\\n        ... ]\\n        >>> model = RandomForest.trainClassifier(sc.parallelize(data), 2, {}, 3, seed=42)\\n        >>> model.numTrees()\\n        3\\n        >>> model.totalNumNodes()\\n        7\\n        >>> print(model)\\n        TreeEnsembleModel classifier with 3 trees\\n        <BLANKLINE>\\n        >>> print(model.toDebugString())\\n        TreeEnsembleModel classifier with 3 trees\\n        <BLANKLINE>\\n          Tree 0:\\n            Predict: 1.0\\n          Tree 1:\\n            If (feature 0 <= 1.5)\\n             Predict: 0.0\\n            Else (feature 0 > 1.5)\\n             Predict: 1.0\\n          Tree 2:\\n            If (feature 0 <= 1.5)\\n             Predict: 0.0\\n            Else (feature 0 > 1.5)\\n             Predict: 1.0\\n        <BLANKLINE>\\n        >>> model.predict([2.0])\\n        1.0\\n        >>> model.predict([0.0])\\n        0.0\\n        >>> rdd = sc.parallelize([[3.0], [1.0]])\\n        >>> model.predict(rdd).collect()\\n        [1.0, 0.0]', 'output': 'def trainClassifier(cls, data, numClasses, categoricalFeaturesInfo, numTrees,\\n                        featureSubsetStrategy=\"auto\", impurity=\"gini\", maxDepth=4, maxBins=32,\\n                        seed=None)'}\n",
      "{'input': 'Train a random forest model for regression.\\n\\n        :param data:\\n          Training dataset: RDD of LabeledPoint. Labels are real numbers.\\n        :param categoricalFeaturesInfo:\\n          Map storing arity of categorical features. An entry (n -> k)\\n          indicates that feature n is categorical with k categories\\n          indexed from 0: {0, 1, ..., k-1}.\\n        :param numTrees:\\n          Number of trees in the random forest.\\n        :param featureSubsetStrategy:\\n          Number of features to consider for splits at each node.\\n          Supported values: \"auto\", \"all\", \"sqrt\", \"log2\", \"onethird\".\\n          If \"auto\" is set, this parameter is set based on numTrees:\\n          if numTrees == 1, set to \"all\";\\n          if numTrees > 1 (forest) set to \"onethird\" for regression.\\n          (default: \"auto\")\\n        :param impurity:\\n          Criterion used for information gain calculation.\\n          The only supported value for regression is \"variance\".\\n          (default: \"variance\")\\n        :param maxDepth:\\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\\n          means 1 internal node + 2 leaf nodes).\\n          (default: 4)\\n        :param maxBins:\\n          Maximum number of bins used for splitting features.\\n          (default: 32)\\n        :param seed:\\n          Random seed for bootstrapping and choosing feature subsets.\\n          Set as None to generate seed based on system time.\\n          (default: None)\\n        :return:\\n          RandomForestModel that can be used for prediction.\\n\\n        Example usage:\\n\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> from pyspark.mllib.tree import RandomForest\\n        >>> from pyspark.mllib.linalg import SparseVector\\n        >>>\\n        >>> sparse_data = [\\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),\\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 1.0})),\\n        ...     LabeledPoint(0.0, SparseVector(2, {0: 1.0})),\\n        ...     LabeledPoint(1.0, SparseVector(2, {1: 2.0}))\\n        ... ]\\n        >>>\\n        >>> model = RandomForest.trainRegressor(sc.parallelize(sparse_data), {}, 2, seed=42)\\n        >>> model.numTrees()\\n        2\\n        >>> model.totalNumNodes()\\n        4\\n        >>> model.predict(SparseVector(2, {1: 1.0}))\\n        1.0\\n        >>> model.predict(SparseVector(2, {0: 1.0}))\\n        0.5\\n        >>> rdd = sc.parallelize([[0.0, 1.0], [1.0, 0.0]])\\n        >>> model.predict(rdd).collect()\\n        [1.0, 0.5]', 'output': 'def trainRegressor(cls, data, categoricalFeaturesInfo, numTrees, featureSubsetStrategy=\"auto\",\\n                       impurity=\"variance\", maxDepth=4, maxBins=32, seed=None)'}\n",
      "{'input': 'Train a gradient-boosted trees model for classification.\\n\\n        :param data:\\n          Training dataset: RDD of LabeledPoint. Labels should take values\\n          {0, 1}.\\n        :param categoricalFeaturesInfo:\\n          Map storing arity of categorical features. An entry (n -> k)\\n          indicates that feature n is categorical with k categories\\n          indexed from 0: {0, 1, ..., k-1}.\\n        :param loss:\\n          Loss function used for minimization during gradient boosting.\\n          Supported values: \"logLoss\", \"leastSquaresError\",\\n          \"leastAbsoluteError\".\\n          (default: \"logLoss\")\\n        :param numIterations:\\n          Number of iterations of boosting.\\n          (default: 100)\\n        :param learningRate:\\n          Learning rate for shrinking the contribution of each estimator.\\n          The learning rate should be between in the interval (0, 1].\\n          (default: 0.1)\\n        :param maxDepth:\\n          Maximum depth of tree (e.g. depth 0 means 1 leaf node, depth 1\\n          means 1 internal node + 2 leaf nodes).\\n          (default: 3)\\n        :param maxBins:\\n          Maximum number of bins used for splitting features. DecisionTree\\n          requires maxBins >= max categories.\\n          (default: 32)\\n        :return:\\n          GradientBoostedTreesModel that can be used for prediction.\\n\\n        Example usage:\\n\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> from pyspark.mllib.tree import GradientBoostedTrees\\n        >>>\\n        >>> data = [\\n        ...     LabeledPoint(0.0, [0.0]),\\n        ...     LabeledPoint(0.0, [1.0]),\\n        ...     LabeledPoint(1.0, [2.0]),\\n        ...     LabeledPoint(1.0, [3.0])\\n        ... ]\\n        >>>\\n        >>> model = GradientBoostedTrees.trainClassifier(sc.parallelize(data), {}, numIterations=10)\\n        >>> model.numTrees()\\n        10\\n        >>> model.totalNumNodes()\\n        30\\n        >>> print(model)  # it already has newline\\n        TreeEnsembleModel classifier with 10 trees\\n        <BLANKLINE>\\n        >>> model.predict([2.0])\\n        1.0\\n        >>> model.predict([0.0])\\n        0.0\\n        >>> rdd = sc.parallelize([[2.0], [0.0]])\\n        >>> model.predict(rdd).collect()\\n        [1.0, 0.0]', 'output': 'def trainClassifier(cls, data, categoricalFeaturesInfo,\\n                        loss=\"logLoss\", numIterations=100, learningRate=0.1, maxDepth=3,\\n                        maxBins=32)'}\n",
      "{'input': 'Set a configuration property.', 'output': 'def set(self, key, value)'}\n",
      "{'input': 'Set a configuration property, if not already set.', 'output': 'def setIfMissing(self, key, value)'}\n",
      "{'input': 'Set an environment variable to be passed to executors.', 'output': 'def setExecutorEnv(self, key=None, value=None, pairs=None)'}\n",
      "{'input': 'Set multiple parameters, passed as a list of key-value pairs.\\n\\n        :param pairs: list of key-value pairs to set', 'output': 'def setAll(self, pairs)'}\n",
      "{'input': 'Get the configured value for some key, or return a default otherwise.', 'output': 'def get(self, key, defaultValue=None)'}\n",
      "{'input': 'Get all values as a list of key-value pairs.', 'output': 'def getAll(self)'}\n",
      "{'input': 'Does this configuration contain a given key?', 'output': 'def contains(self, key)'}\n",
      "{'input': 'Returns a printable version of the configuration, as a list of\\n        key=value pairs, one per line.', 'output': 'def toDebugString(self)'}\n",
      "{'input': 'Returns a list of databases available across all sessions.', 'output': 'def listDatabases(self)'}\n",
      "{'input': 'Returns a list of tables/views in the specified database.\\n\\n        If no database is specified, the current database is used.\\n        This includes all temporary views.', 'output': 'def listTables(self, dbName=None)'}\n",
      "{'input': 'Returns a list of functions registered in the specified database.\\n\\n        If no database is specified, the current database is used.\\n        This includes all temporary functions.', 'output': 'def listFunctions(self, dbName=None)'}\n",
      "{'input': 'Returns a list of columns for the given table/view in the specified database.\\n\\n        If no database is specified, the current database is used.\\n\\n        Note: the order of arguments here is different from that of its JVM counterpart\\n        because Python does not support method overloading.', 'output': 'def listColumns(self, tableName, dbName=None)'}\n",
      "{'input': 'Creates a table based on the dataset in a data source.\\n\\n        It returns the DataFrame associated with the external table.\\n\\n        The data source is specified by the ``source`` and a set of ``options``.\\n        If ``source`` is not specified, the default data source configured by\\n        ``spark.sql.sources.default`` will be used.\\n\\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\\n        created external table.\\n\\n        :return: :class:`DataFrame`', 'output': 'def createExternalTable(self, tableName, path=None, source=None, schema=None, **options)'}\n",
      "{'input': 'Creates a table based on the dataset in a data source.\\n\\n        It returns the DataFrame associated with the table.\\n\\n        The data source is specified by the ``source`` and a set of ``options``.\\n        If ``source`` is not specified, the default data source configured by\\n        ``spark.sql.sources.default`` will be used. When ``path`` is specified, an external table is\\n        created from the data at the given path. Otherwise a managed table is created.\\n\\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\\n        created table.\\n\\n        :return: :class:`DataFrame`', 'output': 'def createTable(self, tableName, path=None, source=None, schema=None, **options)'}\n",
      "{'input': 'Load data from a given socket, this is a blocking method thus only return when the socket\\n    connection has been closed.', 'output': 'def _load_from_socket(port, auth_secret)'}\n",
      "{'input': 'Internal function to get or create global BarrierTaskContext. We need to make sure\\n        BarrierTaskContext is returned from here because it is needed in python worker reuse\\n        scenario, see SPARK-25921 for more details.', 'output': 'def _getOrCreate(cls)'}\n",
      "{'input': 'Initialize BarrierTaskContext, other methods within BarrierTaskContext can only be called\\n        after BarrierTaskContext is initialized.', 'output': 'def _initialize(cls, port, secret)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Sets a global barrier and waits until all tasks in this stage hit this barrier.\\n        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks\\n        in the same stage have reached this routine.\\n\\n        .. warning:: In a barrier stage, each task much have the same number of `barrier()`\\n            calls, in all possible code branches.\\n            Otherwise, you may get the job hanging or a SparkException after timeout.\\n\\n        .. versionadded:: 2.4.0', 'output': 'def barrier(self)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Returns :class:`BarrierTaskInfo` for all tasks in this barrier stage,\\n        ordered by partition ID.\\n\\n        .. versionadded:: 2.4.0', 'output': 'def getTaskInfos(self)'}\n",
      "{'input': 'A decorator that annotates a function to append the version of Spark the function was added.', 'output': 'def since(version)'}\n",
      "{'input': 'Returns a function with same code, globals, defaults, closure, and\\n    name (or provide a new name).', 'output': 'def copy_func(f, name=None, sinceversion=None, doc=None)'}\n",
      "{'input': 'A decorator that forces keyword arguments in the wrapped method\\n    and saves actual input keyword arguments in `_input_kwargs`.\\n\\n    .. note:: Should only be used to wrap a method where first arg is `self`', 'output': 'def keyword_only(func)'}\n",
      "{'input': 'Generates the header part for shared variables\\n\\n    :param name: param name\\n    :param doc: param doc', 'output': 'def _gen_param_header(name, doc, defaultValueStr, typeConverter)'}\n",
      "{'input': 'Generates Python code for a shared param class.\\n\\n    :param name: param name\\n    :param doc: param doc\\n    :param defaultValueStr: string representation of the default value\\n    :return: code string', 'output': 'def _gen_param_code(name, doc, defaultValueStr)'}\n",
      "{'input': 'Runs the bisecting k-means algorithm return the model.\\n\\n        :param rdd:\\n          Training points as an `RDD` of `Vector` or convertible\\n          sequence types.\\n        :param k:\\n          The desired number of leaf clusters. The actual number could\\n          be smaller if there are no divisible leaf clusters.\\n          (default: 4)\\n        :param maxIterations:\\n          Maximum number of iterations allowed to split clusters.\\n          (default: 20)\\n        :param minDivisibleClusterSize:\\n          Minimum number of points (if >= 1.0) or the minimum proportion\\n          of points (if < 1.0) of a divisible cluster.\\n          (default: 1)\\n        :param seed:\\n          Random seed value for cluster initialization.\\n          (default: -1888008604 from classOf[BisectingKMeans].getName.##)', 'output': 'def train(self, rdd, k=4, maxIterations=20, minDivisibleClusterSize=1.0, seed=-1888008604)'}\n",
      "{'input': 'Train a k-means clustering model.\\n\\n        :param rdd:\\n          Training points as an `RDD` of `Vector` or convertible\\n          sequence types.\\n        :param k:\\n          Number of clusters to create.\\n        :param maxIterations:\\n          Maximum number of iterations allowed.\\n          (default: 100)\\n        :param runs:\\n          This param has no effect since Spark 2.0.0.\\n        :param initializationMode:\\n          The initialization algorithm. This can be either \"random\" or\\n          \"k-means||\".\\n          (default: \"k-means||\")\\n        :param seed:\\n          Random seed value for cluster initialization. Set as None to\\n          generate seed based on system time.\\n          (default: None)\\n        :param initializationSteps:\\n          Number of steps for the k-means|| initialization mode.\\n          This is an advanced setting -- the default of 2 is almost\\n          always enough.\\n          (default: 2)\\n        :param epsilon:\\n          Distance threshold within which a center will be considered to\\n          have converged. If all centers move less than this Euclidean\\n          distance, iterations are stopped.\\n          (default: 1e-4)\\n        :param initialModel:\\n          Initial cluster centers can be provided as a KMeansModel object\\n          rather than using the random or k-means|| initializationModel.\\n          (default: None)', 'output': 'def train(cls, rdd, k, maxIterations=100, runs=1, initializationMode=\"k-means||\",\\n              seed=None, initializationSteps=2, epsilon=1e-4, initialModel=None)'}\n",
      "{'input': 'Train a Gaussian Mixture clustering model.\\n\\n        :param rdd:\\n          Training points as an `RDD` of `Vector` or convertible\\n          sequence types.\\n        :param k:\\n          Number of independent Gaussians in the mixture model.\\n        :param convergenceTol:\\n          Maximum change in log-likelihood at which convergence is\\n          considered to have occurred.\\n          (default: 1e-3)\\n        :param maxIterations:\\n          Maximum number of iterations allowed.\\n          (default: 100)\\n        :param seed:\\n          Random seed for initial Gaussian distribution. Set as None to\\n          generate seed based on system time.\\n          (default: None)\\n        :param initialModel:\\n          Initial GMM starting point, bypassing the random\\n          initialization.\\n          (default: None)', 'output': 'def train(cls, rdd, k, convergenceTol=1e-3, maxIterations=100, seed=None, initialModel=None)'}\n",
      "{'input': 'Load a model from the given path.', 'output': 'def load(cls, sc, path)'}\n",
      "{'input': 'r\"\"\"\\n        :param rdd:\\n          An RDD of (i, j, s\\\\ :sub:`ij`\\\\) tuples representing the\\n          affinity matrix, which is the matrix A in the PIC paper.  The\\n          similarity s\\\\ :sub:`ij`\\\\ must be nonnegative.  This is a symmetric\\n          matrix and hence s\\\\ :sub:`ij`\\\\ = s\\\\ :sub:`ji`\\\\  For any (i, j) with\\n          nonzero similarity, there should be either (i, j, s\\\\ :sub:`ij`\\\\) or\\n          (j, i, s\\\\ :sub:`ji`\\\\) in the input.  Tuples with i = j are ignored,\\n          because it is assumed s\\\\ :sub:`ij`\\\\ = 0.0.\\n        :param k:\\n          Number of clusters.\\n        :param maxIterations:\\n          Maximum number of iterations of the PIC algorithm.\\n          (default: 100)\\n        :param initMode:\\n          Initialization mode. This can be either \"random\" to use\\n          a random vector as vertex properties, or \"degree\" to use\\n          normalized sum similarities.\\n          (default: \"random\")', 'output': 'def train(cls, rdd, k, maxIterations=100, initMode=\"random\")'}\n",
      "{'input': 'Update the centroids, according to data\\n\\n        :param data:\\n          RDD with new data for the model update.\\n        :param decayFactor:\\n          Forgetfulness of the previous centroids.\\n        :param timeUnit:\\n          Can be \"batches\" or \"points\". If points, then the decay factor\\n          is raised to the power of number of new points and if batches,\\n          then decay factor will be used as is.', 'output': 'def update(self, data, decayFactor, timeUnit)'}\n",
      "{'input': 'Set number of batches after which the centroids of that\\n        particular batch has half the weightage.', 'output': 'def setHalfLife(self, halfLife, timeUnit)'}\n",
      "{'input': 'Set initial centers. Should be set before calling trainOn.', 'output': 'def setInitialCenters(self, centers, weights)'}\n",
      "{'input': 'Set the initial centres to be random samples from\\n        a gaussian population with constant weights.', 'output': 'def setRandomCenters(self, dim, weight, seed)'}\n",
      "{'input': 'Train the model on the incoming dstream.', 'output': 'def trainOn(self, dstream)'}\n",
      "{'input': 'Make predictions on a dstream.\\n        Returns a transformed dstream object', 'output': 'def predictOn(self, dstream)'}\n",
      "{'input': 'Make predictions on a keyed dstream.\\n        Returns a transformed dstream object.', 'output': 'def predictOnValues(self, dstream)'}\n",
      "{'input': \"Return the topics described by weighted terms.\\n\\n        WARNING: If vocabSize and k are large, this can return a large object!\\n\\n        :param maxTermsPerTopic:\\n          Maximum number of terms to collect for each topic.\\n          (default: vocabulary size)\\n        :return:\\n          Array over topics. Each topic is represented as a pair of\\n          matching arrays: (term indices, term weights in topic).\\n          Each topic's terms are sorted in order of decreasing weight.\", 'output': 'def describeTopics(self, maxTermsPerTopic=None)'}\n",
      "{'input': 'Load the LDAModel from disk.\\n\\n        :param sc:\\n          SparkContext.\\n        :param path:\\n          Path to where the model is stored.', 'output': 'def load(cls, sc, path)'}\n",
      "{'input': 'Train a LDA model.\\n\\n        :param rdd:\\n          RDD of documents, which are tuples of document IDs and term\\n          (word) count vectors. The term count vectors are \"bags of\\n          words\" with a fixed-size vocabulary (where the vocabulary size\\n          is the length of the vector). Document IDs must be unique\\n          and >= 0.\\n        :param k:\\n          Number of topics to infer, i.e., the number of soft cluster\\n          centers.\\n          (default: 10)\\n        :param maxIterations:\\n          Maximum number of iterations allowed.\\n          (default: 20)\\n        :param docConcentration:\\n          Concentration parameter (commonly named \"alpha\") for the prior\\n          placed on documents\\' distributions over topics (\"theta\").\\n          (default: -1.0)\\n        :param topicConcentration:\\n          Concentration parameter (commonly named \"beta\" or \"eta\") for\\n          the prior placed on topics\\' distributions over terms.\\n          (default: -1.0)\\n        :param seed:\\n          Random seed for cluster initialization. Set as None to generate\\n          seed based on system time.\\n          (default: None)\\n        :param checkpointInterval:\\n          Period (in iterations) between checkpoints.\\n          (default: 10)\\n        :param optimizer:\\n          LDAOptimizer used to perform the actual calculation. Currently\\n          \"em\", \"online\" are supported.\\n          (default: \"em\")', 'output': 'def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,\\n              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer=\"em\")'}\n",
      "{'input': 'Return a JavaRDD of Object by unpickling\\n\\n    It will convert each Python object into Java object by Pyrolite, whenever the\\n    RDD is serialized in batch or not.', 'output': 'def _to_java_object_rdd(rdd)'}\n",
      "{'input': 'Convert Python object into Java', 'output': 'def _py2java(sc, obj)'}\n",
      "{'input': 'Call Java Function', 'output': 'def callJavaFunc(sc, func, *args)'}\n",
      "{'input': 'Call API in PythonMLLibAPI', 'output': 'def callMLlibFunc(name, *args)'}\n",
      "{'input': 'A decorator that makes a class inherit documentation from its parents.', 'output': 'def inherit_doc(cls)'}\n",
      "{'input': 'Call method of java_model', 'output': 'def call(self, name, *a)'}\n",
      "{'input': 'Return a new DStream in which each RDD has a single element\\n        generated by counting each RDD of this DStream.', 'output': 'def count(self)'}\n",
      "{'input': 'Return a new DStream containing only the elements that satisfy predicate.', 'output': 'def filter(self, f)'}\n",
      "{'input': 'Return a new DStream by applying a function to each element of DStream.', 'output': 'def map(self, f, preservesPartitioning=False)'}\n",
      "{'input': 'Return a new DStream in which each RDD is generated by applying\\n        mapPartitionsWithIndex() to each RDDs of this DStream.', 'output': 'def mapPartitionsWithIndex(self, f, preservesPartitioning=False)'}\n",
      "{'input': 'Return a new DStream in which each RDD has a single element\\n        generated by reducing each RDD of this DStream.', 'output': 'def reduce(self, func)'}\n",
      "{'input': 'Return a new DStream by applying reduceByKey to each RDD.', 'output': 'def reduceByKey(self, func, numPartitions=None)'}\n",
      "{'input': 'Return a new DStream by applying combineByKey to each RDD.', 'output': 'def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\\n                     numPartitions=None)'}\n",
      "{'input': 'Return a copy of the DStream in which each RDD are partitioned\\n        using the specified partitioner.', 'output': 'def partitionBy(self, numPartitions, partitionFunc=portable_hash)'}\n",
      "{'input': 'Apply a function to each RDD in this DStream.', 'output': 'def foreachRDD(self, func)'}\n",
      "{'input': 'Print the first num elements of each RDD generated in this DStream.\\n\\n        @param num: the number of elements from the first will be printed.', 'output': 'def pprint(self, num=10)'}\n",
      "{'input': 'Persist the RDDs of this DStream with the given storage level', 'output': 'def persist(self, storageLevel)'}\n",
      "{'input': 'Enable periodic checkpointing of RDDs of this DStream\\n\\n        @param interval: time in seconds, after each period of that, generated\\n                         RDD will be checkpointed', 'output': 'def checkpoint(self, interval)'}\n",
      "{'input': 'Return a new DStream by applying groupByKey on each RDD.', 'output': 'def groupByKey(self, numPartitions=None)'}\n",
      "{'input': 'Return a new DStream in which each RDD contains the counts of each\\n        distinct value in each RDD of this DStream.', 'output': 'def countByValue(self)'}\n",
      "{'input': 'Save each RDD in this DStream as at text file, using string\\n        representation of elements.', 'output': 'def saveAsTextFiles(self, prefix, suffix=None)'}\n",
      "{'input': 'Return a new DStream in which each RDD is generated by applying a function\\n        on each RDD of this DStream.\\n\\n        `func` can have one argument of `rdd`, or have two arguments of\\n        (`time`, `rdd`)', 'output': 'def transform(self, func)'}\n",
      "{'input': \"Return a new DStream in which each RDD is generated by applying a function\\n        on each RDD of this DStream and 'other' DStream.\\n\\n        `func` can have two arguments of (`rdd_a`, `rdd_b`) or have three\\n        arguments of (`time`, `rdd_a`, `rdd_b`)\", 'output': 'def transformWith(self, func, other, keepSerializer=False)'}\n",
      "{'input': 'Return a new DStream by unifying data of another DStream with this DStream.\\n\\n        @param other: Another DStream having the same interval (i.e., slideDuration)\\n                     as this DStream.', 'output': 'def union(self, other)'}\n",
      "{'input': \"Return a new DStream by applying 'cogroup' between RDDs of this\\n        DStream and `other` DStream.\\n\\n        Hash partitioning is used to generate the RDDs with `numPartitions` partitions.\", 'output': 'def cogroup(self, other, numPartitions=None)'}\n",
      "{'input': 'Convert datetime or unix_timestamp into Time', 'output': 'def _jtime(self, timestamp)'}\n",
      "{'input': \"Return all the RDDs between 'begin' to 'end' (both included)\\n\\n        `begin`, `end` could be datetime.datetime() or unix_timestamp\", 'output': 'def slice(self, begin, end)'}\n",
      "{'input': \"Return a new DStream in which each RDD contains all the elements in seen in a\\n        sliding window of time over this DStream.\\n\\n        @param windowDuration: width of the window; must be a multiple of this DStream's\\n                              batching interval\\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\\n                              the new DStream will generate RDDs); must be a multiple of this\\n                              DStream's batching interval\", 'output': 'def window(self, windowDuration, slideDuration=None)'}\n",
      "{'input': 'Return a new DStream in which each RDD has a single element generated by reducing all\\n        elements in a sliding window over this DStream.\\n\\n        if `invReduceFunc` is not None, the reduction is done incrementally\\n        using the old window\\'s reduced value :\\n\\n        1. reduce the new values that entered the window (e.g., adding new counts)\\n\\n        2. \"inverse reduce\" the old values that left the window (e.g., subtracting old counts)\\n        This is more efficient than `invReduceFunc` is None.\\n\\n        @param reduceFunc:     associative and commutative reduce function\\n        @param invReduceFunc:  inverse reduce function of `reduceFunc`; such that for all y,\\n                               and invertible x:\\n                               `invReduceFunc(reduceFunc(x, y), x) = y`\\n        @param windowDuration: width of the window; must be a multiple of this DStream\\'s\\n                               batching interval\\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\\n                               the new DStream will generate RDDs); must be a multiple of this\\n                               DStream\\'s batching interval', 'output': 'def reduceByWindow(self, reduceFunc, invReduceFunc, windowDuration, slideDuration)'}\n",
      "{'input': 'Return a new DStream in which each RDD has a single element generated\\n        by counting the number of elements in a window over this DStream.\\n        windowDuration and slideDuration are as defined in the window() operation.\\n\\n        This is equivalent to window(windowDuration, slideDuration).count(),\\n        but will be more efficient if window is large.', 'output': 'def countByWindow(self, windowDuration, slideDuration)'}\n",
      "{'input': \"Return a new DStream in which each RDD contains the count of distinct elements in\\n        RDDs in a sliding window over this DStream.\\n\\n        @param windowDuration: width of the window; must be a multiple of this DStream's\\n                              batching interval\\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\\n                              the new DStream will generate RDDs); must be a multiple of this\\n                              DStream's batching interval\\n        @param numPartitions:  number of partitions of each RDD in the new DStream.\", 'output': 'def countByValueAndWindow(self, windowDuration, slideDuration, numPartitions=None)'}\n",
      "{'input': \"Return a new DStream by applying `groupByKey` over a sliding window.\\n        Similar to `DStream.groupByKey()`, but applies it over a sliding window.\\n\\n        @param windowDuration: width of the window; must be a multiple of this DStream's\\n                              batching interval\\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\\n                              the new DStream will generate RDDs); must be a multiple of this\\n                              DStream's batching interval\\n        @param numPartitions:  Number of partitions of each RDD in the new DStream.\", 'output': 'def groupByKeyAndWindow(self, windowDuration, slideDuration, numPartitions=None)'}\n",
      "{'input': 'Return a new DStream by applying incremental `reduceByKey` over a sliding window.\\n\\n        The reduced value of over a new window is calculated using the old window\\'s reduce value :\\n         1. reduce the new values that entered the window (e.g., adding new counts)\\n         2. \"inverse reduce\" the old values that left the window (e.g., subtracting old counts)\\n\\n        `invFunc` can be None, then it will reduce all the RDDs in window, could be slower\\n        than having `invFunc`.\\n\\n        @param func:           associative and commutative reduce function\\n        @param invFunc:        inverse function of `reduceFunc`\\n        @param windowDuration: width of the window; must be a multiple of this DStream\\'s\\n                              batching interval\\n        @param slideDuration:  sliding interval of the window (i.e., the interval after which\\n                              the new DStream will generate RDDs); must be a multiple of this\\n                              DStream\\'s batching interval\\n        @param numPartitions:  number of partitions of each RDD in the new DStream.\\n        @param filterFunc:     function to filter expired key-value pairs;\\n                              only pairs that satisfy the function are retained\\n                              set this to null if you do not want to filter', 'output': 'def reduceByKeyAndWindow(self, func, invFunc, windowDuration, slideDuration=None,\\n                             numPartitions=None, filterFunc=None)'}\n",
      "{'input': 'Return a new \"state\" DStream where the state for each key is updated by applying\\n        the given function on the previous state of the key and the new values of the key.\\n\\n        @param updateFunc: State update function. If this function returns None, then\\n                           corresponding state key-value pair will be eliminated.', 'output': 'def updateStateByKey(self, updateFunc, numPartitions=None, initialRDD=None)'}\n",
      "{'input': 'setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol=\"items\", \\\\\\n                  predictionCol=\"prediction\", numPartitions=None)', 'output': 'def setParams(self, minSupport=0.3, minConfidence=0.8, itemsCol=\"items\",\\n                  predictionCol=\"prediction\", numPartitions=None)'}\n",
      "{'input': 'setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000, \\\\\\n                  sequenceCol=\"sequence\")', 'output': 'def setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000,\\n                  sequenceCol=\"sequence\")'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Finds the complete set of frequent sequential patterns in the input sequences of itemsets.\\n\\n        :param dataset: A dataframe containing a sequence column which is\\n                        `ArrayType(ArrayType(T))` type, T is the item type for the input dataset.\\n        :return: A `DataFrame` that contains columns of sequence and corresponding frequency.\\n                 The schema of it will be:\\n                 - `sequence: ArrayType(ArrayType(T))` (T is the item type)\\n                 - `freq: Long`\\n\\n        >>> from pyspark.ml.fpm import PrefixSpan\\n        >>> from pyspark.sql import Row\\n        >>> df = sc.parallelize([Row(sequence=[[1, 2], [3]]),\\n        ...                      Row(sequence=[[1], [3, 2], [1, 2]]),\\n        ...                      Row(sequence=[[1, 2], [5]]),\\n        ...                      Row(sequence=[[6]])]).toDF()\\n        >>> prefixSpan = PrefixSpan(minSupport=0.5, maxPatternLength=5)\\n        >>> prefixSpan.findFrequentSequentialPatterns(df).sort(\"sequence\").show(truncate=False)\\n        +----------+----+\\n        |sequence  |freq|\\n        +----------+----+\\n        |[[1]]     |3   |\\n        |[[1], [3]]|2   |\\n        |[[1, 2]]  |3   |\\n        |[[2]]     |3   |\\n        |[[3]]     |2   |\\n        +----------+----+\\n\\n        .. versionadded:: 2.4.0', 'output': 'def findFrequentSequentialPatterns(self, dataset)'}\n",
      "{'input': 'Return a CallSite representing the first Spark call in the current call stack.', 'output': 'def first_spark_call()'}\n",
      "{'input': 'Parse a line of text into an MLlib LabeledPoint object.', 'output': 'def parsePoint(line)'}\n",
      "{'input': 'Returns f-measure.', 'output': 'def fMeasure(self, label, beta=None)'}\n",
      "{'input': 'Returns precision or precision for a given label (category) if specified.', 'output': 'def precision(self, label=None)'}\n",
      "{'input': 'Returns recall or recall for a given label (category) if specified.', 'output': 'def recall(self, label=None)'}\n",
      "{'input': 'Returns f1Measure or f1Measure for a given label (category) if specified.', 'output': 'def f1Measure(self, label=None)'}\n",
      "{'input': 'When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong.\\n    This method gets the corrected data type for Pandas if that type may be inferred uncorrectly.', 'output': 'def _to_corrected_pandas_type(dt)'}\n",
      "{'input': 'Returns the content as an :class:`pyspark.RDD` of :class:`Row`.', 'output': 'def rdd(self)'}\n",
      "{'input': 'Converts a :class:`DataFrame` into a :class:`RDD` of string.\\n\\n        Each row is turned into a JSON document as one element in the returned RDD.\\n\\n        >>> df.toJSON().first()\\n        u\\'{\"age\":2,\"name\":\"Alice\"}\\'', 'output': 'def toJSON(self, use_unicode=True)'}\n",
      "{'input': 'Returns the schema of this :class:`DataFrame` as a :class:`pyspark.sql.types.StructType`.\\n\\n        >>> df.schema\\n        StructType(List(StructField(age,IntegerType,true),StructField(name,StringType,true)))', 'output': 'def schema(self)'}\n",
      "{'input': 'Prints the (logical and physical) plans to the console for debugging purpose.\\n\\n        :param extended: boolean, default ``False``. If ``False``, prints only the physical plan.\\n\\n        >>> df.explain()\\n        == Physical Plan ==\\n        *(1) Scan ExistingRDD[age#0,name#1]\\n\\n        >>> df.explain(True)\\n        == Parsed Logical Plan ==\\n        ...\\n        == Analyzed Logical Plan ==\\n        ...\\n        == Optimized Logical Plan ==\\n        ...\\n        == Physical Plan ==\\n        ...', 'output': 'def explain(self, extended=False)'}\n",
      "{'input': 'Return a new :class:`DataFrame` containing rows in this :class:`DataFrame` but\\n        not in another :class:`DataFrame` while preserving duplicates.\\n\\n        This is equivalent to `EXCEPT ALL` in SQL.\\n\\n        >>> df1 = spark.createDataFrame(\\n        ...         [(\"a\", 1), (\"a\", 1), (\"a\", 1), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\\n        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\\n\\n        >>> df1.exceptAll(df2).show()\\n        +---+---+\\n        | C1| C2|\\n        +---+---+\\n        |  a|  1|\\n        |  a|  1|\\n        |  a|  2|\\n        |  c|  4|\\n        +---+---+\\n\\n        Also as standard in SQL, this function resolves columns by position (not by name).', 'output': 'def exceptAll(self, other)'}\n",
      "{'input': 'Prints the first ``n`` rows to the console.\\n\\n        :param n: Number of rows to show.\\n        :param truncate: If set to True, truncate strings longer than 20 chars by default.\\n            If set to a number greater than one, truncates long strings to length ``truncate``\\n            and align cells right.\\n        :param vertical: If set to True, print output rows vertically (one line\\n            per column value).\\n\\n        >>> df\\n        DataFrame[age: int, name: string]\\n        >>> df.show()\\n        +---+-----+\\n        |age| name|\\n        +---+-----+\\n        |  2|Alice|\\n        |  5|  Bob|\\n        +---+-----+\\n        >>> df.show(truncate=3)\\n        +---+----+\\n        |age|name|\\n        +---+----+\\n        |  2| Ali|\\n        |  5| Bob|\\n        +---+----+\\n        >>> df.show(vertical=True)\\n        -RECORD 0-----\\n         age  | 2\\n         name | Alice\\n        -RECORD 1-----\\n         age  | 5\\n         name | Bob', 'output': 'def show(self, n=20, truncate=True, vertical=False)'}\n",
      "{'input': \"Returns a dataframe with html code when you enabled eager evaluation\\n        by 'spark.sql.repl.eagerEval.enabled', this only called by REPL you are\\n        using support eager evaluation with HTML.\", 'output': 'def _repr_html_(self)'}\n",
      "{'input': 'Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the\\n        logical plan of this DataFrame, which is especially useful in iterative algorithms where the\\n        plan may grow exponentially. It will be saved to files inside the checkpoint\\n        directory set with L{SparkContext.setCheckpointDir()}.\\n\\n        :param eager: Whether to checkpoint this DataFrame immediately\\n\\n        .. note:: Experimental', 'output': 'def checkpoint(self, eager=True)'}\n",
      "{'input': 'Returns a locally checkpointed version of this Dataset. Checkpointing can be used to\\n        truncate the logical plan of this DataFrame, which is especially useful in iterative\\n        algorithms where the plan may grow exponentially. Local checkpoints are stored in the\\n        executors using the caching subsystem and therefore they are not reliable.\\n\\n        :param eager: Whether to checkpoint this DataFrame immediately\\n\\n        .. note:: Experimental', 'output': 'def localCheckpoint(self, eager=True)'}\n",
      "{'input': 'Defines an event time watermark for this :class:`DataFrame`. A watermark tracks a point\\n        in time before which we assume no more late data is going to arrive.\\n\\n        Spark will use this watermark for several purposes:\\n          - To know when a given time window aggregation can be finalized and thus can be emitted\\n            when using output modes that do not allow updates.\\n\\n          - To minimize the amount of state that we need to keep for on-going aggregations.\\n\\n        The current watermark is computed by looking at the `MAX(eventTime)` seen across\\n        all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost\\n        of coordinating this value across partitions, the actual watermark used is only guaranteed\\n        to be at least `delayThreshold` behind the actual event time.  In some cases we may still\\n        process records that arrive more than `delayThreshold` late.\\n\\n        :param eventTime: the name of the column that contains the event time of the row.\\n        :param delayThreshold: the minimum delay to wait to data to arrive late, relative to the\\n            latest record that has been processed in the form of an interval\\n            (e.g. \"1 minute\" or \"5 hours\").\\n\\n        .. note:: Evolving\\n\\n        >>> sdf.select(\\'name\\', sdf.time.cast(\\'timestamp\\')).withWatermark(\\'time\\', \\'10 minutes\\')\\n        DataFrame[name: string, time: timestamp]', 'output': 'def withWatermark(self, eventTime, delayThreshold)'}\n",
      "{'input': 'Specifies some hint on the current DataFrame.\\n\\n        :param name: A name of the hint.\\n        :param parameters: Optional parameters.\\n        :return: :class:`DataFrame`\\n\\n        >>> df.join(df2.hint(\"broadcast\"), \"name\").show()\\n        +----+---+------+\\n        |name|age|height|\\n        +----+---+------+\\n        | Bob|  5|    85|\\n        +----+---+------+', 'output': 'def hint(self, name, *parameters)'}\n",
      "{'input': \"Returns all the records as a list of :class:`Row`.\\n\\n        >>> df.collect()\\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\", 'output': 'def collect(self)'}\n",
      "{'input': \"Returns an iterator that contains all of the rows in this :class:`DataFrame`.\\n        The iterator will consume as much memory as the largest partition in this DataFrame.\\n\\n        >>> list(df.toLocalIterator())\\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\", 'output': 'def toLocalIterator(self)'}\n",
      "{'input': \"Limits the result count to the number specified.\\n\\n        >>> df.limit(1).collect()\\n        [Row(age=2, name=u'Alice')]\\n        >>> df.limit(0).collect()\\n        []\", 'output': 'def limit(self, num)'}\n",
      "{'input': 'Sets the storage level to persist the contents of the :class:`DataFrame` across\\n        operations after the first time it is computed. This can only be used to assign\\n        a new storage level if the :class:`DataFrame` does not have a storage level set yet.\\n        If no storage level is specified defaults to (C{MEMORY_AND_DISK}).\\n\\n        .. note:: The default storage level has changed to C{MEMORY_AND_DISK} to match Scala in 2.0.', 'output': 'def persist(self, storageLevel=StorageLevel.MEMORY_AND_DISK)'}\n",
      "{'input': \"Get the :class:`DataFrame`'s current storage level.\\n\\n        >>> df.storageLevel\\n        StorageLevel(False, False, False, False, 1)\\n        >>> df.cache().storageLevel\\n        StorageLevel(True, True, False, True, 1)\\n        >>> df2.persist(StorageLevel.DISK_ONLY_2).storageLevel\\n        StorageLevel(True, False, False, False, 2)\", 'output': 'def storageLevel(self)'}\n",
      "{'input': 'Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\\n        memory and disk.\\n\\n        .. note:: `blocking` default has changed to False to match Scala in 2.0.', 'output': 'def unpersist(self, blocking=False)'}\n",
      "{'input': \"Returns a new :class:`DataFrame` that has exactly `numPartitions` partitions.\\n\\n        :param numPartitions: int, to specify the target number of partitions\\n\\n        Similar to coalesce defined on an :class:`RDD`, this operation results in a\\n        narrow dependency, e.g. if you go from 1000 partitions to 100 partitions,\\n        there will not be a shuffle, instead each of the 100 new partitions will\\n        claim 10 of the current partitions. If a larger number of partitions is requested,\\n        it will stay at the current number of partitions.\\n\\n        However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,\\n        this may result in your computation taking place on fewer nodes than\\n        you like (e.g. one node in the case of numPartitions = 1). To avoid this,\\n        you can call repartition(). This will add a shuffle step, but means the\\n        current upstream partitions will be executed in parallel (per whatever\\n        the current partitioning is).\\n\\n        >>> df.coalesce(1).rdd.getNumPartitions()\\n        1\", 'output': 'def coalesce(self, numPartitions)'}\n",
      "{'input': 'Returns a new :class:`DataFrame` partitioned by the given partitioning expressions. The\\n        resulting DataFrame is hash partitioned.\\n\\n        :param numPartitions:\\n            can be an int to specify the target number of partitions or a Column.\\n            If it is a Column, it will be used as the first partitioning column. If not specified,\\n            the default number of partitions is used.\\n\\n        .. versionchanged:: 1.6\\n           Added optional arguments to specify the partitioning columns. Also made numPartitions\\n           optional if partitioning columns are specified.\\n\\n        >>> df.repartition(10).rdd.getNumPartitions()\\n        10\\n        >>> data = df.union(df).repartition(\"age\")\\n        >>> data.show()\\n        +---+-----+\\n        |age| name|\\n        +---+-----+\\n        |  5|  Bob|\\n        |  5|  Bob|\\n        |  2|Alice|\\n        |  2|Alice|\\n        +---+-----+\\n        >>> data = data.repartition(7, \"age\")\\n        >>> data.show()\\n        +---+-----+\\n        |age| name|\\n        +---+-----+\\n        |  2|Alice|\\n        |  5|  Bob|\\n        |  2|Alice|\\n        |  5|  Bob|\\n        +---+-----+\\n        >>> data.rdd.getNumPartitions()\\n        7\\n        >>> data = data.repartition(\"name\", \"age\")\\n        >>> data.show()\\n        +---+-----+\\n        |age| name|\\n        +---+-----+\\n        |  5|  Bob|\\n        |  5|  Bob|\\n        |  2|Alice|\\n        |  2|Alice|\\n        +---+-----+', 'output': 'def repartition(self, numPartitions, *cols)'}\n",
      "{'input': 'Returns a sampled subset of this :class:`DataFrame`.\\n\\n        :param withReplacement: Sample with replacement or not (default False).\\n        :param fraction: Fraction of rows to generate, range [0.0, 1.0].\\n        :param seed: Seed for sampling (default a random seed).\\n\\n        .. note:: This is not guaranteed to provide exactly the fraction specified of the total\\n            count of the given :class:`DataFrame`.\\n\\n        .. note:: `fraction` is required and, `withReplacement` and `seed` are optional.\\n\\n        >>> df = spark.range(10)\\n        >>> df.sample(0.5, 3).count()\\n        7\\n        >>> df.sample(fraction=0.5, seed=3).count()\\n        7\\n        >>> df.sample(withReplacement=True, fraction=0.5, seed=3).count()\\n        1\\n        >>> df.sample(1.0).count()\\n        10\\n        >>> df.sample(fraction=1.0).count()\\n        10\\n        >>> df.sample(False, fraction=1.0).count()\\n        10', 'output': 'def sample(self, withReplacement=None, fraction=None, seed=None)'}\n",
      "{'input': 'Returns a stratified sample without replacement based on the\\n        fraction given on each stratum.\\n\\n        :param col: column that defines strata\\n        :param fractions:\\n            sampling fraction for each stratum. If a stratum is not\\n            specified, we treat its fraction as zero.\\n        :param seed: random seed\\n        :return: a new DataFrame that represents the stratified sample\\n\\n        >>> from pyspark.sql.functions import col\\n        >>> dataset = sqlContext.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\\n        >>> sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\\n        >>> sampled.groupBy(\"key\").count().orderBy(\"key\").show()\\n        +---+-----+\\n        |key|count|\\n        +---+-----+\\n        |  0|    3|\\n        |  1|    6|\\n        +---+-----+\\n        >>> dataset.sampleBy(col(\"key\"), fractions={2: 1.0}, seed=0).count()\\n        33\\n\\n        .. versionchanged:: 3.0\\n           Added sampling by a column of :class:`Column`', 'output': 'def sampleBy(self, col, fractions, seed=None)'}\n",
      "{'input': \"Randomly splits this :class:`DataFrame` with the provided weights.\\n\\n        :param weights: list of doubles as weights with which to split the DataFrame. Weights will\\n            be normalized if they don't sum up to 1.0.\\n        :param seed: The seed for sampling.\\n\\n        >>> splits = df4.randomSplit([1.0, 2.0], 24)\\n        >>> splits[0].count()\\n        2\\n\\n        >>> splits[1].count()\\n        2\", 'output': 'def randomSplit(self, weights, seed=None)'}\n",
      "{'input': \"Returns all column names and their data types as a list.\\n\\n        >>> df.dtypes\\n        [('age', 'int'), ('name', 'string')]\", 'output': 'def dtypes(self)'}\n",
      "{'input': 'Selects column based on the column name specified as a regex and returns it\\n        as :class:`Column`.\\n\\n        :param colName: string, column name specified as a regex.\\n\\n        >>> df = spark.createDataFrame([(\"a\", 1), (\"b\", 2), (\"c\",  3)], [\"Col1\", \"Col2\"])\\n        >>> df.select(df.colRegex(\"`(Col1)?+.+`\")).show()\\n        +----+\\n        |Col2|\\n        +----+\\n        |   1|\\n        |   2|\\n        |   3|\\n        +----+', 'output': 'def colRegex(self, colName)'}\n",
      "{'input': 'Returns a new :class:`DataFrame` with an alias set.\\n\\n        :param alias: string, an alias name to be set for the DataFrame.\\n\\n        >>> from pyspark.sql.functions import *\\n        >>> df_as1 = df.alias(\"df_as1\")\\n        >>> df_as2 = df.alias(\"df_as2\")\\n        >>> joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), \\'inner\\')\\n        >>> joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()\\n        [Row(name=u\\'Bob\\', name=u\\'Bob\\', age=5), Row(name=u\\'Alice\\', name=u\\'Alice\\', age=2)]', 'output': 'def alias(self, alias)'}\n",
      "{'input': 'Returns the cartesian product with another :class:`DataFrame`.\\n\\n        :param other: Right side of the cartesian product.\\n\\n        >>> df.select(\"age\", \"name\").collect()\\n        [Row(age=2, name=u\\'Alice\\'), Row(age=5, name=u\\'Bob\\')]\\n        >>> df2.select(\"name\", \"height\").collect()\\n        [Row(name=u\\'Tom\\', height=80), Row(name=u\\'Bob\\', height=85)]\\n        >>> df.crossJoin(df2.select(\"height\")).select(\"age\", \"name\", \"height\").collect()\\n        [Row(age=2, name=u\\'Alice\\', height=80), Row(age=2, name=u\\'Alice\\', height=85),\\n         Row(age=5, name=u\\'Bob\\', height=80), Row(age=5, name=u\\'Bob\\', height=85)]', 'output': 'def crossJoin(self, other)'}\n",
      "{'input': \"Joins with another :class:`DataFrame`, using the given join expression.\\n\\n        :param other: Right side of the join\\n        :param on: a string for the join column name, a list of column names,\\n            a join expression (Column), or a list of Columns.\\n            If `on` is a string or a list of strings indicating the name of the join column(s),\\n            the column(s) must exist on both sides, and this performs an equi-join.\\n        :param how: str, default ``inner``. Must be one of: ``inner``, ``cross``, ``outer``,\\n            ``full``, ``fullouter``, ``full_outer``, ``left``, ``leftouter``, ``left_outer``,\\n            ``right``, ``rightouter``, ``right_outer``, ``semi``, ``leftsemi``, ``left_semi``,\\n            ``anti``, ``leftanti`` and ``left_anti``.\\n\\n        The following performs a full outer join between ``df1`` and ``df2``.\\n\\n        >>> df.join(df2, df.name == df2.name, 'outer').select(df.name, df2.height).collect()\\n        [Row(name=None, height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]\\n\\n        >>> df.join(df2, 'name', 'outer').select('name', 'height').collect()\\n        [Row(name=u'Tom', height=80), Row(name=u'Bob', height=85), Row(name=u'Alice', height=None)]\\n\\n        >>> cond = [df.name == df3.name, df.age == df3.age]\\n        >>> df.join(df3, cond, 'outer').select(df.name, df3.age).collect()\\n        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]\\n\\n        >>> df.join(df2, 'name').select(df.name, df2.height).collect()\\n        [Row(name=u'Bob', height=85)]\\n\\n        >>> df.join(df4, ['name', 'age']).select(df.name, df.age).collect()\\n        [Row(name=u'Bob', age=5)]\", 'output': 'def join(self, other, on=None, how=None)'}\n",
      "{'input': 'Returns a new :class:`DataFrame` with each partition sorted by the specified column(s).\\n\\n        :param cols: list of :class:`Column` or column names to sort by.\\n        :param ascending: boolean or list of boolean (default True).\\n            Sort ascending vs. descending. Specify list for multiple sort orders.\\n            If a list is specified, length of the list must equal length of the `cols`.\\n\\n        >>> df.sortWithinPartitions(\"age\", ascending=False).show()\\n        +---+-----+\\n        |age| name|\\n        +---+-----+\\n        |  2|Alice|\\n        |  5|  Bob|\\n        +---+-----+', 'output': 'def sortWithinPartitions(self, *cols, **kwargs)'}\n",
      "{'input': 'Return a JVM Seq of Columns from a list of Column or names', 'output': 'def _jseq(self, cols, converter=None)'}\n",
      "{'input': 'Return a JVM Seq of Columns from a list of Column or column names\\n\\n        If `cols` has only one list in it, cols[0] will be used as the list.', 'output': 'def _jcols(self, *cols)'}\n",
      "{'input': 'Return a JVM Seq of Columns that describes the sort order', 'output': 'def _sort_cols(self, cols, kwargs)'}\n",
      "{'input': \"Computes basic statistics for numeric and string columns.\\n\\n        This include count, mean, stddev, min, and max. If no columns are\\n        given, this function computes statistics for all numerical or string columns.\\n\\n        .. note:: This function is meant for exploratory data analysis, as we make no\\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\\n\\n        >>> df.describe(['age']).show()\\n        +-------+------------------+\\n        |summary|               age|\\n        +-------+------------------+\\n        |  count|                 2|\\n        |   mean|               3.5|\\n        | stddev|2.1213203435596424|\\n        |    min|                 2|\\n        |    max|                 5|\\n        +-------+------------------+\\n        >>> df.describe().show()\\n        +-------+------------------+-----+\\n        |summary|               age| name|\\n        +-------+------------------+-----+\\n        |  count|                 2|    2|\\n        |   mean|               3.5| null|\\n        | stddev|2.1213203435596424| null|\\n        |    min|                 2|Alice|\\n        |    max|                 5|  Bob|\\n        +-------+------------------+-----+\\n\\n        Use summary for expanded statistics and control over which statistics to compute.\", 'output': 'def describe(self, *cols)'}\n",
      "{'input': 'Computes specified statistics for numeric and string columns. Available statistics are:\\n        - count\\n        - mean\\n        - stddev\\n        - min\\n        - max\\n        - arbitrary approximate percentiles specified as a percentage (eg, 75%)\\n\\n        If no statistics are given, this function computes count, mean, stddev, min,\\n        approximate quartiles (percentiles at 25%, 50%, and 75%), and max.\\n\\n        .. note:: This function is meant for exploratory data analysis, as we make no\\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\\n\\n        >>> df.summary().show()\\n        +-------+------------------+-----+\\n        |summary|               age| name|\\n        +-------+------------------+-----+\\n        |  count|                 2|    2|\\n        |   mean|               3.5| null|\\n        | stddev|2.1213203435596424| null|\\n        |    min|                 2|Alice|\\n        |    25%|                 2| null|\\n        |    50%|                 2| null|\\n        |    75%|                 5| null|\\n        |    max|                 5|  Bob|\\n        +-------+------------------+-----+\\n\\n        >>> df.summary(\"count\", \"min\", \"25%\", \"75%\", \"max\").show()\\n        +-------+---+-----+\\n        |summary|age| name|\\n        +-------+---+-----+\\n        |  count|  2|    2|\\n        |    min|  2|Alice|\\n        |    25%|  2| null|\\n        |    75%|  5| null|\\n        |    max|  5|  Bob|\\n        +-------+---+-----+\\n\\n        To do a summary for specific columns first select them:\\n\\n        >>> df.select(\"age\", \"name\").summary(\"count\").show()\\n        +-------+---+----+\\n        |summary|age|name|\\n        +-------+---+----+\\n        |  count|  2|   2|\\n        +-------+---+----+\\n\\n        See also describe for basic statistics.', 'output': 'def summary(self, *statistics)'}\n",
      "{'input': \"Returns the first ``n`` rows.\\n\\n        .. note:: This method should only be used if the resulting array is expected\\n            to be small, as all the data is loaded into the driver's memory.\\n\\n        :param n: int, default 1. Number of rows to return.\\n        :return: If n is greater than 1, return a list of :class:`Row`.\\n            If n is 1, return a single Row.\\n\\n        >>> df.head()\\n        Row(age=2, name=u'Alice')\\n        >>> df.head(1)\\n        [Row(age=2, name=u'Alice')]\", 'output': 'def head(self, n=None)'}\n",
      "{'input': \"Projects a set of expressions and returns a new :class:`DataFrame`.\\n\\n        :param cols: list of column names (string) or expressions (:class:`Column`).\\n            If one of the column names is '*', that column is expanded to include all columns\\n            in the current DataFrame.\\n\\n        >>> df.select('*').collect()\\n        [Row(age=2, name=u'Alice'), Row(age=5, name=u'Bob')]\\n        >>> df.select('name', 'age').collect()\\n        [Row(name=u'Alice', age=2), Row(name=u'Bob', age=5)]\\n        >>> df.select(df.name, (df.age + 10).alias('age')).collect()\\n        [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]\", 'output': 'def select(self, *cols)'}\n",
      "{'input': 'Projects a set of SQL expressions and returns a new :class:`DataFrame`.\\n\\n        This is a variant of :func:`select` that accepts SQL expressions.\\n\\n        >>> df.selectExpr(\"age * 2\", \"abs(age)\").collect()\\n        [Row((age * 2)=4, abs(age)=2), Row((age * 2)=10, abs(age)=5)]', 'output': 'def selectExpr(self, *expr)'}\n",
      "{'input': 'Filters rows using the given condition.\\n\\n        :func:`where` is an alias for :func:`filter`.\\n\\n        :param condition: a :class:`Column` of :class:`types.BooleanType`\\n            or a string of SQL expression.\\n\\n        >>> df.filter(df.age > 3).collect()\\n        [Row(age=5, name=u\\'Bob\\')]\\n        >>> df.where(df.age == 2).collect()\\n        [Row(age=2, name=u\\'Alice\\')]\\n\\n        >>> df.filter(\"age > 3\").collect()\\n        [Row(age=5, name=u\\'Bob\\')]\\n        >>> df.where(\"age = 2\").collect()\\n        [Row(age=2, name=u\\'Alice\\')]', 'output': 'def filter(self, condition)'}\n",
      "{'input': \"Groups the :class:`DataFrame` using the specified columns,\\n        so we can run aggregation on them. See :class:`GroupedData`\\n        for all the available aggregate functions.\\n\\n        :func:`groupby` is an alias for :func:`groupBy`.\\n\\n        :param cols: list of columns to group by.\\n            Each element should be a column name (string) or an expression (:class:`Column`).\\n\\n        >>> df.groupBy().avg().collect()\\n        [Row(avg(age)=3.5)]\\n        >>> sorted(df.groupBy('name').agg({'age': 'mean'}).collect())\\n        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]\\n        >>> sorted(df.groupBy(df.name).avg().collect())\\n        [Row(name=u'Alice', avg(age)=2.0), Row(name=u'Bob', avg(age)=5.0)]\\n        >>> sorted(df.groupBy(['name', df.age]).count().collect())\\n        [Row(name=u'Alice', age=2, count=1), Row(name=u'Bob', age=5, count=1)]\", 'output': 'def groupBy(self, *cols)'}\n",
      "{'input': 'Return a new :class:`DataFrame` containing union of rows in this and another frame.\\n\\n        This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union\\n        (that does deduplication of elements), use this function followed by :func:`distinct`.\\n\\n        Also as standard in SQL, this function resolves columns by position (not by name).', 'output': 'def union(self, other)'}\n",
      "{'input': 'Returns a new :class:`DataFrame` containing union of rows in this and another frame.\\n\\n        This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set\\n        union (that does deduplication of elements), use this function followed by :func:`distinct`.\\n\\n        The difference between this function and :func:`union` is that this function\\n        resolves columns by name (not by position):\\n\\n        >>> df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\\n        >>> df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\\n        >>> df1.unionByName(df2).show()\\n        +----+----+----+\\n        |col0|col1|col2|\\n        +----+----+----+\\n        |   1|   2|   3|\\n        |   6|   4|   5|\\n        +----+----+----+', 'output': 'def unionByName(self, other)'}\n",
      "{'input': 'Return a new :class:`DataFrame` containing rows only in\\n        both this frame and another frame.\\n\\n        This is equivalent to `INTERSECT` in SQL.', 'output': 'def intersect(self, other)'}\n",
      "{'input': 'Return a new :class:`DataFrame` containing rows in both this dataframe and other\\n        dataframe while preserving duplicates.\\n\\n        This is equivalent to `INTERSECT ALL` in SQL.\\n        >>> df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\\n        >>> df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\\n\\n        >>> df1.intersectAll(df2).sort(\"C1\", \"C2\").show()\\n        +---+---+\\n        | C1| C2|\\n        +---+---+\\n        |  a|  1|\\n        |  a|  1|\\n        |  b|  3|\\n        +---+---+\\n\\n        Also as standard in SQL, this function resolves columns by position (not by name).', 'output': 'def intersectAll(self, other)'}\n",
      "{'input': 'Return a new :class:`DataFrame` containing rows in this frame\\n        but not in another frame.\\n\\n        This is equivalent to `EXCEPT DISTINCT` in SQL.', 'output': 'def subtract(self, other)'}\n",
      "{'input': \"Return a new :class:`DataFrame` with duplicate rows removed,\\n        optionally only considering certain columns.\\n\\n        For a static batch :class:`DataFrame`, it just drops duplicate rows. For a streaming\\n        :class:`DataFrame`, it will keep all data across triggers as intermediate state to drop\\n        duplicates rows. You can use :func:`withWatermark` to limit how late the duplicate data can\\n        be and system will accordingly limit the state. In addition, too late data older than\\n        watermark will be dropped to avoid any possibility of duplicates.\\n\\n        :func:`drop_duplicates` is an alias for :func:`dropDuplicates`.\\n\\n        >>> from pyspark.sql import Row\\n        >>> df = sc.parallelize([ \\\\\\\\\\n        ...     Row(name='Alice', age=5, height=80), \\\\\\\\\\n        ...     Row(name='Alice', age=5, height=80), \\\\\\\\\\n        ...     Row(name='Alice', age=10, height=80)]).toDF()\\n        >>> df.dropDuplicates().show()\\n        +---+------+-----+\\n        |age|height| name|\\n        +---+------+-----+\\n        |  5|    80|Alice|\\n        | 10|    80|Alice|\\n        +---+------+-----+\\n\\n        >>> df.dropDuplicates(['name', 'height']).show()\\n        +---+------+-----+\\n        |age|height| name|\\n        +---+------+-----+\\n        |  5|    80|Alice|\\n        +---+------+-----+\", 'output': 'def dropDuplicates(self, subset=None)'}\n",
      "{'input': \"Returns a new :class:`DataFrame` omitting rows with null values.\\n        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\\n\\n        :param how: 'any' or 'all'.\\n            If 'any', drop a row if it contains any nulls.\\n            If 'all', drop a row only if all its values are null.\\n        :param thresh: int, default None\\n            If specified, drop rows that have less than `thresh` non-null values.\\n            This overwrites the `how` parameter.\\n        :param subset: optional list of column names to consider.\\n\\n        >>> df4.na.drop().show()\\n        +---+------+-----+\\n        |age|height| name|\\n        +---+------+-----+\\n        | 10|    80|Alice|\\n        +---+------+-----+\", 'output': \"def dropna(self, how='any', thresh=None, subset=None)\"}\n",
      "{'input': \"Replace null values, alias for ``na.fill()``.\\n        :func:`DataFrame.fillna` and :func:`DataFrameNaFunctions.fill` are aliases of each other.\\n\\n        :param value: int, long, float, string, bool or dict.\\n            Value to replace null values with.\\n            If the value is a dict, then `subset` is ignored and `value` must be a mapping\\n            from column name (string) to replacement value. The replacement value must be\\n            an int, long, float, boolean, or string.\\n        :param subset: optional list of column names to consider.\\n            Columns specified in subset that do not have matching data type are ignored.\\n            For example, if `value` is a string, and subset contains a non-string column,\\n            then the non-string column is simply ignored.\\n\\n        >>> df4.na.fill(50).show()\\n        +---+------+-----+\\n        |age|height| name|\\n        +---+------+-----+\\n        | 10|    80|Alice|\\n        |  5|    50|  Bob|\\n        | 50|    50|  Tom|\\n        | 50|    50| null|\\n        +---+------+-----+\\n\\n        >>> df5.na.fill(False).show()\\n        +----+-------+-----+\\n        | age|   name|  spy|\\n        +----+-------+-----+\\n        |  10|  Alice|false|\\n        |   5|    Bob|false|\\n        |null|Mallory| true|\\n        +----+-------+-----+\\n\\n        >>> df4.na.fill({'age': 50, 'name': 'unknown'}).show()\\n        +---+------+-------+\\n        |age|height|   name|\\n        +---+------+-------+\\n        | 10|    80|  Alice|\\n        |  5|  null|    Bob|\\n        | 50|  null|    Tom|\\n        | 50|  null|unknown|\\n        +---+------+-------+\", 'output': 'def fillna(self, value, subset=None)'}\n",
      "{'input': \"Returns a new :class:`DataFrame` replacing a value with another value.\\n        :func:`DataFrame.replace` and :func:`DataFrameNaFunctions.replace` are\\n        aliases of each other.\\n        Values to_replace and value must have the same type and can only be numerics, booleans,\\n        or strings. Value can have None. When replacing, the new value will be cast\\n        to the type of the existing column.\\n        For numeric replacements all values to be replaced should have unique\\n        floating point representation. In case of conflicts (for example with `{42: -1, 42.0: 1}`)\\n        and arbitrary replacement will be used.\\n\\n        :param to_replace: bool, int, long, float, string, list or dict.\\n            Value to be replaced.\\n            If the value is a dict, then `value` is ignored or can be omitted, and `to_replace`\\n            must be a mapping between a value and a replacement.\\n        :param value: bool, int, long, float, string, list or None.\\n            The replacement value must be a bool, int, long, float, string or None. If `value` is a\\n            list, `value` should be of the same length and type as `to_replace`.\\n            If `value` is a scalar and `to_replace` is a sequence, then `value` is\\n            used as a replacement for each item in `to_replace`.\\n        :param subset: optional list of column names to consider.\\n            Columns specified in subset that do not have matching data type are ignored.\\n            For example, if `value` is a string, and subset contains a non-string column,\\n            then the non-string column is simply ignored.\\n\\n        >>> df4.na.replace(10, 20).show()\\n        +----+------+-----+\\n        | age|height| name|\\n        +----+------+-----+\\n        |  20|    80|Alice|\\n        |   5|  null|  Bob|\\n        |null|  null|  Tom|\\n        |null|  null| null|\\n        +----+------+-----+\\n\\n        >>> df4.na.replace('Alice', None).show()\\n        +----+------+----+\\n        | age|height|name|\\n        +----+------+----+\\n        |  10|    80|null|\\n        |   5|  null| Bob|\\n        |null|  null| Tom|\\n        |null|  null|null|\\n        +----+------+----+\\n\\n        >>> df4.na.replace({'Alice': None}).show()\\n        +----+------+----+\\n        | age|height|name|\\n        +----+------+----+\\n        |  10|    80|null|\\n        |   5|  null| Bob|\\n        |null|  null| Tom|\\n        |null|  null|null|\\n        +----+------+----+\\n\\n        >>> df4.na.replace(['Alice', 'Bob'], ['A', 'B'], 'name').show()\\n        +----+------+----+\\n        | age|height|name|\\n        +----+------+----+\\n        |  10|    80|   A|\\n        |   5|  null|   B|\\n        |null|  null| Tom|\\n        |null|  null|null|\\n        +----+------+----+\", 'output': 'def replace(self, to_replace, value=_NoValue, subset=None)'}\n",
      "{'input': 'Calculates the approximate quantiles of numerical columns of a\\n        DataFrame.\\n\\n        The result of this algorithm has the following deterministic bound:\\n        If the DataFrame has N elements and if we request the quantile at\\n        probability `p` up to error `err`, then the algorithm will return\\n        a sample `x` from the DataFrame so that the *exact* rank of `x` is\\n        close to (p * N). More precisely,\\n\\n          floor((p - err) * N) <= rank(x) <= ceil((p + err) * N).\\n\\n        This method implements a variation of the Greenwald-Khanna\\n        algorithm (with some speed optimizations). The algorithm was first\\n        present in [[https://doi.org/10.1145/375663.375670\\n        Space-efficient Online Computation of Quantile Summaries]]\\n        by Greenwald and Khanna.\\n\\n        Note that null values will be ignored in numerical columns before calculation.\\n        For columns only containing null values, an empty list is returned.\\n\\n        :param col: str, list.\\n          Can be a single column name, or a list of names for multiple columns.\\n        :param probabilities: a list of quantile probabilities\\n          Each number must belong to [0, 1].\\n          For example 0 is the minimum, 0.5 is the median, 1 is the maximum.\\n        :param relativeError:  The relative target precision to achieve\\n          (>= 0). If set to zero, the exact quantiles are computed, which\\n          could be very expensive. Note that values greater than 1 are\\n          accepted but give the same result as 1.\\n        :return:  the approximate quantiles at the given probabilities. If\\n          the input `col` is a string, the output is a list of floats. If the\\n          input `col` is a list or tuple of strings, the output is also a\\n          list, but each element in it is a list of floats, i.e., the output\\n          is a list of list of floats.\\n\\n        .. versionchanged:: 2.2\\n           Added support for multiple columns.', 'output': 'def approxQuantile(self, col, probabilities, relativeError)'}\n",
      "{'input': 'Calculates the correlation of two columns of a DataFrame as a double value.\\n        Currently only supports the Pearson Correlation Coefficient.\\n        :func:`DataFrame.corr` and :func:`DataFrameStatFunctions.corr` are aliases of each other.\\n\\n        :param col1: The name of the first column\\n        :param col2: The name of the second column\\n        :param method: The correlation method. Currently only supports \"pearson\"', 'output': 'def corr(self, col1, col2, method=None)'}\n",
      "{'input': 'Calculate the sample covariance for the given columns, specified by their names, as a\\n        double value. :func:`DataFrame.cov` and :func:`DataFrameStatFunctions.cov` are aliases.\\n\\n        :param col1: The name of the first column\\n        :param col2: The name of the second column', 'output': 'def cov(self, col1, col2)'}\n",
      "{'input': 'Computes a pair-wise frequency table of the given columns. Also known as a contingency\\n        table. The number of distinct values for each column should be less than 1e4. At most 1e6\\n        non-zero pair frequencies will be returned.\\n        The first column of each row will be the distinct values of `col1` and the column names\\n        will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\\n        Pairs that have no occurrences will have zero as their counts.\\n        :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\\n\\n        :param col1: The name of the first column. Distinct items will make the first item of\\n            each row.\\n        :param col2: The name of the second column. Distinct items will make the column names\\n            of the DataFrame.', 'output': 'def crosstab(self, col1, col2)'}\n",
      "{'input': 'Finding frequent items for columns, possibly with false positives. Using the\\n        frequent element count algorithm described in\\n        \"https://doi.org/10.1145/762471.762473, proposed by Karp, Schenker, and Papadimitriou\".\\n        :func:`DataFrame.freqItems` and :func:`DataFrameStatFunctions.freqItems` are aliases.\\n\\n        .. note:: This function is meant for exploratory data analysis, as we make no\\n            guarantee about the backward compatibility of the schema of the resulting DataFrame.\\n\\n        :param cols: Names of the columns to calculate frequent items for as a list or tuple of\\n            strings.\\n        :param support: The frequency with which to consider an item \\'frequent\\'. Default is 1%.\\n            The support must be greater than 1e-4.', 'output': 'def freqItems(self, cols, support=None)'}\n",
      "{'input': \"Returns a new :class:`DataFrame` by adding a column or replacing the\\n        existing column that has the same name.\\n\\n        The column expression must be an expression over this DataFrame; attempting to add\\n        a column from some other dataframe will raise an error.\\n\\n        :param colName: string, name of the new column.\\n        :param col: a :class:`Column` expression for the new column.\\n\\n        .. note:: This method introduces a projection internally. Therefore, calling it multiple\\n            times, for instance, via loops in order to add multiple columns can generate big\\n            plans which can cause performance issues and even `StackOverflowException`.\\n            To avoid this, use :func:`select` with the multiple columns at once.\\n\\n        >>> df.withColumn('age2', df.age + 2).collect()\\n        [Row(age=2, name=u'Alice', age2=4), Row(age=5, name=u'Bob', age2=7)]\", 'output': 'def withColumn(self, colName, col)'}\n",
      "{'input': \"Returns a new :class:`DataFrame` by renaming an existing column.\\n        This is a no-op if schema doesn't contain the given column name.\\n\\n        :param existing: string, name of the existing column to rename.\\n        :param new: string, new name of the column.\\n\\n        >>> df.withColumnRenamed('age', 'age2').collect()\\n        [Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]\", 'output': 'def withColumnRenamed(self, existing, new)'}\n",
      "{'input': \"Returns a new :class:`DataFrame` that drops the specified column.\\n        This is a no-op if schema doesn't contain the given column name(s).\\n\\n        :param cols: a string name of the column to drop, or a\\n            :class:`Column` to drop, or a list of string name of the columns to drop.\\n\\n        >>> df.drop('age').collect()\\n        [Row(name=u'Alice'), Row(name=u'Bob')]\\n\\n        >>> df.drop(df.age).collect()\\n        [Row(name=u'Alice'), Row(name=u'Bob')]\\n\\n        >>> df.join(df2, df.name == df2.name, 'inner').drop(df.name).collect()\\n        [Row(age=5, height=85, name=u'Bob')]\\n\\n        >>> df.join(df2, df.name == df2.name, 'inner').drop(df2.name).collect()\\n        [Row(age=5, name=u'Bob', height=85)]\\n\\n        >>> df.join(df2, 'name', 'inner').drop('age', 'height').collect()\\n        [Row(name=u'Bob')]\", 'output': 'def drop(self, *cols)'}\n",
      "{'input': \"Returns a new class:`DataFrame` that with new specified column names\\n\\n        :param cols: list of new column names (string)\\n\\n        >>> df.toDF('f1', 'f2').collect()\\n        [Row(f1=2, f2=u'Alice'), Row(f1=5, f2=u'Bob')]\", 'output': 'def toDF(self, *cols)'}\n",
      "{'input': 'Returns a new class:`DataFrame`. Concise syntax for chaining custom transformations.\\n\\n        :param func: a function that takes and returns a class:`DataFrame`.\\n\\n        >>> from pyspark.sql.functions import col\\n        >>> df = spark.createDataFrame([(1, 1.0), (2, 2.0)], [\"int\", \"float\"])\\n        >>> def cast_all_to_int(input_df):\\n        ...     return input_df.select([col(col_name).cast(\"int\") for col_name in input_df.columns])\\n        >>> def sort_columns_asc(input_df):\\n        ...     return input_df.select(*sorted(input_df.columns))\\n        >>> df.transform(cast_all_to_int).transform(sort_columns_asc).show()\\n        +-----+---+\\n        |float|int|\\n        +-----+---+\\n        |    1|  1|\\n        |    2|  2|\\n        +-----+---+', 'output': 'def transform(self, func)'}\n",
      "{'input': \"Returns the contents of this :class:`DataFrame` as Pandas ``pandas.DataFrame``.\\n\\n        This is only available if Pandas is installed and available.\\n\\n        .. note:: This method should only be used if the resulting Pandas's DataFrame is expected\\n            to be small, as all the data is loaded into the driver's memory.\\n\\n        .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\\n\\n        >>> df.toPandas()  # doctest: +SKIP\\n           age   name\\n        0    2  Alice\\n        1    5    Bob\", 'output': 'def toPandas(self)'}\n",
      "{'input': 'Returns all records as a list of ArrowRecordBatches, pyarrow must be installed\\n        and available on driver and worker Python environments.\\n\\n        .. note:: Experimental.', 'output': 'def _collectAsArrow(self)'}\n",
      "{'input': \"Returns the :class:`StatCounter` members as a ``dict``.\\n\\n        >>> sc.parallelize([1., 2., 3., 4.]).stats().asDict()\\n        {'count': 4L,\\n         'max': 4.0,\\n         'mean': 2.5,\\n         'min': 1.0,\\n         'stdev': 1.2909944487358056,\\n         'sum': 10.0,\\n         'variance': 1.6666666666666667}\", 'output': 'def asDict(self, sample=False)'}\n",
      "{'input': 'Returns a list of function information via JVM. Sorts wrapped expression infos by name\\n    and returns them.', 'output': 'def _list_function_infos(jvm)'}\n",
      "{'input': 'Makes the usage description pretty and returns a formatted string if `usage`\\n    is not an empty string. Otherwise, returns None.', 'output': 'def _make_pretty_usage(usage)'}\n",
      "{'input': 'Makes the arguments description pretty and returns a formatted string if `arguments`\\n    starts with the argument prefix. Otherwise, returns None.\\n\\n    Expected input:\\n\\n        Arguments:\\n          * arg0 - ...\\n              ...\\n          * arg0 - ...\\n              ...\\n\\n    Expected output:\\n    **Arguments:**\\n\\n    * arg0 - ...\\n        ...\\n    * arg0 - ...\\n        ...', 'output': 'def _make_pretty_arguments(arguments)'}\n",
      "{'input': 'Makes the examples description pretty and returns a formatted string if `examples`\\n    starts with the example prefix. Otherwise, returns None.\\n\\n    Expected input:\\n\\n        Examples:\\n          > SELECT ...;\\n           ...\\n          > SELECT ...;\\n           ...\\n\\n    Expected output:\\n    **Examples:**\\n\\n    ```\\n    > SELECT ...;\\n     ...\\n    > SELECT ...;\\n     ...\\n    ```', 'output': 'def _make_pretty_examples(examples)'}\n",
      "{'input': 'Makes the note description pretty and returns a formatted string if `note` is not\\n    an empty string. Otherwise, returns None.\\n\\n    Expected input:\\n\\n        ...\\n\\n    Expected output:\\n    **Note:**\\n\\n    ...', 'output': 'def _make_pretty_note(note)'}\n",
      "{'input': 'Makes the deprecated description pretty and returns a formatted string if `deprecated`\\n    is not an empty string. Otherwise, returns None.\\n\\n    Expected input:\\n\\n        ...\\n\\n    Expected output:\\n    **Deprecated:**\\n\\n    ...', 'output': 'def _make_pretty_deprecated(deprecated)'}\n",
      "{'input': 'Generates a markdown file after listing the function information. The output file\\n    is created in `path`.\\n\\n    Expected output:\\n    ### NAME\\n\\n    USAGE\\n\\n    **Arguments:**\\n\\n    ARGUMENTS\\n\\n    **Examples:**\\n\\n    ```\\n    EXAMPLES\\n    ```\\n\\n    **Note:**\\n\\n    NOTE\\n\\n    **Since:** SINCE\\n\\n    **Deprecated:**\\n\\n    DEPRECATED\\n\\n    <br/>', 'output': 'def generate_sql_markdown(jvm, path)'}\n",
      "{'input': 'Predict values for a single data point or an RDD of points\\n        using the model trained.', 'output': 'def predict(self, x)'}\n",
      "{'input': 'Save this model to the given path.', 'output': 'def save(self, sc, path)'}\n",
      "{'input': 'Train a logistic regression model on the given data.\\n\\n        :param data:\\n          The training data, an RDD of LabeledPoint.\\n        :param iterations:\\n          The number of iterations.\\n          (default: 100)\\n        :param initialWeights:\\n          The initial weights.\\n          (default: None)\\n        :param regParam:\\n          The regularizer parameter.\\n          (default: 0.0)\\n        :param regType:\\n          The type of regularizer used for training our model.\\n          Supported values:\\n\\n            - \"l1\" for using L1 regularization\\n            - \"l2\" for using L2 regularization (default)\\n            - None for no regularization\\n        :param intercept:\\n          Boolean parameter which indicates the use or not of the\\n          augmented representation for training data (i.e., whether bias\\n          features are activated or not).\\n          (default: False)\\n        :param corrections:\\n          The number of corrections used in the LBFGS update.\\n          If a known updater is used for binary classification,\\n          it calls the ml implementation and this parameter will\\n          have no effect. (default: 10)\\n        :param tolerance:\\n          The convergence tolerance of iterations for L-BFGS.\\n          (default: 1e-6)\\n        :param validateData:\\n          Boolean parameter which indicates if the algorithm should\\n          validate data before training.\\n          (default: True)\\n        :param numClasses:\\n          The number of classes (i.e., outcomes) a label can take in\\n          Multinomial Logistic Regression.\\n          (default: 2)\\n\\n        >>> data = [\\n        ...     LabeledPoint(0.0, [0.0, 1.0]),\\n        ...     LabeledPoint(1.0, [1.0, 0.0]),\\n        ... ]\\n        >>> lrm = LogisticRegressionWithLBFGS.train(sc.parallelize(data), iterations=10)\\n        >>> lrm.predict([1.0, 0.0])\\n        1\\n        >>> lrm.predict([0.0, 1.0])\\n        0', 'output': 'def train(cls, data, iterations=100, initialWeights=None, regParam=0.0, regType=\"l2\",\\n              intercept=False, corrections=10, tolerance=1e-6, validateData=True, numClasses=2)'}\n",
      "{'input': 'Predict values for a single data point or an RDD of points\\n        using the model trained.', 'output': 'def predict(self, x)'}\n",
      "{'input': 'Save this model to the given path.', 'output': 'def save(self, sc, path)'}\n",
      "{'input': 'Load a model from the given path.', 'output': 'def load(cls, sc, path)'}\n",
      "{'input': 'Train a Naive Bayes model given an RDD of (label, features)\\n        vectors.\\n\\n        This is the Multinomial NB (U{http://tinyurl.com/lsdw6p}) which\\n        can handle all kinds of discrete data.  For example, by\\n        converting documents into TF-IDF vectors, it can be used for\\n        document classification. By making every vector a 0-1 vector,\\n        it can also be used as Bernoulli NB (U{http://tinyurl.com/p7c96j6}).\\n        The input feature values must be nonnegative.\\n\\n        :param data:\\n          RDD of LabeledPoint.\\n        :param lambda_:\\n          The smoothing parameter.\\n          (default: 1.0)', 'output': 'def train(cls, data, lambda_=1.0)'}\n",
      "{'input': 'Push item onto heap, maintaining the heap invariant.', 'output': 'def heappush(heap, item)'}\n",
      "{'input': 'Pop the smallest item off the heap, maintaining the heap invariant.', 'output': 'def heappop(heap)'}\n",
      "{'input': 'Pop and return the current smallest value, and add the new item.\\n\\n    This is more efficient than heappop() followed by heappush(), and can be\\n    more appropriate when using a fixed-size heap.  Note that the value\\n    returned may be larger than item!  That constrains reasonable uses of\\n    this routine unless written as part of a conditional replacement:\\n\\n        if item > heap[0]:\\n            item = heapreplace(heap, item)', 'output': 'def heapreplace(heap, item)'}\n",
      "{'input': 'Fast version of a heappush followed by a heappop.', 'output': 'def heappushpop(heap, item)'}\n",
      "{'input': 'Transform list into a heap, in-place, in O(len(x)) time.', 'output': 'def heapify(x)'}\n",
      "{'input': 'Maxheap version of a heappop.', 'output': 'def _heappop_max(heap)'}\n",
      "{'input': 'Maxheap version of a heappop followed by a heappush.', 'output': 'def _heapreplace_max(heap, item)'}\n",
      "{'input': 'Transform list into a maxheap, in-place, in O(len(x)) time.', 'output': 'def _heapify_max(x)'}\n",
      "{'input': 'Maxheap variant of _siftdown', 'output': 'def _siftdown_max(heap, startpos, pos)'}\n",
      "{'input': 'Maxheap variant of _siftup', 'output': 'def _siftup_max(heap, pos)'}\n",
      "{'input': \"Merge multiple sorted inputs into a single sorted output.\\n\\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\\n    does not pull the data into memory all at once, and assumes that each of\\n    the input streams is already sorted (smallest to largest).\\n\\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\\n\\n    If *key* is not None, applies a key function to each element to determine\\n    its sort order.\\n\\n    >>> list(merge(['dog', 'horse'], ['cat', 'fish', 'kangaroo'], key=len))\\n    ['dog', 'cat', 'fish', 'horse', 'kangaroo']\", 'output': 'def merge(iterables, key=None, reverse=False)'}\n",
      "{'input': 'Find the n smallest elements in a dataset.\\n\\n    Equivalent to:  sorted(iterable, key=key)[:n]', 'output': 'def nsmallest(n, iterable, key=None)'}\n",
      "{'input': 'Find the n largest elements in a dataset.\\n\\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]', 'output': 'def nlargest(n, iterable, key=None)'}\n",
      "{'input': \"Compute the correlation matrix with specified method using dataset.\\n\\n        :param dataset:\\n          A Dataset or a DataFrame.\\n        :param column:\\n          The name of the column of vectors for which the correlation coefficient needs\\n          to be computed. This must be a column of the dataset, and it must contain\\n          Vector objects.\\n        :param method:\\n          String specifying the method to use for computing correlation.\\n          Supported: `pearson` (default), `spearman`.\\n        :return:\\n          A DataFrame that contains the correlation matrix of the column of vectors. This\\n          DataFrame contains a single row and a single column of name\\n          '$METHODNAME($COLUMN)'.\\n\\n        >>> from pyspark.ml.linalg import Vectors\\n        >>> from pyspark.ml.stat import Correlation\\n        >>> dataset = [[Vectors.dense([1, 0, 0, -2])],\\n        ...            [Vectors.dense([4, 5, 0, 3])],\\n        ...            [Vectors.dense([6, 7, 0, 8])],\\n        ...            [Vectors.dense([9, 0, 0, 1])]]\\n        >>> dataset = spark.createDataFrame(dataset, ['features'])\\n        >>> pearsonCorr = Correlation.corr(dataset, 'features', 'pearson').collect()[0][0]\\n        >>> print(str(pearsonCorr).replace('nan', 'NaN'))\\n        DenseMatrix([[ 1.        ,  0.0556...,         NaN,  0.4004...],\\n                     [ 0.0556...,  1.        ,         NaN,  0.9135...],\\n                     [        NaN,         NaN,  1.        ,         NaN],\\n                     [ 0.4004...,  0.9135...,         NaN,  1.        ]])\\n        >>> spearmanCorr = Correlation.corr(dataset, 'features', method='spearman').collect()[0][0]\\n        >>> print(str(spearmanCorr).replace('nan', 'NaN'))\\n        DenseMatrix([[ 1.        ,  0.1054...,         NaN,  0.4       ],\\n                     [ 0.1054...,  1.        ,         NaN,  0.9486... ],\\n                     [        NaN,         NaN,  1.        ,         NaN],\\n                     [ 0.4       ,  0.9486... ,         NaN,  1.        ]])\", 'output': 'def corr(dataset, column, method=\"pearson\")'}\n",
      "{'input': 'Given a list of metrics, provides a builder that it turns computes metrics from a column.\\n\\n        See the documentation of [[Summarizer]] for an example.\\n\\n        The following metrics are accepted (case sensitive):\\n         - mean: a vector that contains the coefficient-wise mean.\\n         - variance: a vector tha contains the coefficient-wise variance.\\n         - count: the count of all vectors seen.\\n         - numNonzeros: a vector with the number of non-zeros for each coefficients\\n         - max: the maximum for each coefficient.\\n         - min: the minimum for each coefficient.\\n         - normL2: the Euclidean norm for each coefficient.\\n         - normL1: the L1 norm of each coefficient (sum of the absolute values).\\n\\n        :param metrics:\\n         metrics that can be provided.\\n        :return:\\n         an object of :py:class:`pyspark.ml.stat.SummaryBuilder`\\n\\n        Note: Currently, the performance of this interface is about 2x~3x slower then using the RDD\\n        interface.', 'output': 'def metrics(*metrics)'}\n",
      "{'input': 'Returns an aggregate object that contains the summary of the column with the requested\\n        metrics.\\n\\n        :param featuresCol:\\n         a column that contains features Vector object.\\n        :param weightCol:\\n         a column that contains weight value. Default weight is 1.0.\\n        :return:\\n         an aggregate column that contains the statistics. The exact content of this\\n         structure is determined during the creation of the builder.', 'output': 'def summary(self, featuresCol, weightCol=None)'}\n",
      "{'input': 'Compute the correlation (matrix) for the input RDD(s) using the\\n        specified method.\\n        Methods currently supported: I{pearson (default), spearman}.\\n\\n        If a single RDD of Vectors is passed in, a correlation matrix\\n        comparing the columns in the input RDD is returned. Use C{method=}\\n        to specify the method to be used for single RDD inout.\\n        If two RDDs of floats are passed in, a single float is returned.\\n\\n        :param x: an RDD of vector for which the correlation matrix is to be computed,\\n                  or an RDD of float of the same cardinality as y when y is specified.\\n        :param y: an RDD of float of the same cardinality as x.\\n        :param method: String specifying the method to use for computing correlation.\\n                       Supported: `pearson` (default), `spearman`\\n        :return: Correlation matrix comparing columns in x.\\n\\n        >>> x = sc.parallelize([1.0, 0.0, -2.0], 2)\\n        >>> y = sc.parallelize([4.0, 5.0, 3.0], 2)\\n        >>> zeros = sc.parallelize([0.0, 0.0, 0.0], 2)\\n        >>> abs(Statistics.corr(x, y) - 0.6546537) < 1e-7\\n        True\\n        >>> Statistics.corr(x, y) == Statistics.corr(x, y, \"pearson\")\\n        True\\n        >>> Statistics.corr(x, y, \"spearman\")\\n        0.5\\n        >>> from math import isnan\\n        >>> isnan(Statistics.corr(x, zeros))\\n        True\\n        >>> from pyspark.mllib.linalg import Vectors\\n        >>> rdd = sc.parallelize([Vectors.dense([1, 0, 0, -2]), Vectors.dense([4, 5, 0, 3]),\\n        ...                       Vectors.dense([6, 7, 0,  8]), Vectors.dense([9, 0, 0, 1])])\\n        >>> pearsonCorr = Statistics.corr(rdd)\\n        >>> print(str(pearsonCorr).replace(\\'nan\\', \\'NaN\\'))\\n        [[ 1.          0.05564149         NaN  0.40047142]\\n         [ 0.05564149  1.                 NaN  0.91359586]\\n         [        NaN         NaN  1.                 NaN]\\n         [ 0.40047142  0.91359586         NaN  1.        ]]\\n        >>> spearmanCorr = Statistics.corr(rdd, method=\"spearman\")\\n        >>> print(str(spearmanCorr).replace(\\'nan\\', \\'NaN\\'))\\n        [[ 1.          0.10540926         NaN  0.4       ]\\n         [ 0.10540926  1.                 NaN  0.9486833 ]\\n         [        NaN         NaN  1.                 NaN]\\n         [ 0.4         0.9486833          NaN  1.        ]]\\n        >>> try:\\n        ...     Statistics.corr(rdd, \"spearman\")\\n        ...     print(\"Method name as second argument without \\'method=\\' shouldn\\'t be allowed.\")\\n        ... except TypeError:\\n        ...     pass', 'output': 'def corr(x, y=None, method=None)'}\n",
      "{'input': 'Creates a list of callables which can be called from different threads to fit and evaluate\\n    an estimator in parallel. Each callable returns an `(index, metric)` pair.\\n\\n    :param est: Estimator, the estimator to be fit.\\n    :param train: DataFrame, training data set, used for fitting.\\n    :param eva: Evaluator, used to compute `metric`\\n    :param validation: DataFrame, validation data set, used for evaluation.\\n    :param epm: Sequence of ParamMap, params maps to be used during fitting & evaluation.\\n    :param collectSubModel: Whether to collect sub model.\\n    :return: (int, float, subModel), an index into `epm` and the associated metric value.', 'output': 'def _parallelFitTasks(est, train, eva, validation, epm, collectSubModel)'}\n",
      "{'input': 'Sets the given parameters in this grid to fixed values.\\n        Accepts either a parameter dictionary or a list of (parameter, value) pairs.', 'output': 'def baseOn(self, *args)'}\n",
      "{'input': 'Builds and returns all combinations of parameters specified\\n        by the param grid.', 'output': 'def build(self)'}\n",
      "{'input': 'Return Python estimator, estimatorParamMaps, and evaluator from a Java ValidatorParams.', 'output': 'def _from_java_impl(cls, java_stage)'}\n",
      "{'input': 'Return Java estimator, estimatorParamMaps, and evaluator from this Python instance.', 'output': 'def _to_java_impl(self)'}\n",
      "{'input': 'Given a Java CrossValidator, create and return a Python wrapper of it.\\n        Used for ML persistence.', 'output': 'def _from_java(cls, java_stage)'}\n",
      "{'input': 'Transfer this instance to a Java CrossValidator. Used for ML persistence.\\n\\n        :return: Java object equivalent to this instance.', 'output': 'def _to_java(self)'}\n",
      "{'input': 'Creates a copy of this instance with a randomly generated uid\\n        and some extra params. This copies the underlying bestModel,\\n        creates a deep copy of the embedded paramMap, and\\n        copies the embedded and extra parameters over.\\n        It does not copy the extra Params into the subModels.\\n\\n        :param extra: Extra parameters to copy to the new instance\\n        :return: Copy of this instance', 'output': 'def copy(self, extra=None)'}\n",
      "{'input': 'setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\\\\\\n                  parallelism=1, collectSubModels=False, seed=None):\\n        Sets params for the train validation split.', 'output': 'def setParams(self, estimator=None, estimatorParamMaps=None, evaluator=None, trainRatio=0.75,\\n                  parallelism=1, collectSubModels=False, seed=None)'}\n",
      "{'input': 'Creates a copy of this instance with a randomly generated uid\\n        and some extra params. This copies creates a deep copy of\\n        the embedded paramMap, and copies the embedded and extra parameters over.\\n\\n        :param extra: Extra parameters to copy to the new instance\\n        :return: Copy of this instance', 'output': 'def copy(self, extra=None)'}\n",
      "{'input': 'Given a Java TrainValidationSplit, create and return a Python wrapper of it.\\n        Used for ML persistence.', 'output': 'def _from_java(cls, java_stage)'}\n",
      "{'input': 'Transfer this instance to a Java TrainValidationSplit. Used for ML persistence.\\n        :return: Java object equivalent to this instance.', 'output': 'def _to_java(self)'}\n",
      "{'input': 'Creates a copy of this instance with a randomly generated uid\\n        and some extra params. This copies the underlying bestModel,\\n        creates a deep copy of the embedded paramMap, and\\n        copies the embedded and extra parameters over.\\n        And, this creates a shallow copy of the validationMetrics.\\n        It does not copy the extra Params into the subModels.\\n\\n        :param extra: Extra parameters to copy to the new instance\\n        :return: Copy of this instance', 'output': 'def copy(self, extra=None)'}\n",
      "{'input': 'Given a Java TrainValidationSplitModel, create and return a Python wrapper of it.\\n        Used for ML persistence.', 'output': 'def _from_java(cls, java_stage)'}\n",
      "{'input': 'Transfer this instance to a Java TrainValidationSplitModel. Used for ML persistence.\\n        :return: Java object equivalent to this instance.', 'output': 'def _to_java(self)'}\n",
      "{'input': 'Returns the value of Spark runtime configuration property for the given key,\\n        assuming it is set.', 'output': 'def get(self, key, default=_NoValue)'}\n",
      "{'input': 'Assert that an object is of type str.', 'output': 'def _checkType(self, obj, identifier)'}\n",
      "{'input': 'Create a PySpark function by its name', 'output': 'def _create_function(name, doc=\"\")'}\n",
      "{'input': 'Similar with `_create_function` but creates a PySpark function that takes a column\\n    (as string as well). This is mainly for PySpark functions to take strings as\\n    column names.', 'output': 'def _create_function_over_column(name, doc=\"\")'}\n",
      "{'input': 'Wrap the deprecated function to print out deprecation warnings', 'output': 'def _wrap_deprecated_function(func, message)'}\n",
      "{'input': 'Create a binary mathfunction by name', 'output': 'def _create_binary_mathfunction(name, doc=\"\")'}\n",
      "{'input': 'Create a window function by name', 'output': \"def _create_window_function(name, doc='')\"}\n",
      "{'input': \"Aggregate function: returns a new :class:`Column` for approximate distinct count of\\n    column `col`.\\n\\n    :param rsd: maximum estimation error allowed (default = 0.05). For rsd < 0.01, it is more\\n        efficient to use :func:`countDistinct`\\n\\n    >>> df.agg(approx_count_distinct(df.age).alias('distinct_ages')).collect()\\n    [Row(distinct_ages=2)]\", 'output': 'def approx_count_distinct(col, rsd=None)'}\n",
      "{'input': 'Marks a DataFrame as small enough for use in broadcast joins.', 'output': 'def broadcast(df)'}\n",
      "{'input': 'Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\\n\\n    >>> df.agg(countDistinct(df.age, df.name).alias(\\'c\\')).collect()\\n    [Row(c=2)]\\n\\n    >>> df.agg(countDistinct(\"age\", \"name\").alias(\\'c\\')).collect()\\n    [Row(c=2)]', 'output': 'def countDistinct(col, *cols)'}\n",
      "{'input': 'Aggregate function: returns the last value in a group.\\n\\n    The function by default returns the last values it sees. It will return the last non-null\\n    value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\\n\\n    .. note:: The function is non-deterministic because its results depends on order of rows\\n        which may be non-deterministic after a shuffle.', 'output': 'def last(col, ignorenulls=False)'}\n",
      "{'input': 'Returns col1 if it is not NaN, or col2 if col1 is NaN.\\n\\n    Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\\n\\n    >>> df = spark.createDataFrame([(1.0, float(\\'nan\\')), (float(\\'nan\\'), 2.0)], (\"a\", \"b\"))\\n    >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\\n    [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]', 'output': 'def nanvl(col1, col2)'}\n",
      "{'input': \"Generates a random column with independent and identically distributed (i.i.d.) samples\\n    from U[0.0, 1.0].\\n\\n    .. note:: The function is non-deterministic in general case.\\n\\n    >>> df.withColumn('rand', rand(seed=42) * 3).collect()\\n    [Row(age=2, name=u'Alice', rand=2.4052597283576684),\\n     Row(age=5, name=u'Bob', rand=2.3913904055683974)]\", 'output': 'def rand(seed=None)'}\n",
      "{'input': \"Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\\n    or at integral part when `scale` < 0.\\n\\n    >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\\n    [Row(r=3.0)]\", 'output': 'def round(col, scale=0)'}\n",
      "{'input': \"Shift the given value numBits left.\\n\\n    >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\\n    [Row(r=42)]\", 'output': 'def shiftLeft(col, numBits)'}\n",
      "{'input': \"(Signed) shift the given value numBits right.\\n\\n    >>> spark.createDataFrame([(42,)], ['a']).select(shiftRight('a', 1).alias('r')).collect()\\n    [Row(r=21)]\", 'output': 'def shiftRight(col, numBits)'}\n",
      "{'input': 'Parses the expression string into the column that it represents\\n\\n    >>> df.select(expr(\"length(name)\")).collect()\\n    [Row(length(name)=5), Row(length(name)=3)]', 'output': 'def expr(str)'}\n",
      "{'input': 'Evaluates a list of conditions and returns one of multiple possible result expressions.\\n    If :func:`Column.otherwise` is not invoked, None is returned for unmatched conditions.\\n\\n    :param condition: a boolean :class:`Column` expression.\\n    :param value: a literal value, or a :class:`Column` expression.\\n\\n    >>> df.select(when(df[\\'age\\'] == 2, 3).otherwise(4).alias(\"age\")).collect()\\n    [Row(age=3), Row(age=4)]\\n\\n    >>> df.select(when(df.age == 2, df.age + 1).alias(\"age\")).collect()\\n    [Row(age=3), Row(age=None)]', 'output': 'def when(condition, value)'}\n",
      "{'input': \"Returns the first argument-based logarithm of the second argument.\\n\\n    If there is only one argument, then this takes the natural logarithm of the argument.\\n\\n    >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\\n    ['0.30102', '0.69897']\\n\\n    >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\\n    ['0.69314', '1.60943']\", 'output': 'def log(arg1, arg2=None)'}\n",
      "{'input': 'Convert a number in a string column from one base to another.\\n\\n    >>> df = spark.createDataFrame([(\"010101\",)], [\\'n\\'])\\n    >>> df.select(conv(df.n, 2, 16).alias(\\'hex\\')).collect()\\n    [Row(hex=u\\'15\\')]', 'output': 'def conv(col, fromBase, toBase)'}\n",
      "{'input': 'Window function: returns the value that is `offset` rows before the current row, and\\n    `defaultValue` if there is less than `offset` rows before the current row. For example,\\n    an `offset` of one will return the previous row at any given point in the window partition.\\n\\n    This is equivalent to the LAG function in SQL.\\n\\n    :param col: name of column or expression\\n    :param offset: number of row to extend\\n    :param default: default value', 'output': 'def lag(col, offset=1, default=None)'}\n",
      "{'input': 'Window function: returns the ntile group id (from 1 to `n` inclusive)\\n    in an ordered window partition. For example, if `n` is 4, the first\\n    quarter of the rows will get value 1, the second quarter will get 2,\\n    the third quarter will get 3, and the last quarter will get 4.\\n\\n    This is equivalent to the NTILE function in SQL.\\n\\n    :param n: an integer', 'output': 'def ntile(n)'}\n",
      "{'input': \"Converts a date/timestamp/string to a value of string in the format specified by the date\\n    format given by the second argument.\\n\\n    A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\\n    pattern letters of the Java class `java.time.format.DateTimeFormatter` can be used.\\n\\n    .. note:: Use when ever possible specialized functions like `year`. These benefit from a\\n        specialized implementation.\\n\\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\\n    >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\\n    [Row(date=u'04/08/2015')]\", 'output': 'def date_format(date, format)'}\n",
      "{'input': \"Returns the date that is `days` days after `start`\\n\\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\\n    >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\\n    [Row(next_date=datetime.date(2015, 4, 9))]\", 'output': 'def date_add(start, days)'}\n",
      "{'input': \"Returns the number of days from `start` to `end`.\\n\\n    >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\\n    >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\\n    [Row(diff=32)]\", 'output': 'def datediff(end, start)'}\n",
      "{'input': \"Returns the date that is `months` months after `start`\\n\\n    >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\\n    >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\\n    [Row(next_month=datetime.date(2015, 5, 8))]\", 'output': 'def add_months(start, months)'}\n",
      "{'input': \"Returns number of months between dates date1 and date2.\\n    If date1 is later than date2, then the result is positive.\\n    If date1 and date2 are on the same day of month, or both are the last day of month,\\n    returns an integer (time of day will be ignored).\\n    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\\n\\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\\n    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\\n    [Row(months=3.94959677)]\\n    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\\n    [Row(months=3.9495967741935485)]\", 'output': 'def months_between(date1, date2, roundOff=True)'}\n",
      "{'input': 'Converts a :class:`Column` of :class:`pyspark.sql.types.StringType` or\\n    :class:`pyspark.sql.types.TimestampType` into :class:`pyspark.sql.types.DateType`\\n    using the optionally specified format. Specify formats according to\\n    `DateTimeFormatter <https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html>`_. # noqa\\n    By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\\n    is omitted (equivalent to ``col.cast(\"date\")``).\\n\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\',)], [\\'t\\'])\\n    >>> df.select(to_date(df.t).alias(\\'date\\')).collect()\\n    [Row(date=datetime.date(1997, 2, 28))]\\n\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\',)], [\\'t\\'])\\n    >>> df.select(to_date(df.t, \\'yyyy-MM-dd HH:mm:ss\\').alias(\\'date\\')).collect()\\n    [Row(date=datetime.date(1997, 2, 28))]', 'output': 'def to_date(col, format=None)'}\n",
      "{'input': \"Returns timestamp truncated to the unit specified by the format.\\n\\n    :param format: 'year', 'yyyy', 'yy', 'month', 'mon', 'mm',\\n        'day', 'dd', 'hour', 'minute', 'second', 'week', 'quarter'\\n\\n    >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\\n    >>> df.select(date_trunc('year', df.t).alias('year')).collect()\\n    [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\\n    >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\\n    [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\", 'output': 'def date_trunc(format, timestamp)'}\n",
      "{'input': 'Returns the first date which is later than the value of the date column.\\n\\n    Day of the week parameter is case insensitive, and accepts:\\n        \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\".\\n\\n    >>> df = spark.createDataFrame([(\\'2015-07-27\\',)], [\\'d\\'])\\n    >>> df.select(next_day(df.d, \\'Sun\\').alias(\\'date\\')).collect()\\n    [Row(date=datetime.date(2015, 8, 2))]', 'output': 'def next_day(date, dayOfWeek)'}\n",
      "{'input': \"Returns the last day of the month which the given date belongs to.\\n\\n    >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\\n    >>> df.select(last_day(df.d).alias('date')).collect()\\n    [Row(date=datetime.date(1997, 2, 28))]\", 'output': 'def last_day(date)'}\n",
      "{'input': 'Convert time string with given pattern (\\'yyyy-MM-dd HH:mm:ss\\', by default)\\n    to Unix time stamp (in seconds), using the default timezone and the default\\n    locale, return null if fail.\\n\\n    if `timestamp` is None, then it returns current timestamp.\\n\\n    >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\\n    >>> time_df = spark.createDataFrame([(\\'2015-04-08\\',)], [\\'dt\\'])\\n    >>> time_df.select(unix_timestamp(\\'dt\\', \\'yyyy-MM-dd\\').alias(\\'unix_time\\')).collect()\\n    [Row(unix_time=1428476400)]\\n    >>> spark.conf.unset(\"spark.sql.session.timeZone\")', 'output': \"def unix_timestamp(timestamp=None, format='yyyy-MM-dd HH:mm:ss')\"}\n",
      "{'input': 'This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\\n    takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\\n    renders that timestamp as a timestamp in the given time zone.\\n\\n    However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\\n    timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\\n    the given timezone.\\n\\n    This function may return confusing result if the input is a string with timezone, e.g.\\n    \\'2018-03-13T06:18:23+00:00\\'. The reason is that, Spark firstly cast the string to timestamp\\n    according to the timezone in the string, and finally display the result by converting the\\n    timestamp to string according to the session local timezone.\\n\\n    :param timestamp: the column that contains timestamps\\n    :param tz: a string that has the ID of timezone, e.g. \"GMT\", \"America/Los_Angeles\", etc\\n\\n    .. versionchanged:: 2.4\\n       `tz` can take a :class:`Column` containing timezone ID strings.\\n\\n    >>> df = spark.createDataFrame([(\\'1997-02-28 10:30:00\\', \\'JST\\')], [\\'ts\\', \\'tz\\'])\\n    >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias(\\'local_time\\')).collect()\\n    [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\\n    >>> df.select(from_utc_timestamp(df.ts, df.tz).alias(\\'local_time\\')).collect()\\n    [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\\n\\n    .. note:: Deprecated in 3.0. See SPARK-25496', 'output': 'def from_utc_timestamp(timestamp, tz)'}\n",
      "{'input': 'Bucketize rows into one or more time windows given a timestamp specifying column. Window\\n    starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\\n    [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\\n    the order of months are not supported.\\n\\n    The time column must be of :class:`pyspark.sql.types.TimestampType`.\\n\\n    Durations are provided as strings, e.g. \\'1 second\\', \\'1 day 12 hours\\', \\'2 minutes\\'. Valid\\n    interval strings are \\'week\\', \\'day\\', \\'hour\\', \\'minute\\', \\'second\\', \\'millisecond\\', \\'microsecond\\'.\\n    If the ``slideDuration`` is not provided, the windows will be tumbling windows.\\n\\n    The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\\n    window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\\n    past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\\n\\n    The output column will be a struct called \\'window\\' by default with the nested columns \\'start\\'\\n    and \\'end\\', where \\'start\\' and \\'end\\' will be of :class:`pyspark.sql.types.TimestampType`.\\n\\n    >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\\n    >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\\n    >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\\n    ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\\n    [Row(start=u\\'2016-03-11 09:00:05\\', end=u\\'2016-03-11 09:00:10\\', sum=1)]', 'output': 'def window(timeColumn, windowDuration, slideDuration=None, startTime=None)'}\n",
      "{'input': \"Calculates the hash code of given columns, and returns the result as an int column.\\n\\n    >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\\n    [Row(hash=-757602832)]\", 'output': 'def hash(*cols)'}\n",
      "{'input': \"Concatenates multiple input string columns together into a single string column,\\n    using the given separator.\\n\\n    >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\\n    >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\\n    [Row(s=u'abcd-123')]\", 'output': 'def concat_ws(sep, *cols)'}\n",
      "{'input': \"Computes the first argument into a string from a binary using the provided character set\\n    (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\", 'output': 'def decode(col, charset)'}\n",
      "{'input': \"Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\\n    with HALF_EVEN round mode, and returns the result as a string.\\n\\n    :param col: the column name of the numeric value to be formatted\\n    :param d: the N decimal places\\n\\n    >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\\n    [Row(v=u'5.0000')]\", 'output': 'def format_number(col, d)'}\n",
      "{'input': 'Formats the arguments in printf-style and returns the result as a string column.\\n\\n    :param col: the column name of the numeric value to be formatted\\n    :param d: the N decimal places\\n\\n    >>> df = spark.createDataFrame([(5, \"hello\")], [\\'a\\', \\'b\\'])\\n    >>> df.select(format_string(\\'%d %s\\', df.a, df.b).alias(\\'v\\')).collect()\\n    [Row(v=u\\'5 hello\\')]', 'output': 'def format_string(format, *cols)'}\n",
      "{'input': \"Locate the position of the first occurrence of substr column in the given string.\\n    Returns null if either of the arguments are null.\\n\\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\\n        could not be found in str.\\n\\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\\n    >>> df.select(instr(df.s, 'b').alias('s')).collect()\\n    [Row(s=2)]\", 'output': 'def instr(str, substr)'}\n",
      "{'input': \"Substring starts at `pos` and is of length `len` when str is String type or\\n    returns the slice of byte array that starts at `pos` in byte and is of length `len`\\n    when str is Binary type.\\n\\n    .. note:: The position is not zero based, but 1 based index.\\n\\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\\n    >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\\n    [Row(s=u'ab')]\", 'output': 'def substring(str, pos, len)'}\n",
      "{'input': \"Returns the substring from string str before count occurrences of the delimiter delim.\\n    If count is positive, everything the left of the final delimiter (counting from left) is\\n    returned. If count is negative, every to the right of the final delimiter (counting from the\\n    right) is returned. substring_index performs a case-sensitive match when searching for delim.\\n\\n    >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\\n    >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\\n    [Row(s=u'a.b')]\\n    >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\\n    [Row(s=u'b.c.d')]\", 'output': 'def substring_index(str, delim, count)'}\n",
      "{'input': \"Computes the Levenshtein distance of the two given strings.\\n\\n    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\\n    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\\n    [Row(d=3)]\", 'output': 'def levenshtein(left, right)'}\n",
      "{'input': \"Locate the position of the first occurrence of substr in a string column, after position pos.\\n\\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\\n        could not be found in str.\\n\\n    :param substr: a string\\n    :param str: a Column of :class:`pyspark.sql.types.StringType`\\n    :param pos: start position (zero based)\\n\\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\\n    >>> df.select(locate('b', df.s, 1).alias('s')).collect()\\n    [Row(s=2)]\", 'output': 'def locate(substr, str, pos=1)'}\n",
      "{'input': \"Left-pad the string column to width `len` with `pad`.\\n\\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\\n    >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\\n    [Row(s=u'##abcd')]\", 'output': 'def lpad(col, len, pad)'}\n",
      "{'input': \"Repeats a string column n times, and returns it as a new string column.\\n\\n    >>> df = spark.createDataFrame([('ab',)], ['s',])\\n    >>> df.select(repeat(df.s, 3).alias('s')).collect()\\n    [Row(s=u'ababab')]\", 'output': 'def repeat(col, n)'}\n",
      "{'input': \"Splits str around matches of the given pattern.\\n\\n    :param str: a string expression to split\\n    :param pattern: a string representing a regular expression. The regex string should be\\n        a Java regular expression.\\n    :param limit: an integer which controls the number of times `pattern` is applied.\\n\\n        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\\n                         resulting array's last entry will contain all input beyond the last\\n                         matched pattern.\\n        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\\n                          array can be of any size.\\n\\n    .. versionchanged:: 3.0\\n       `split` now takes an optional `limit` field. If not provided, default limit value is -1.\\n\\n    >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\\n    >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\\n    [Row(s=[u'one', u'twoBthreeC'])]\\n    >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\\n    [Row(s=[u'one', u'two', u'three', u''])]\", 'output': 'def split(str, pattern, limit=-1)'}\n",
      "{'input': 'r\"\"\"Extract a specific group matched by a Java regex, from the specified string column.\\n    If the regex did not match, or the specified group did not match, an empty string is returned.\\n\\n    >>> df = spark.createDataFrame([(\\'100-200\\',)], [\\'str\\'])\\n    >>> df.select(regexp_extract(\\'str\\', r\\'(\\\\d+)-(\\\\d+)\\', 1).alias(\\'d\\')).collect()\\n    [Row(d=u\\'100\\')]\\n    >>> df = spark.createDataFrame([(\\'foo\\',)], [\\'str\\'])\\n    >>> df.select(regexp_extract(\\'str\\', r\\'(\\\\d+)\\', 1).alias(\\'d\\')).collect()\\n    [Row(d=u\\'\\')]\\n    >>> df = spark.createDataFrame([(\\'aaaac\\',)], [\\'str\\'])\\n    >>> df.select(regexp_extract(\\'str\\', \\'(a+)(b)?(c)\\', 2).alias(\\'d\\')).collect()\\n    [Row(d=u\\'\\')]', 'output': 'def regexp_extract(str, pattern, idx)'}\n",
      "{'input': 'r\"\"\"Replace all substrings of the specified string value that match regexp with rep.\\n\\n    >>> df = spark.createDataFrame([(\\'100-200\\',)], [\\'str\\'])\\n    >>> df.select(regexp_replace(\\'str\\', r\\'(\\\\d+)\\', \\'--\\').alias(\\'d\\')).collect()\\n    [Row(d=u\\'-----\\')]', 'output': 'def regexp_replace(str, pattern, replacement)'}\n",
      "{'input': 'A function translate any character in the `srcCol` by a character in `matching`.\\n    The characters in `replace` is corresponding to the characters in `matching`.\\n    The translate will happen when any character in the string matching with the character\\n    in the `matching`.\\n\\n    >>> spark.createDataFrame([(\\'translate\\',)], [\\'a\\']).select(translate(\\'a\\', \"rnlt\", \"123\") \\\\\\\\\\n    ...     .alias(\\'r\\')).collect()\\n    [Row(r=u\\'1a2s3ae\\')]', 'output': 'def translate(srcCol, matching, replace)'}\n",
      "{'input': 'Collection function: returns true if the arrays contain any common non-null element; if not,\\n    returns null if both the arrays are non-empty and any of them contains a null element; returns\\n    false otherwise.\\n\\n    >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], [\\'x\\', \\'y\\'])\\n    >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\\n    [Row(overlap=True), Row(overlap=False)]', 'output': 'def arrays_overlap(a1, a2)'}\n",
      "{'input': 'Collection function: returns an array containing  all the elements in `x` from index `start`\\n    (or starting from the end if `start` is negative) with the specified `length`.\\n    >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], [\\'x\\'])\\n    >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\\n    [Row(sliced=[2, 3]), Row(sliced=[5])]', 'output': 'def slice(x, start, length)'}\n",
      "{'input': 'Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\\n    `null_replacement` if set, otherwise they are ignored.\\n\\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], [\\'data\\'])\\n    >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\\n    [Row(joined=u\\'a,b,c\\'), Row(joined=u\\'a\\')]\\n    >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\\n    [Row(joined=u\\'a,b,c\\'), Row(joined=u\\'a,NULL\\')]', 'output': 'def array_join(col, delimiter, null_replacement=None)'}\n",
      "{'input': 'Concatenates multiple input columns together into a single column.\\n    The function works with strings, binary and compatible array columns.\\n\\n    >>> df = spark.createDataFrame([(\\'abcd\\',\\'123\\')], [\\'s\\', \\'d\\'])\\n    >>> df.select(concat(df.s, df.d).alias(\\'s\\')).collect()\\n    [Row(s=u\\'abcd123\\')]\\n\\n    >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], [\\'a\\', \\'b\\', \\'c\\'])\\n    >>> df.select(concat(df.a, df.b, df.c).alias(\"arr\")).collect()\\n    [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]', 'output': 'def concat(*cols)'}\n",
      "{'input': 'Collection function: Locates the position of the first occurrence of the given value\\n    in the given array. Returns null if either of the arguments are null.\\n\\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if the given\\n        value could not be found in the array.\\n\\n    >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], [\\'data\\'])\\n    >>> df.select(array_position(df.data, \"a\")).collect()\\n    [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]', 'output': 'def array_position(col, value)'}\n",
      "{'input': 'Collection function: Returns element of array at given index in extraction if col is array.\\n    Returns value for the given key in extraction if col is map.\\n\\n    :param col: name of column containing array or map\\n    :param extraction: index to check for in array or key to check for in map\\n\\n    .. note:: The position is not zero based, but 1 based index.\\n\\n    >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], [\\'data\\'])\\n    >>> df.select(element_at(df.data, 1)).collect()\\n    [Row(element_at(data, 1)=u\\'a\\'), Row(element_at(data, 1)=None)]\\n\\n    >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},), ({},)], [\\'data\\'])\\n    >>> df.select(element_at(df.data, \"a\")).collect()\\n    [Row(element_at(data, a)=1.0), Row(element_at(data, a)=None)]', 'output': 'def element_at(col, extraction)'}\n",
      "{'input': \"Collection function: Remove all elements that equal to element from the given array.\\n\\n    :param col: name of column containing array\\n    :param element: element to be removed from the array\\n\\n    >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\\n    >>> df.select(array_remove(df.data, 1)).collect()\\n    [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\", 'output': 'def array_remove(col, element)'}\n",
      "{'input': 'Returns a new row for each element in the given array or map.\\n    Uses the default column name `col` for elements in the array and\\n    `key` and `value` for elements in the map unless specified otherwise.\\n\\n    >>> from pyspark.sql import Row\\n    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\\n    >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\\n    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\\n\\n    >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\\n    +---+-----+\\n    |key|value|\\n    +---+-----+\\n    |  a|    b|\\n    +---+-----+', 'output': 'def explode(col)'}\n",
      "{'input': 'Extracts json object from a json string based on json path specified, and returns json string\\n    of the extracted json object. It will return null if the input json string is invalid.\\n\\n    :param col: string column in json format\\n    :param path: path to the json object to extract\\n\\n    >>> data = [(\"1\", \\'\\'\\'{\"f1\": \"value1\", \"f2\": \"value2\"}\\'\\'\\'), (\"2\", \\'\\'\\'{\"f1\": \"value12\"}\\'\\'\\')]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\\n    >>> df.select(df.key, get_json_object(df.jstring, \\'$.f1\\').alias(\"c0\"), \\\\\\\\\\n    ...                   get_json_object(df.jstring, \\'$.f2\\').alias(\"c1\") ).collect()\\n    [Row(key=u\\'1\\', c0=u\\'value1\\', c1=u\\'value2\\'), Row(key=u\\'2\\', c0=u\\'value12\\', c1=None)]', 'output': 'def get_json_object(col, path)'}\n",
      "{'input': 'Creates a new row for a json column according to the given field names.\\n\\n    :param col: string column in json format\\n    :param fields: list of fields to extract\\n\\n    >>> data = [(\"1\", \\'\\'\\'{\"f1\": \"value1\", \"f2\": \"value2\"}\\'\\'\\'), (\"2\", \\'\\'\\'{\"f1\": \"value12\"}\\'\\'\\')]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\\n    >>> df.select(df.key, json_tuple(df.jstring, \\'f1\\', \\'f2\\')).collect()\\n    [Row(key=u\\'1\\', c0=u\\'value1\\', c1=u\\'value2\\'), Row(key=u\\'2\\', c0=u\\'value12\\', c1=None)]', 'output': 'def json_tuple(col, *fields)'}\n",
      "{'input': 'Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\\n    as keys type, :class:`StructType` or :class:`ArrayType` with\\n    the specified schema. Returns `null`, in the case of an unparseable string.\\n\\n    :param col: string column in json format\\n    :param schema: a StructType or ArrayType of StructType to use when parsing the json column.\\n    :param options: options to control parsing. accepts the same options as the json datasource\\n\\n    .. note:: Since Spark 2.3, the DDL-formatted string or a JSON format string is also\\n              supported for ``schema``.\\n\\n    >>> from pyspark.sql.types import *\\n    >>> data = [(1, \\'\\'\\'{\"a\": 1}\\'\\'\\')]\\n    >>> schema = StructType([StructField(\"a\", IntegerType())])\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\\n    [Row(json=Row(a=1))]\\n    >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\\n    [Row(json=Row(a=1))]\\n    >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\\n    [Row(json={u\\'a\\': 1})]\\n    >>> data = [(1, \\'\\'\\'[{\"a\": 1}]\\'\\'\\')]\\n    >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\\n    [Row(json=[Row(a=1)])]\\n    >>> schema = schema_of_json(lit(\\'\\'\\'{\"a\": 0}\\'\\'\\'))\\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\\n    [Row(json=Row(a=None))]\\n    >>> data = [(1, \\'\\'\\'[1, 2, 3]\\'\\'\\')]\\n    >>> schema = ArrayType(IntegerType())\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\\n    [Row(json=[1, 2, 3])]', 'output': 'def from_json(col, schema, options={})'}\n",
      "{'input': 'Parses a JSON string and infers its schema in DDL format.\\n\\n    :param json: a JSON string or a string literal containing a JSON string.\\n    :param options: options to control parsing. accepts the same options as the JSON datasource\\n\\n    .. versionchanged:: 3.0\\n       It accepts `options` parameter to control schema inferring.\\n\\n    >>> df = spark.range(1)\\n    >>> df.select(schema_of_json(lit(\\'{\"a\": 0}\\')).alias(\"json\")).collect()\\n    [Row(json=u\\'struct<a:bigint>\\')]\\n    >>> schema = schema_of_json(\\'{a: 1}\\', {\\'allowUnquotedFieldNames\\':\\'true\\'})\\n    >>> df.select(schema.alias(\"json\")).collect()\\n    [Row(json=u\\'struct<a:bigint>\\')]', 'output': 'def schema_of_json(json, options={})'}\n",
      "{'input': 'Parses a CSV string and infers its schema in DDL format.\\n\\n    :param col: a CSV string or a string literal containing a CSV string.\\n    :param options: options to control parsing. accepts the same options as the CSV datasource\\n\\n    >>> df = spark.range(1)\\n    >>> df.select(schema_of_csv(lit(\\'1|a\\'), {\\'sep\\':\\'|\\'}).alias(\"csv\")).collect()\\n    [Row(csv=u\\'struct<_c0:int,_c1:string>\\')]\\n    >>> df.select(schema_of_csv(\\'1|a\\', {\\'sep\\':\\'|\\'}).alias(\"csv\")).collect()\\n    [Row(csv=u\\'struct<_c0:int,_c1:string>\\')]', 'output': 'def schema_of_csv(csv, options={})'}\n",
      "{'input': 'Converts a column containing a :class:`StructType` into a CSV string.\\n    Throws an exception, in the case of an unsupported type.\\n\\n    :param col: name of column containing a struct.\\n    :param options: options to control converting. accepts the same options as the CSV datasource.\\n\\n    >>> from pyspark.sql import Row\\n    >>> data = [(1, Row(name=\\'Alice\\', age=2))]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\\n    [Row(csv=u\\'2,Alice\\')]', 'output': 'def to_csv(col, options={})'}\n",
      "{'input': \"Collection function: returns the length of the array or map stored in the column.\\n\\n    :param col: name of column or expression\\n\\n    >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\\n    >>> df.select(size(df.data)).collect()\\n    [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\", 'output': 'def size(col)'}\n",
      "{'input': \"Collection function: sorts the input array in ascending or descending order according\\n    to the natural ordering of the array elements. Null elements will be placed at the beginning\\n    of the returned array in ascending order or at the end of the returned array in descending\\n    order.\\n\\n    :param col: name of column or expression\\n\\n    >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\\n    >>> df.select(sort_array(df.data).alias('r')).collect()\\n    [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\\n    >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\\n    [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\", 'output': 'def sort_array(col, asc=True)'}\n",
      "{'input': \"Collection function: creates an array containing a column repeated count times.\\n\\n    >>> df = spark.createDataFrame([('ab',)], ['data'])\\n    >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\\n    [Row(r=[u'ab', u'ab', u'ab'])]\", 'output': 'def array_repeat(col, count)'}\n",
      "{'input': 'Returns the union of all the given maps.\\n\\n    :param cols: list of column names (string) or list of :class:`Column` expressions\\n\\n    >>> from pyspark.sql.functions import map_concat\\n    >>> df = spark.sql(\"SELECT map(1, \\'a\\', 2, \\'b\\') as map1, map(3, \\'c\\', 1, \\'d\\') as map2\")\\n    >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\\n    +------------------------+\\n    |map3                    |\\n    +------------------------+\\n    |[1 -> d, 2 -> b, 3 -> c]|\\n    +------------------------+', 'output': 'def map_concat(*cols)'}\n",
      "{'input': \"Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\\n    If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\\n    otherwise -1.\\n\\n    >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\\n    >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\\n    [Row(r=[-2, -1, 0, 1, 2])]\\n    >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\\n    >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\\n    [Row(r=[4, 2, 0, -2, -4])]\", 'output': 'def sequence(start, stop, step=None)'}\n",
      "{'input': 'Parses a column containing a CSV string to a row with the specified schema.\\n    Returns `null`, in the case of an unparseable string.\\n\\n    :param col: string column in CSV format\\n    :param schema: a string with schema in DDL format to use when parsing the CSV column.\\n    :param options: options to control parsing. accepts the same options as the CSV datasource\\n\\n    >>> data = [(\"1,2,3\",)]\\n    >>> df = spark.createDataFrame(data, (\"value\",))\\n    >>> df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\\n    [Row(csv=Row(a=1, b=2, c=3))]\\n    >>> value = data[0][0]\\n    >>> df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\\n    [Row(csv=Row(_c0=1, _c1=2, _c2=3))]', 'output': 'def from_csv(col, schema, options={})'}\n",
      "{'input': 'Creates a user defined function (UDF).\\n\\n    .. note:: The user-defined functions are considered deterministic by default. Due to\\n        optimization, duplicate invocations may be eliminated or the function may even be invoked\\n        more times than it is present in the query. If your function is not deterministic, call\\n        `asNondeterministic` on the user defined function. E.g.:\\n\\n    >>> from pyspark.sql.types import IntegerType\\n    >>> import random\\n    >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\\n\\n    .. note:: The user-defined functions do not support conditional expressions or short circuiting\\n        in boolean expressions and it ends up with being executed all internally. If the functions\\n        can fail on special rows, the workaround is to incorporate the condition into the functions.\\n\\n    .. note:: The user-defined functions do not take keyword arguments on the calling side.\\n\\n    :param f: python function if used as a standalone function\\n    :param returnType: the return type of the user-defined function. The value can be either a\\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\\n\\n    >>> from pyspark.sql.types import IntegerType\\n    >>> slen = udf(lambda s: len(s), IntegerType())\\n    >>> @udf\\n    ... def to_upper(s):\\n    ...     if s is not None:\\n    ...         return s.upper()\\n    ...\\n    >>> @udf(returnType=IntegerType())\\n    ... def add_one(x):\\n    ...     if x is not None:\\n    ...         return x + 1\\n    ...\\n    >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\\n    >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\\n    +----------+--------------+------------+\\n    |slen(name)|to_upper(name)|add_one(age)|\\n    +----------+--------------+------------+\\n    |         8|      JOHN DOE|          22|\\n    +----------+--------------+------------+', 'output': 'def udf(f=None, returnType=StringType()'}\n",
      "{'input': 'Creates a vectorized user defined function (UDF).\\n\\n    :param f: user-defined function. A python function if used as a standalone function\\n    :param returnType: the return type of the user-defined function. The value can be either a\\n        :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\\n    :param functionType: an enum value in :class:`pyspark.sql.functions.PandasUDFType`.\\n                         Default: SCALAR.\\n\\n    .. note:: Experimental\\n\\n    The function type of the UDF can be one of the following:\\n\\n    1. SCALAR\\n\\n       A scalar UDF defines a transformation: One or more `pandas.Series` -> A `pandas.Series`.\\n       The length of the returned `pandas.Series` must be of the same as the input `pandas.Series`.\\n       If the return type is :class:`StructType`, the returned value should be a `pandas.DataFrame`.\\n\\n       :class:`MapType`, nested :class:`StructType` are currently not supported as output types.\\n\\n       Scalar UDFs are used with :meth:`pyspark.sql.DataFrame.withColumn` and\\n       :meth:`pyspark.sql.DataFrame.select`.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> from pyspark.sql.types import IntegerType, StringType\\n       >>> slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\\n       >>> @pandas_udf(StringType())  # doctest: +SKIP\\n       ... def to_upper(s):\\n       ...     return s.str.upper()\\n       ...\\n       >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\\n       ... def add_one(x):\\n       ...     return x + 1\\n       ...\\n       >>> df = spark.createDataFrame([(1, \"John Doe\", 21)],\\n       ...                            (\"id\", \"name\", \"age\"))  # doctest: +SKIP\\n       >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")) \\\\\\\\\\n       ...     .show()  # doctest: +SKIP\\n       +----------+--------------+------------+\\n       |slen(name)|to_upper(name)|add_one(age)|\\n       +----------+--------------+------------+\\n       |         8|      JOHN DOE|          22|\\n       +----------+--------------+------------+\\n       >>> @pandas_udf(\"first string, last string\")  # doctest: +SKIP\\n       ... def split_expand(n):\\n       ...     return n.str.split(expand=True)\\n       >>> df.select(split_expand(\"name\")).show()  # doctest: +SKIP\\n       +------------------+\\n       |split_expand(name)|\\n       +------------------+\\n       |       [John, Doe]|\\n       +------------------+\\n\\n       .. note:: The length of `pandas.Series` within a scalar UDF is not that of the whole input\\n           column, but is the length of an internal batch used for each call to the function.\\n           Therefore, this can be used, for example, to ensure the length of each returned\\n           `pandas.Series`, and can not be used as the column length.\\n\\n    2. GROUPED_MAP\\n\\n       A grouped map UDF defines transformation: A `pandas.DataFrame` -> A `pandas.DataFrame`\\n       The returnType should be a :class:`StructType` describing the schema of the returned\\n       `pandas.DataFrame`. The column labels of the returned `pandas.DataFrame` must either match\\n       the field names in the defined returnType schema if specified as strings, or match the\\n       field data types by position if not strings, e.g. integer indices.\\n       The length of the returned `pandas.DataFrame` can be arbitrary.\\n\\n       Grouped map UDFs are used with :meth:`pyspark.sql.GroupedData.apply`.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))  # doctest: +SKIP\\n       >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       ... def normalize(pdf):\\n       ...     v = pdf.v\\n       ...     return pdf.assign(v=(v - v.mean()) / v.std())\\n       >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\\n       +---+-------------------+\\n       | id|                  v|\\n       +---+-------------------+\\n       |  1|-0.7071067811865475|\\n       |  1| 0.7071067811865475|\\n       |  2|-0.8320502943378437|\\n       |  2|-0.2773500981126146|\\n       |  2| 1.1094003924504583|\\n       +---+-------------------+\\n\\n       Alternatively, the user can define a function that takes two arguments.\\n       In this case, the grouping key(s) will be passed as the first argument and the data will\\n       be passed as the second argument. The grouping key(s) will be passed as a tuple of numpy\\n       data types, e.g., `numpy.int32` and `numpy.float64`. The data will still be passed in\\n       as a `pandas.DataFrame` containing all columns from the original Spark DataFrame.\\n       This is useful when the user does not want to hardcode grouping key(s) in the function.\\n\\n       >>> import pandas as pd  # doctest: +SKIP\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))  # doctest: +SKIP\\n       >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       ... def mean_udf(key, pdf):\\n       ...     # key is a tuple of one numpy.int64, which is the value\\n       ...     # of \\'id\\' for the current group\\n       ...     return pd.DataFrame([key + (pdf.v.mean(),)])\\n       >>> df.groupby(\\'id\\').apply(mean_udf).show()  # doctest: +SKIP\\n       +---+---+\\n       | id|  v|\\n       +---+---+\\n       |  1|1.5|\\n       |  2|6.0|\\n       +---+---+\\n       >>> @pandas_udf(\\n       ...    \"id long, `ceil(v / 2)` long, v double\",\\n       ...    PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n       >>> def sum_udf(key, pdf):\\n       ...     # key is a tuple of two numpy.int64s, which is the values\\n       ...     # of \\'id\\' and \\'ceil(df.v / 2)\\' for the current group\\n       ...     return pd.DataFrame([key + (pdf.v.sum(),)])\\n       >>> df.groupby(df.id, ceil(df.v / 2)).apply(sum_udf).show()  # doctest: +SKIP\\n       +---+-----------+----+\\n       | id|ceil(v / 2)|   v|\\n       +---+-----------+----+\\n       |  2|          5|10.0|\\n       |  1|          1| 3.0|\\n       |  2|          3| 5.0|\\n       |  2|          2| 3.0|\\n       +---+-----------+----+\\n\\n       .. note:: If returning a new `pandas.DataFrame` constructed with a dictionary, it is\\n           recommended to explicitly index the columns by name to ensure the positions are correct,\\n           or alternatively use an `OrderedDict`.\\n           For example, `pd.DataFrame({\\'id\\': ids, \\'a\\': data}, columns=[\\'id\\', \\'a\\'])` or\\n           `pd.DataFrame(OrderedDict([(\\'id\\', ids), (\\'a\\', data)]))`.\\n\\n       .. seealso:: :meth:`pyspark.sql.GroupedData.apply`\\n\\n    3. GROUPED_AGG\\n\\n       A grouped aggregate UDF defines a transformation: One or more `pandas.Series` -> A scalar\\n       The `returnType` should be a primitive data type, e.g., :class:`DoubleType`.\\n       The returned scalar can be either a python primitive type, e.g., `int` or `float`\\n       or a numpy data type, e.g., `numpy.int64` or `numpy.float64`.\\n\\n       :class:`MapType` and :class:`StructType` are currently not supported as output types.\\n\\n       Group aggregate UDFs are used with :meth:`pyspark.sql.GroupedData.agg` and\\n       :class:`pyspark.sql.Window`\\n\\n       This example shows using grouped aggregated UDFs with groupby:\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))\\n       >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\\n       ... def mean_udf(v):\\n       ...     return v.mean()\\n       >>> df.groupby(\"id\").agg(mean_udf(df[\\'v\\'])).show()  # doctest: +SKIP\\n       +---+-----------+\\n       | id|mean_udf(v)|\\n       +---+-----------+\\n       |  1|        1.5|\\n       |  2|        6.0|\\n       +---+-----------+\\n\\n       This example shows using grouped aggregated UDFs as window functions.\\n\\n       >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n       >>> from pyspark.sql import Window\\n       >>> df = spark.createDataFrame(\\n       ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n       ...     (\"id\", \"v\"))\\n       >>> @pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\\n       ... def mean_udf(v):\\n       ...     return v.mean()\\n       >>> w = (Window.partitionBy(\\'id\\')\\n       ...            .orderBy(\\'v\\')\\n       ...            .rowsBetween(-1, 0))\\n       >>> df.withColumn(\\'mean_v\\', mean_udf(df[\\'v\\']).over(w)).show()  # doctest: +SKIP\\n       +---+----+------+\\n       | id|   v|mean_v|\\n       +---+----+------+\\n       |  1| 1.0|   1.0|\\n       |  1| 2.0|   1.5|\\n       |  2| 3.0|   3.0|\\n       |  2| 5.0|   4.0|\\n       |  2|10.0|   7.5|\\n       +---+----+------+\\n\\n       .. note:: For performance reasons, the input series to window functions are not copied.\\n            Therefore, mutating the input series is not allowed and will cause incorrect results.\\n            For the same reason, users should also not rely on the index of the input series.\\n\\n       .. seealso:: :meth:`pyspark.sql.GroupedData.agg` and :class:`pyspark.sql.Window`\\n\\n    .. note:: The user-defined functions are considered deterministic by default. Due to\\n        optimization, duplicate invocations may be eliminated or the function may even be invoked\\n        more times than it is present in the query. If your function is not deterministic, call\\n        `asNondeterministic` on the user defined function. E.g.:\\n\\n    >>> @pandas_udf(\\'double\\', PandasUDFType.SCALAR)  # doctest: +SKIP\\n    ... def random(v):\\n    ...     import numpy as np\\n    ...     import pandas as pd\\n    ...     return pd.Series(np.random.randn(len(v))\\n    >>> random = random.asNondeterministic()  # doctest: +SKIP\\n\\n    .. note:: The user-defined functions do not support conditional expressions or short circuiting\\n        in boolean expressions and it ends up with being executed all internally. If the functions\\n        can fail on special rows, the workaround is to incorporate the condition into the functions.\\n\\n    .. note:: The user-defined functions do not take keyword arguments on the calling side.\\n\\n    .. note:: The data type of returned `pandas.Series` from the user-defined functions should be\\n        matched with defined returnType (see :meth:`types.to_arrow_type` and\\n        :meth:`types.from_arrow_type`). When there is mismatch between them, Spark might do\\n        conversion on returned data. The conversion is not guaranteed to be correct and results\\n        should be checked for accuracy by users.', 'output': 'def pandas_udf(f=None, returnType=None, functionType=None)'}\n",
      "{'input': 'A wrapper over str(), but converts bool values to lower case strings.\\n    If None is given, just returns None, instead of converting it to string \"None\".', 'output': 'def to_str(value)'}\n",
      "{'input': 'Set named options (filter out those the value is None)', 'output': 'def _set_opts(self, schema=None, **options)'}\n",
      "{'input': \"Specifies the input data source format.\\n\\n        :param source: string, name of the data source, e.g. 'json', 'parquet'.\\n\\n        >>> df = spark.read.format('json').load('python/test_support/sql/people.json')\\n        >>> df.dtypes\\n        [('age', 'bigint'), ('name', 'string')]\", 'output': 'def format(self, source)'}\n",
      "{'input': 'Specifies the input schema.\\n\\n        Some data sources (e.g. JSON) can infer the input schema automatically from data.\\n        By specifying the schema here, the underlying data source can skip the schema\\n        inference step, and thus speed up data loading.\\n\\n        :param schema: a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\\n                       (For example ``col0 INT, col1 DOUBLE``).\\n\\n        >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")', 'output': 'def schema(self, schema)'}\n",
      "{'input': \"Adds an input option for the underlying data source.\\n\\n        You can set the following option(s) for reading files:\\n            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\\n                in the JSON/CSV datasources or partition values.\\n                If it isn't set, it uses the default value, session local timezone.\", 'output': 'def option(self, key, value)'}\n",
      "{'input': \"Adds input options for the underlying data source.\\n\\n        You can set the following option(s) for reading files:\\n            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\\n                in the JSON/CSV datasources or partition values.\\n                If it isn't set, it uses the default value, session local timezone.\", 'output': 'def options(self, **options)'}\n",
      "{'input': 'Loads data from a data source and returns it as a :class`DataFrame`.\\n\\n        :param path: optional string or a list of string for file-system backed data sources.\\n        :param format: optional string for format of the data source. Default to \\'parquet\\'.\\n        :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema\\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\\n        :param options: all other string options\\n\\n        >>> df = spark.read.format(\"parquet\").load(\\'python/test_support/sql/parquet_partitioned\\',\\n        ...     opt1=True, opt2=1, opt3=\\'str\\')\\n        >>> df.dtypes\\n        [(\\'name\\', \\'string\\'), (\\'year\\', \\'int\\'), (\\'month\\', \\'int\\'), (\\'day\\', \\'int\\')]\\n\\n        >>> df = spark.read.format(\\'json\\').load([\\'python/test_support/sql/people.json\\',\\n        ...     \\'python/test_support/sql/people1.json\\'])\\n        >>> df.dtypes\\n        [(\\'age\\', \\'bigint\\'), (\\'aka\\', \\'string\\'), (\\'name\\', \\'string\\')]', 'output': 'def load(self, path=None, format=None, schema=None, **options)'}\n",
      "{'input': \"Loads JSON files and returns the results as a :class:`DataFrame`.\\n\\n        `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\\n        For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\\n\\n        If the ``schema`` parameter is not specified, this function goes\\n        through the input once to determine the input schema.\\n\\n        :param path: string represents path to the JSON dataset, or a list of paths,\\n                     or RDD of Strings storing JSON objects.\\n        :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema or\\n                       a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\\n        :param primitivesAsString: infers all primitive values as a string type. If None is set,\\n                                   it uses the default value, ``false``.\\n        :param prefersDecimal: infers all floating-point values as a decimal type. If the values\\n                               do not fit in decimal, then it infers them as doubles. If None is\\n                               set, it uses the default value, ``false``.\\n        :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\\n                              it uses the default value, ``false``.\\n        :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\\n                                        it uses the default value, ``false``.\\n        :param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\\n                                        set, it uses the default value, ``true``.\\n        :param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\\n                                        set, it uses the default value, ``false``.\\n        :param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\\n                                                   using backslash quoting mechanism. If None is\\n                                                   set, it uses the default value, ``false``.\\n        :param mode: allows a mode for dealing with corrupt records during parsing. If None is\\n                     set, it uses the default value, ``PERMISSIVE``.\\n\\n                * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\\\\n                  into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\\\\n                  fields to ``null``. To keep corrupt records, an user can set a string type \\\\\\n                  field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\\\\n                  schema does not have the field, it drops corrupt records during parsing. \\\\\\n                  When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord`` \\\\\\n                  field in an output schema.\\n                *  ``DROPMALFORMED`` : ignores the whole corrupted records.\\n                *  ``FAILFAST`` : throws an exception when it meets corrupted records.\\n\\n        :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\\n                                          created by ``PERMISSIVE`` mode. This overrides\\n                                          ``spark.sql.columnNameOfCorruptRecord``. If None is set,\\n                                          it uses the value specified in\\n                                          ``spark.sql.columnNameOfCorruptRecord``.\\n        :param dateFormat: sets the string that indicates a date format. Custom date formats\\n                           follow the formats at ``java.time.format.DateTimeFormatter``. This\\n                           applies to date type. If None is set, it uses the\\n                           default value, ``yyyy-MM-dd``.\\n        :param timestampFormat: sets the string that indicates a timestamp format.\\n                                Custom date formats follow the formats at\\n                                ``java.time.format.DateTimeFormatter``.\\n                                This applies to timestamp type. If None is set, it uses the\\n                                default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\\n        :param multiLine: parse one record, which may span multiple lines, per file. If None is\\n                          set, it uses the default value, ``false``.\\n        :param allowUnquotedControlChars: allows JSON Strings to contain unquoted control\\n                                          characters (ASCII characters with value less than 32,\\n                                          including tab and line feed characters) or not.\\n        :param encoding: allows to forcibly set one of standard basic or extended encoding for\\n                         the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\\n                         the encoding of input JSON will be detected automatically\\n                         when the multiLine option is set to ``true``.\\n        :param lineSep: defines the line separator that should be used for parsing. If None is\\n                        set, it covers all ``\\\\\\\\r``, ``\\\\\\\\r\\\\\\\\n`` and ``\\\\\\\\n``.\\n        :param samplingRatio: defines fraction of input JSON objects used for schema inferring.\\n                              If None is set, it uses the default value, ``1.0``.\\n        :param dropFieldIfAllNull: whether to ignore column of all null values or empty\\n                                   array/struct during schema inference. If None is set, it\\n                                   uses the default value, ``false``.\\n        :param locale: sets a locale as language tag in IETF BCP 47 format. If None is set,\\n                       it uses the default value, ``en-US``. For instance, ``locale`` is used while\\n                       parsing dates and timestamps.\\n\\n        >>> df1 = spark.read.json('python/test_support/sql/people.json')\\n        >>> df1.dtypes\\n        [('age', 'bigint'), ('name', 'string')]\\n        >>> rdd = sc.textFile('python/test_support/sql/people.json')\\n        >>> df2 = spark.read.json(rdd)\\n        >>> df2.dtypes\\n        [('age', 'bigint'), ('name', 'string')]\", 'output': 'def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\\n             allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None,\\n             allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None,\\n             mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None,\\n             multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None,\\n             dropFieldIfAllNull=None, encoding=None, locale=None)'}\n",
      "{'input': \"Loads Parquet files, returning the result as a :class:`DataFrame`.\\n\\n        You can set the following Parquet-specific option(s) for reading Parquet files:\\n            * ``mergeSchema``: sets whether we should merge schemas collected from all \\\\\\n                Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``. \\\\\\n                The default value is specified in ``spark.sql.parquet.mergeSchema``.\\n\\n        >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\\n        >>> df.dtypes\\n        [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\", 'output': 'def parquet(self, *paths)'}\n",
      "{'input': 'Loads text files and returns a :class:`DataFrame` whose schema starts with a\\n        string column named \"value\", and followed by partitioned columns if there\\n        are any.\\n        The text files must be encoded as UTF-8.\\n\\n        By default, each line in the text file is a new row in the resulting DataFrame.\\n\\n        :param paths: string, or list of strings, for input path(s).\\n        :param wholetext: if true, read each file from input path(s) as a single row.\\n        :param lineSep: defines the line separator that should be used for parsing. If None is\\n                        set, it covers all ``\\\\\\\\r``, ``\\\\\\\\r\\\\\\\\n`` and ``\\\\\\\\n``.\\n\\n        >>> df = spark.read.text(\\'python/test_support/sql/text-test.txt\\')\\n        >>> df.collect()\\n        [Row(value=u\\'hello\\'), Row(value=u\\'this\\')]\\n        >>> df = spark.read.text(\\'python/test_support/sql/text-test.txt\\', wholetext=True)\\n        >>> df.collect()\\n        [Row(value=u\\'hello\\\\\\\\nthis\\')]', 'output': 'def text(self, paths, wholetext=False, lineSep=None)'}\n",
      "{'input': 'r\"\"\"Loads a CSV file and returns the result as a  :class:`DataFrame`.\\n\\n        This function will go through the input once to determine the input schema if\\n        ``inferSchema`` is enabled. To avoid going through the entire data once, disable\\n        ``inferSchema`` option or specify the schema explicitly using ``schema``.\\n\\n        :param path: string, or list of strings, for input path(s),\\n                     or RDD of Strings storing CSV rows.\\n        :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\\n        :param sep: sets a single character as a separator for each field and value.\\n                    If None is set, it uses the default value, ``,``.\\n        :param encoding: decodes the CSV files by the given encoding type. If None is set,\\n                         it uses the default value, ``UTF-8``.\\n        :param quote: sets a single character used for escaping quoted values where the\\n                      separator can be part of the value. If None is set, it uses the default\\n                      value, ``\"``. If you would like to turn off quotations, you need to set an\\n                      empty string.\\n        :param escape: sets a single character used for escaping quotes inside an already\\n                       quoted value. If None is set, it uses the default value, ``\\\\``.\\n        :param comment: sets a single character used for skipping lines beginning with this\\n                        character. By default (None), it is disabled.\\n        :param header: uses the first line as names of columns. If None is set, it uses the\\n                       default value, ``false``.\\n        :param inferSchema: infers the input schema automatically from data. It requires one extra\\n                       pass over the data. If None is set, it uses the default value, ``false``.\\n        :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\\n                              forcibly applied to datasource files, and headers in CSV files will be\\n                              ignored. If the option is set to ``false``, the schema will be\\n                              validated against all headers in CSV files or the first header in RDD\\n                              if the ``header`` option is set to ``true``. Field names in the schema\\n                              and column names in CSV headers are checked by their positions\\n                              taking into account ``spark.sql.caseSensitive``. If None is set,\\n                              ``true`` is used by default. Though the default value is ``true``,\\n                              it is recommended to disable the ``enforceSchema`` option\\n                              to avoid incorrect results.\\n        :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\\n                                        values being read should be skipped. If None is set, it\\n                                        uses the default value, ``false``.\\n        :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\\n                                         values being read should be skipped. If None is set, it\\n                                         uses the default value, ``false``.\\n        :param nullValue: sets the string representation of a null value. If None is set, it uses\\n                          the default value, empty string. Since 2.0.1, this ``nullValue`` param\\n                          applies to all supported types including the string type.\\n        :param nanValue: sets the string representation of a non-number value. If None is set, it\\n                         uses the default value, ``NaN``.\\n        :param positiveInf: sets the string representation of a positive infinity value. If None\\n                            is set, it uses the default value, ``Inf``.\\n        :param negativeInf: sets the string representation of a negative infinity value. If None\\n                            is set, it uses the default value, ``Inf``.\\n        :param dateFormat: sets the string that indicates a date format. Custom date formats\\n                           follow the formats at ``java.time.format.DateTimeFormatter``. This\\n                           applies to date type. If None is set, it uses the\\n                           default value, ``yyyy-MM-dd``.\\n        :param timestampFormat: sets the string that indicates a timestamp format.\\n                                Custom date formats follow the formats at\\n                                ``java.time.format.DateTimeFormatter``.\\n                                This applies to timestamp type. If None is set, it uses the\\n                                default value, ``yyyy-MM-dd\\'T\\'HH:mm:ss.SSSXXX``.\\n        :param maxColumns: defines a hard limit of how many columns a record can have. If None is\\n                           set, it uses the default value, ``20480``.\\n        :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\\n                                  value being read. If None is set, it uses the default value,\\n                                  ``-1`` meaning unlimited length.\\n        :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\\n                                            If specified, it is ignored.\\n        :param mode: allows a mode for dealing with corrupt records during parsing. If None is\\n                     set, it uses the default value, ``PERMISSIVE``.\\n\\n                * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\\\\n                  into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\\\\n                  fields to ``null``. To keep corrupt records, an user can set a string type \\\\\\n                  field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\\\\n                  schema does not have the field, it drops corrupt records during parsing. \\\\\\n                  A record with less/more tokens than schema is not a corrupted record to CSV. \\\\\\n                  When it meets a record having fewer tokens than the length of the schema, \\\\\\n                  sets ``null`` to extra fields. When the record has more tokens than the \\\\\\n                  length of the schema, it drops extra tokens.\\n                * ``DROPMALFORMED`` : ignores the whole corrupted records.\\n                * ``FAILFAST`` : throws an exception when it meets corrupted records.\\n\\n        :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\\n                                          created by ``PERMISSIVE`` mode. This overrides\\n                                          ``spark.sql.columnNameOfCorruptRecord``. If None is set,\\n                                          it uses the value specified in\\n                                          ``spark.sql.columnNameOfCorruptRecord``.\\n        :param multiLine: parse records, which may span multiple lines. If None is\\n                          set, it uses the default value, ``false``.\\n        :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\\n                                          the quote character. If None is set, the default value is\\n                                          escape character when escape and quote characters are\\n                                          different, ``\\\\0`` otherwise.\\n        :param samplingRatio: defines fraction of rows used for schema inferring.\\n                              If None is set, it uses the default value, ``1.0``.\\n        :param emptyValue: sets the string representation of an empty value. If None is set, it uses\\n                           the default value, empty string.\\n        :param locale: sets a locale as language tag in IETF BCP 47 format. If None is set,\\n                       it uses the default value, ``en-US``. For instance, ``locale`` is used while\\n                       parsing dates and timestamps.\\n        :param lineSep: defines the line separator that should be used for parsing. If None is\\n                        set, it covers all ``\\\\\\\\r``, ``\\\\\\\\r\\\\\\\\n`` and ``\\\\\\\\n``.\\n                        Maximum length is 1 character.\\n\\n        >>> df = spark.read.csv(\\'python/test_support/sql/ages.csv\\')\\n        >>> df.dtypes\\n        [(\\'_c0\\', \\'string\\'), (\\'_c1\\', \\'string\\')]\\n        >>> rdd = sc.textFile(\\'python/test_support/sql/ages.csv\\')\\n        >>> df2 = spark.read.csv(rdd)\\n        >>> df2.dtypes\\n        [(\\'_c0\\', \\'string\\'), (\\'_c1\\', \\'string\\')]', 'output': 'def csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None,\\n            comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None,\\n            ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None,\\n            negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None,\\n            maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None,\\n            columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None,\\n            samplingRatio=None, enforceSchema=None, emptyValue=None, locale=None, lineSep=None)'}\n",
      "{'input': \"Loads ORC files, returning the result as a :class:`DataFrame`.\\n\\n        >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\\n        >>> df.dtypes\\n        [('a', 'bigint'), ('b', 'int'), ('c', 'int')]\", 'output': 'def orc(self, path)'}\n",
      "{'input': 'Construct a :class:`DataFrame` representing the database table named ``table``\\n        accessible via JDBC URL ``url`` and connection ``properties``.\\n\\n        Partitions of the table will be retrieved in parallel if either ``column`` or\\n        ``predicates`` is specified. ``lowerBound`, ``upperBound`` and ``numPartitions``\\n        is needed when ``column`` is specified.\\n\\n        If both ``column`` and ``predicates`` are specified, ``column`` will be used.\\n\\n        .. note:: Don\\'t create too many partitions in parallel on a large cluster;\\n            otherwise Spark might crash your external database systems.\\n\\n        :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\\n        :param table: the name of the table\\n        :param column: the name of an integer column that will be used for partitioning;\\n                       if this parameter is specified, then ``numPartitions``, ``lowerBound``\\n                       (inclusive), and ``upperBound`` (exclusive) will form partition strides\\n                       for generated WHERE clause expressions used to split the column\\n                       ``column`` evenly\\n        :param lowerBound: the minimum value of ``column`` used to decide partition stride\\n        :param upperBound: the maximum value of ``column`` used to decide partition stride\\n        :param numPartitions: the number of partitions\\n        :param predicates: a list of expressions suitable for inclusion in WHERE clauses;\\n                           each one defines one partition of the :class:`DataFrame`\\n        :param properties: a dictionary of JDBC database connection arguments. Normally at\\n                           least properties \"user\" and \"password\" with their corresponding values.\\n                           For example { \\'user\\' : \\'SYSTEM\\', \\'password\\' : \\'mypassword\\' }\\n        :return: a DataFrame', 'output': 'def jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None,\\n             predicates=None, properties=None)'}\n",
      "{'input': \"Specifies the behavior when data or table already exists.\\n\\n        Options include:\\n\\n        * `append`: Append contents of this :class:`DataFrame` to existing data.\\n        * `overwrite`: Overwrite existing data.\\n        * `error` or `errorifexists`: Throw an exception if data already exists.\\n        * `ignore`: Silently ignore this operation if data already exists.\\n\\n        >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\", 'output': 'def mode(self, saveMode)'}\n",
      "{'input': \"Specifies the underlying output data source.\\n\\n        :param source: string, name of the data source, e.g. 'json', 'parquet'.\\n\\n        >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))\", 'output': 'def format(self, source)'}\n",
      "{'input': \"Adds an output option for the underlying data source.\\n\\n        You can set the following option(s) for writing files:\\n            * ``timeZone``: sets the string that indicates a timezone to be used to format\\n                timestamps in the JSON/CSV datasources or partition values.\\n                If it isn't set, it uses the default value, session local timezone.\", 'output': 'def option(self, key, value)'}\n",
      "{'input': \"Adds output options for the underlying data source.\\n\\n        You can set the following option(s) for writing files:\\n            * ``timeZone``: sets the string that indicates a timezone to be used to format\\n                timestamps in the JSON/CSV datasources or partition values.\\n                If it isn't set, it uses the default value, session local timezone.\", 'output': 'def options(self, **options)'}\n",
      "{'input': \"Partitions the output by the given columns on the file system.\\n\\n        If specified, the output is laid out on the file system similar\\n        to Hive's partitioning scheme.\\n\\n        :param cols: name of columns\\n\\n        >>> df.write.partitionBy('year', 'month').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\", 'output': 'def partitionBy(self, *cols)'}\n",
      "{'input': 'Sorts the output in each bucket by the given columns on the file system.\\n\\n        :param col: a name of a column, or a list of names.\\n        :param cols: additional names (optional). If `col` is a list it should be empty.\\n\\n        >>> (df.write.format(\\'parquet\\')  # doctest: +SKIP\\n        ...     .bucketBy(100, \\'year\\', \\'month\\')\\n        ...     .sortBy(\\'day\\')\\n        ...     .mode(\"overwrite\")\\n        ...     .saveAsTable(\\'sorted_bucketed_table\\'))', 'output': 'def sortBy(self, col, *cols)'}\n",
      "{'input': \"Saves the contents of the :class:`DataFrame` to a data source.\\n\\n        The data source is specified by the ``format`` and a set of ``options``.\\n        If ``format`` is not specified, the default data source configured by\\n        ``spark.sql.sources.default`` will be used.\\n\\n        :param path: the path in a Hadoop supported file system\\n        :param format: the format used to save\\n        :param mode: specifies the behavior of the save operation when data already exists.\\n\\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\\n            * ``overwrite``: Overwrite existing data.\\n            * ``ignore``: Silently ignore this operation if data already exists.\\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\\\\n                exists.\\n        :param partitionBy: names of partitioning columns\\n        :param options: all other string options\\n\\n        >>> df.write.mode('append').parquet(os.path.join(tempfile.mkdtemp(), 'data'))\", 'output': 'def save(self, path=None, format=None, mode=None, partitionBy=None, **options)'}\n",
      "{'input': 'Inserts the content of the :class:`DataFrame` to the specified table.\\n\\n        It requires that the schema of the class:`DataFrame` is the same as the\\n        schema of the table.\\n\\n        Optionally overwriting any existing data.', 'output': 'def insertInto(self, tableName, overwrite=False)'}\n",
      "{'input': 'Saves the content of the :class:`DataFrame` as the specified table.\\n\\n        In the case the table already exists, behavior of this function depends on the\\n        save mode, specified by the `mode` function (default to throwing an exception).\\n        When `mode` is `Overwrite`, the schema of the :class:`DataFrame` does not need to be\\n        the same as that of the existing table.\\n\\n        * `append`: Append contents of this :class:`DataFrame` to existing data.\\n        * `overwrite`: Overwrite existing data.\\n        * `error` or `errorifexists`: Throw an exception if data already exists.\\n        * `ignore`: Silently ignore this operation if data already exists.\\n\\n        :param name: the table name\\n        :param format: the format used to save\\n        :param mode: one of `append`, `overwrite`, `error`, `errorifexists`, `ignore` \\\\\\n                     (default: error)\\n        :param partitionBy: names of partitioning columns\\n        :param options: all other string options', 'output': 'def saveAsTable(self, name, format=None, mode=None, partitionBy=None, **options)'}\n",
      "{'input': \"Saves the content of the :class:`DataFrame` in JSON format\\n        (`JSON Lines text format or newline-delimited JSON <http://jsonlines.org/>`_) at the\\n        specified path.\\n\\n        :param path: the path in any Hadoop supported file system\\n        :param mode: specifies the behavior of the save operation when data already exists.\\n\\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\\n            * ``overwrite``: Overwrite existing data.\\n            * ``ignore``: Silently ignore this operation if data already exists.\\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\\\\n                exists.\\n        :param compression: compression codec to use when saving to file. This can be one of the\\n                            known case-insensitive shorten names (none, bzip2, gzip, lz4,\\n                            snappy and deflate).\\n        :param dateFormat: sets the string that indicates a date format. Custom date formats\\n                           follow the formats at ``java.time.format.DateTimeFormatter``. This\\n                           applies to date type. If None is set, it uses the\\n                           default value, ``yyyy-MM-dd``.\\n        :param timestampFormat: sets the string that indicates a timestamp format.\\n                                Custom date formats follow the formats at\\n                                ``java.time.format.DateTimeFormatter``.\\n                                This applies to timestamp type. If None is set, it uses the\\n                                default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\\n        :param encoding: specifies encoding (charset) of saved json files. If None is set,\\n                        the default UTF-8 charset will be used.\\n        :param lineSep: defines the line separator that should be used for writing. If None is\\n                        set, it uses the default value, ``\\\\\\\\n``.\\n\\n        >>> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))\", 'output': 'def json(self, path, mode=None, compression=None, dateFormat=None, timestampFormat=None,\\n             lineSep=None, encoding=None)'}\n",
      "{'input': \"Saves the content of the :class:`DataFrame` in Parquet format at the specified path.\\n\\n        :param path: the path in any Hadoop supported file system\\n        :param mode: specifies the behavior of the save operation when data already exists.\\n\\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\\n            * ``overwrite``: Overwrite existing data.\\n            * ``ignore``: Silently ignore this operation if data already exists.\\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\\\\n                exists.\\n        :param partitionBy: names of partitioning columns\\n        :param compression: compression codec to use when saving to file. This can be one of the\\n                            known case-insensitive shorten names (none, uncompressed, snappy, gzip,\\n                            lzo, brotli, lz4, and zstd). This will override\\n                            ``spark.sql.parquet.compression.codec``. If None is set, it uses the\\n                            value specified in ``spark.sql.parquet.compression.codec``.\\n\\n        >>> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))\", 'output': 'def parquet(self, path, mode=None, partitionBy=None, compression=None)'}\n",
      "{'input': 'Saves the content of the DataFrame in a text file at the specified path.\\n        The text files will be encoded as UTF-8.\\n\\n        :param path: the path in any Hadoop supported file system\\n        :param compression: compression codec to use when saving to file. This can be one of the\\n                            known case-insensitive shorten names (none, bzip2, gzip, lz4,\\n                            snappy and deflate).\\n        :param lineSep: defines the line separator that should be used for writing. If None is\\n                        set, it uses the default value, ``\\\\\\\\n``.\\n\\n        The DataFrame must have only one column that is of string type.\\n        Each row becomes a new line in the output file.', 'output': 'def text(self, path, compression=None, lineSep=None)'}\n",
      "{'input': 'r\"\"\"Saves the content of the :class:`DataFrame` in CSV format at the specified path.\\n\\n        :param path: the path in any Hadoop supported file system\\n        :param mode: specifies the behavior of the save operation when data already exists.\\n\\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\\n            * ``overwrite``: Overwrite existing data.\\n            * ``ignore``: Silently ignore this operation if data already exists.\\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\\\\n                exists.\\n\\n        :param compression: compression codec to use when saving to file. This can be one of the\\n                            known case-insensitive shorten names (none, bzip2, gzip, lz4,\\n                            snappy and deflate).\\n        :param sep: sets a single character as a separator for each field and value. If None is\\n                    set, it uses the default value, ``,``.\\n        :param quote: sets a single character used for escaping quoted values where the\\n                      separator can be part of the value. If None is set, it uses the default\\n                      value, ``\"``. If an empty string is set, it uses ``u0000`` (null character).\\n        :param escape: sets a single character used for escaping quotes inside an already\\n                       quoted value. If None is set, it uses the default value, ``\\\\``\\n        :param escapeQuotes: a flag indicating whether values containing quotes should always\\n                             be enclosed in quotes. If None is set, it uses the default value\\n                             ``true``, escaping all values containing a quote character.\\n        :param quoteAll: a flag indicating whether all values should always be enclosed in\\n                          quotes. If None is set, it uses the default value ``false``,\\n                          only escaping values containing a quote character.\\n        :param header: writes the names of columns as the first line. If None is set, it uses\\n                       the default value, ``false``.\\n        :param nullValue: sets the string representation of a null value. If None is set, it uses\\n                          the default value, empty string.\\n        :param dateFormat: sets the string that indicates a date format. Custom date formats\\n                           follow the formats at ``java.time.format.DateTimeFormatter``. This\\n                           applies to date type. If None is set, it uses the\\n                           default value, ``yyyy-MM-dd``.\\n        :param timestampFormat: sets the string that indicates a timestamp format.\\n                                Custom date formats follow the formats at\\n                                ``java.time.format.DateTimeFormatter``.\\n                                This applies to timestamp type. If None is set, it uses the\\n                                default value, ``yyyy-MM-dd\\'T\\'HH:mm:ss.SSSXXX``.\\n        :param ignoreLeadingWhiteSpace: a flag indicating whether or not leading whitespaces from\\n                                        values being written should be skipped. If None is set, it\\n                                        uses the default value, ``true``.\\n        :param ignoreTrailingWhiteSpace: a flag indicating whether or not trailing whitespaces from\\n                                         values being written should be skipped. If None is set, it\\n                                         uses the default value, ``true``.\\n        :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\\n                                          the quote character. If None is set, the default value is\\n                                          escape character when escape and quote characters are\\n                                          different, ``\\\\0`` otherwise..\\n        :param encoding: sets the encoding (charset) of saved csv files. If None is set,\\n                         the default UTF-8 charset will be used.\\n        :param emptyValue: sets the string representation of an empty value. If None is set, it uses\\n                           the default value, ``\"\"``.\\n        :param lineSep: defines the line separator that should be used for writing. If None is\\n                        set, it uses the default value, ``\\\\\\\\n``. Maximum length is 1 character.\\n\\n        >>> df.write.csv(os.path.join(tempfile.mkdtemp(), \\'data\\'))', 'output': 'def csv(self, path, mode=None, compression=None, sep=None, quote=None, escape=None,\\n            header=None, nullValue=None, escapeQuotes=None, quoteAll=None, dateFormat=None,\\n            timestampFormat=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None,\\n            charToEscapeQuoteEscaping=None, encoding=None, emptyValue=None, lineSep=None)'}\n",
      "{'input': \"Saves the content of the :class:`DataFrame` in ORC format at the specified path.\\n\\n        :param path: the path in any Hadoop supported file system\\n        :param mode: specifies the behavior of the save operation when data already exists.\\n\\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\\n            * ``overwrite``: Overwrite existing data.\\n            * ``ignore``: Silently ignore this operation if data already exists.\\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\\\\n                exists.\\n        :param partitionBy: names of partitioning columns\\n        :param compression: compression codec to use when saving to file. This can be one of the\\n                            known case-insensitive shorten names (none, snappy, zlib, and lzo).\\n                            This will override ``orc.compress`` and\\n                            ``spark.sql.orc.compression.codec``. If None is set, it uses the value\\n                            specified in ``spark.sql.orc.compression.codec``.\\n\\n        >>> orc_df = spark.read.orc('python/test_support/sql/orc_partitioned')\\n        >>> orc_df.write.orc(os.path.join(tempfile.mkdtemp(), 'data'))\", 'output': 'def orc(self, path, mode=None, partitionBy=None, compression=None)'}\n",
      "{'input': 'Saves the content of the :class:`DataFrame` to an external database table via JDBC.\\n\\n        .. note:: Don\\'t create too many partitions in parallel on a large cluster;\\n            otherwise Spark might crash your external database systems.\\n\\n        :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\\n        :param table: Name of the table in the external database.\\n        :param mode: specifies the behavior of the save operation when data already exists.\\n\\n            * ``append``: Append contents of this :class:`DataFrame` to existing data.\\n            * ``overwrite``: Overwrite existing data.\\n            * ``ignore``: Silently ignore this operation if data already exists.\\n            * ``error`` or ``errorifexists`` (default case): Throw an exception if data already \\\\\\n                exists.\\n        :param properties: a dictionary of JDBC database connection arguments. Normally at\\n                           least properties \"user\" and \"password\" with their corresponding values.\\n                           For example { \\'user\\' : \\'SYSTEM\\', \\'password\\' : \\'mypassword\\' }', 'output': 'def jdbc(self, url, table, mode=None, properties=None)'}\n",
      "{'input': \"Create an input stream that pulls messages from a Kinesis stream. This uses the\\n        Kinesis Client Library (KCL) to pull messages from Kinesis.\\n\\n        .. note:: The given AWS credentials will get saved in DStream checkpoints if checkpointing\\n            is enabled. Make sure that your checkpoint directory is secure.\\n\\n        :param ssc:  StreamingContext object\\n        :param kinesisAppName:  Kinesis application name used by the Kinesis Client Library (KCL) to\\n                                update DynamoDB\\n        :param streamName:  Kinesis stream name\\n        :param endpointUrl:  Url of Kinesis service (e.g., https://kinesis.us-east-1.amazonaws.com)\\n        :param regionName:  Name of region used by the Kinesis Client Library (KCL) to update\\n                            DynamoDB (lease coordination and checkpointing) and CloudWatch (metrics)\\n        :param initialPositionInStream:  In the absence of Kinesis checkpoint info, this is the\\n                                         worker's initial starting position in the stream. The\\n                                         values are either the beginning of the stream per Kinesis'\\n                                         limit of 24 hours (InitialPositionInStream.TRIM_HORIZON) or\\n                                         the tip of the stream (InitialPositionInStream.LATEST).\\n        :param checkpointInterval:  Checkpoint interval for Kinesis checkpointing. See the Kinesis\\n                                    Spark Streaming documentation for more details on the different\\n                                    types of checkpoints.\\n        :param storageLevel:  Storage level to use for storing the received objects (default is\\n                              StorageLevel.MEMORY_AND_DISK_2)\\n        :param awsAccessKeyId:  AWS AccessKeyId (default is None. If None, will use\\n                                DefaultAWSCredentialsProviderChain)\\n        :param awsSecretKey:  AWS SecretKey (default is None. If None, will use\\n                              DefaultAWSCredentialsProviderChain)\\n        :param decoder:  A function used to decode value (default is utf8_decoder)\\n        :param stsAssumeRoleArn: ARN of IAM role to assume when using STS sessions to read from\\n                                 the Kinesis stream (default is None).\\n        :param stsSessionName: Name to uniquely identify STS sessions used to read from Kinesis\\n                               stream, if STS is being used (default is None).\\n        :param stsExternalId: External ID that can be used to validate against the assumed IAM\\n                              role's trust policy, if STS is being used (default is None).\\n        :return: A DStream object\", 'output': 'def createStream(ssc, kinesisAppName, streamName, endpointUrl, regionName,\\n                     initialPositionInStream, checkpointInterval,\\n                     storageLevel=StorageLevel.MEMORY_AND_DISK_2,\\n                     awsAccessKeyId=None, awsSecretKey=None, decoder=utf8_decoder,\\n                     stsAssumeRoleArn=None, stsSessionName=None, stsExternalId=None)'}\n",
      "{'input': 'Prompt the user to choose who to assign the issue to in jira, given a list of candidates,\\n    including the original reporter and all commentors', 'output': 'def choose_jira_assignee(issue, asf_jira)'}\n",
      "{'input': 'Standardize the [SPARK-XXXXX] [MODULE] prefix\\n    Converts \"[SPARK-XXX][mllib] Issue\", \"[MLLib] SPARK-XXX. Issue\" or \"SPARK XXX [MLLIB]: Issue\" to\\n    \"[SPARK-XXX][MLLIB] Issue\"\\n\\n    >>> standardize_jira_ref(\\n    ...     \"[SPARK-5821] [SQL] ParquetRelation2 CTAS should check if delete is successful\")\\n    \\'[SPARK-5821][SQL] ParquetRelation2 CTAS should check if delete is successful\\'\\n    >>> standardize_jira_ref(\\n    ...     \"[SPARK-4123][Project Infra][WIP]: Show new dependencies added in pull requests\")\\n    \\'[SPARK-4123][PROJECT INFRA][WIP] Show new dependencies added in pull requests\\'\\n    >>> standardize_jira_ref(\"[MLlib] Spark  5954: Top by key\")\\n    \\'[SPARK-5954][MLLIB] Top by key\\'\\n    >>> standardize_jira_ref(\"[SPARK-979] a LRU scheduler for load balancing in TaskSchedulerImpl\")\\n    \\'[SPARK-979] a LRU scheduler for load balancing in TaskSchedulerImpl\\'\\n    >>> standardize_jira_ref(\\n    ...     \"SPARK-1094 Support MiMa for reporting binary compatibility across versions.\")\\n    \\'[SPARK-1094] Support MiMa for reporting binary compatibility across versions.\\'\\n    >>> standardize_jira_ref(\"[WIP]  [SPARK-1146] Vagrant support for Spark\")\\n    \\'[SPARK-1146][WIP] Vagrant support for Spark\\'\\n    >>> standardize_jira_ref(\\n    ...     \"SPARK-1032. If Yarn app fails before registering, app master stays aroun...\")\\n    \\'[SPARK-1032] If Yarn app fails before registering, app master stays aroun...\\'\\n    >>> standardize_jira_ref(\\n    ...     \"[SPARK-6250][SPARK-6146][SPARK-5911][SQL] Types are now reserved words in DDL parser.\")\\n    \\'[SPARK-6250][SPARK-6146][SPARK-5911][SQL] Types are now reserved words in DDL parser.\\'\\n    >>> standardize_jira_ref(\"Additional information for users building from source code\")\\n    \\'Additional information for users building from source code\\'', 'output': 'def standardize_jira_ref(text)'}\n",
      "{'input': 'Parses a line in LIBSVM format into (label, indices, values).', 'output': 'def _parse_libsvm_line(line)'}\n",
      "{'input': 'Converts a LabeledPoint to a string in LIBSVM format.', 'output': 'def _convert_labeled_point_to_libsvm(p)'}\n",
      "{'input': 'Loads labeled data in the LIBSVM format into an RDD of\\n        LabeledPoint. The LIBSVM format is a text-based format used by\\n        LIBSVM and LIBLINEAR. Each line represents a labeled sparse\\n        feature vector using the following format:\\n\\n        label index1:value1 index2:value2 ...\\n\\n        where the indices are one-based and in ascending order. This\\n        method parses each line into a LabeledPoint, where the feature\\n        indices are converted to zero-based.\\n\\n        :param sc: Spark context\\n        :param path: file or directory path in any Hadoop-supported file\\n                     system URI\\n        :param numFeatures: number of features, which will be determined\\n                            from the input data if a nonpositive value\\n                            is given. This is useful when the dataset is\\n                            already split into multiple files and you\\n                            want to load them separately, because some\\n                            features may not present in certain files,\\n                            which leads to inconsistent feature\\n                            dimensions.\\n        :param minPartitions: min number of partitions\\n        @return: labeled data stored as an RDD of LabeledPoint\\n\\n        >>> from tempfile import NamedTemporaryFile\\n        >>> from pyspark.mllib.util import MLUtils\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> tempFile = NamedTemporaryFile(delete=True)\\n        >>> _ = tempFile.write(b\"+1 1:1.0 3:2.0 5:3.0\\\\\\\\n-1\\\\\\\\n-1 2:4.0 4:5.0 6:6.0\")\\n        >>> tempFile.flush()\\n        >>> examples = MLUtils.loadLibSVMFile(sc, tempFile.name).collect()\\n        >>> tempFile.close()\\n        >>> examples[0]\\n        LabeledPoint(1.0, (6,[0,2,4],[1.0,2.0,3.0]))\\n        >>> examples[1]\\n        LabeledPoint(-1.0, (6,[],[]))\\n        >>> examples[2]\\n        LabeledPoint(-1.0, (6,[1,3,5],[4.0,5.0,6.0]))', 'output': 'def loadLibSVMFile(sc, path, numFeatures=-1, minPartitions=None)'}\n",
      "{'input': 'Save labeled data in LIBSVM format.\\n\\n        :param data: an RDD of LabeledPoint to be saved\\n        :param dir: directory to save the data\\n\\n        >>> from tempfile import NamedTemporaryFile\\n        >>> from fileinput import input\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> from glob import glob\\n        >>> from pyspark.mllib.util import MLUtils\\n        >>> examples = [LabeledPoint(1.1, Vectors.sparse(3, [(0, 1.23), (2, 4.56)])),\\n        ...             LabeledPoint(0.0, Vectors.dense([1.01, 2.02, 3.03]))]\\n        >>> tempFile = NamedTemporaryFile(delete=True)\\n        >>> tempFile.close()\\n        >>> MLUtils.saveAsLibSVMFile(sc.parallelize(examples), tempFile.name)\\n        >>> \\'\\'.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\\n        \\'0.0 1:1.01 2:2.02 3:3.03\\\\\\\\n1.1 1:1.23 3:4.56\\\\\\\\n\\'', 'output': 'def saveAsLibSVMFile(data, dir)'}\n",
      "{'input': 'Load labeled points saved using RDD.saveAsTextFile.\\n\\n        :param sc: Spark context\\n        :param path: file or directory path in any Hadoop-supported file\\n                     system URI\\n        :param minPartitions: min number of partitions\\n        @return: labeled data stored as an RDD of LabeledPoint\\n\\n        >>> from tempfile import NamedTemporaryFile\\n        >>> from pyspark.mllib.util import MLUtils\\n        >>> from pyspark.mllib.regression import LabeledPoint\\n        >>> examples = [LabeledPoint(1.1, Vectors.sparse(3, [(0, -1.23), (2, 4.56e-7)])),\\n        ...             LabeledPoint(0.0, Vectors.dense([1.01, 2.02, 3.03]))]\\n        >>> tempFile = NamedTemporaryFile(delete=True)\\n        >>> tempFile.close()\\n        >>> sc.parallelize(examples, 1).saveAsTextFile(tempFile.name)\\n        >>> MLUtils.loadLabeledPoints(sc, tempFile.name).collect()\\n        [LabeledPoint(1.1, (3,[0,2],[-1.23,4.56e-07])), LabeledPoint(0.0, [1.01,2.02,3.03])]', 'output': 'def loadLabeledPoints(sc, path, minPartitions=None)'}\n",
      "{'input': 'Returns a new vector with `1.0` (bias) appended to\\n        the end of the input vector.', 'output': 'def appendBias(data)'}\n",
      "{'input': 'Converts vector columns in an input DataFrame from the\\n        :py:class:`pyspark.mllib.linalg.Vector` type to the new\\n        :py:class:`pyspark.ml.linalg.Vector` type under the `spark.ml`\\n        package.\\n\\n        :param dataset:\\n          input dataset\\n        :param cols:\\n          a list of vector columns to be converted.\\n          New vector columns will be ignored. If unspecified, all old\\n          vector columns will be converted excepted nested ones.\\n        :return:\\n          the input dataset with old vector columns converted to the\\n          new vector type\\n\\n        >>> import pyspark\\n        >>> from pyspark.mllib.linalg import Vectors\\n        >>> from pyspark.mllib.util import MLUtils\\n        >>> df = spark.createDataFrame(\\n        ...     [(0, Vectors.sparse(2, [1], [1.0]), Vectors.dense(2.0, 3.0))],\\n        ...     [\"id\", \"x\", \"y\"])\\n        >>> r1 = MLUtils.convertVectorColumnsToML(df).first()\\n        >>> isinstance(r1.x, pyspark.ml.linalg.SparseVector)\\n        True\\n        >>> isinstance(r1.y, pyspark.ml.linalg.DenseVector)\\n        True\\n        >>> r2 = MLUtils.convertVectorColumnsToML(df, \"x\").first()\\n        >>> isinstance(r2.x, pyspark.ml.linalg.SparseVector)\\n        True\\n        >>> isinstance(r2.y, pyspark.mllib.linalg.DenseVector)\\n        True', 'output': 'def convertVectorColumnsToML(dataset, *cols)'}\n",
      "{'input': \":param: intercept bias factor, the term c in X'w + c\\n        :param: weights   feature vector, the term w in X'w + c\\n        :param: xMean     Point around which the data X is centered.\\n        :param: xVariance Variance of the given data\\n        :param: nPoints   Number of points to be generated\\n        :param: seed      Random Seed\\n        :param: eps       Used to scale the noise. If eps is set high,\\n                          the amount of gaussian noise added is more.\\n\\n        Returns a list of LabeledPoints of length nPoints\", 'output': 'def generateLinearInput(intercept, weights, xMean, xVariance,\\n                            nPoints, seed, eps)'}\n",
      "{'input': 'Generate an RDD of LabeledPoints.', 'output': 'def generateLinearRDD(sc, nexamples, nfeatures, eps,\\n                          nParts=2, intercept=0.0)'}\n",
      "{'input': 'Train a linear regression model using Stochastic Gradient\\n        Descent (SGD). This solves the least squares regression\\n        formulation\\n\\n            f(weights) = 1/(2n) ||A weights - y||^2\\n\\n        which is the mean squared error. Here the data matrix has n rows,\\n        and the input RDD holds the set of rows of A, each with its\\n        corresponding right hand side label y.\\n        See also the documentation for the precise formulation.\\n\\n        :param data:\\n          The training data, an RDD of LabeledPoint.\\n        :param iterations:\\n          The number of iterations.\\n          (default: 100)\\n        :param step:\\n          The step parameter used in SGD.\\n          (default: 1.0)\\n        :param miniBatchFraction:\\n          Fraction of data to be used for each SGD iteration.\\n          (default: 1.0)\\n        :param initialWeights:\\n          The initial weights.\\n          (default: None)\\n        :param regParam:\\n          The regularizer parameter.\\n          (default: 0.0)\\n        :param regType:\\n          The type of regularizer used for training our model.\\n          Supported values:\\n\\n            - \"l1\" for using L1 regularization\\n            - \"l2\" for using L2 regularization\\n            - None for no regularization (default)\\n        :param intercept:\\n          Boolean parameter which indicates the use or not of the\\n          augmented representation for training data (i.e., whether bias\\n          features are activated or not).\\n          (default: False)\\n        :param validateData:\\n          Boolean parameter which indicates if the algorithm should\\n          validate data before training.\\n          (default: True)\\n        :param convergenceTol:\\n          A condition which decides iteration termination.\\n          (default: 0.001)', 'output': 'def train(cls, data, iterations=100, step=1.0, miniBatchFraction=1.0,\\n              initialWeights=None, regParam=0.0, regType=None, intercept=False,\\n              validateData=True, convergenceTol=0.001)'}\n",
      "{'input': 'Predict labels for provided features.\\n        Using a piecewise linear function.\\n        1) If x exactly matches a boundary then associated prediction\\n        is returned. In case there are multiple predictions with the\\n        same boundary then one of them is returned. Which one is\\n        undefined (same as java.util.Arrays.binarySearch).\\n        2) If x is lower or higher than all boundaries then first or\\n        last prediction is returned respectively. In case there are\\n        multiple predictions with the same boundary then the lowest\\n        or highest is returned respectively.\\n        3) If x falls between two values in boundary array then\\n        prediction is treated as piecewise linear function and\\n        interpolated value is returned. In case there are multiple\\n        values with the same boundary then the same rules as in 2)\\n        are used.\\n\\n        :param x:\\n          Feature or RDD of Features to be labeled.', 'output': 'def predict(self, x)'}\n",
      "{'input': 'Save an IsotonicRegressionModel.', 'output': 'def save(self, sc, path)'}\n",
      "{'input': 'Load an IsotonicRegressionModel.', 'output': 'def load(cls, sc, path)'}\n",
      "{'input': 'Train an isotonic regression model on the given data.\\n\\n        :param data:\\n          RDD of (label, feature, weight) tuples.\\n        :param isotonic:\\n          Whether this is isotonic (which is default) or antitonic.\\n          (default: True)', 'output': 'def train(cls, data, isotonic=True)'}\n",
      "{'input': 'Compute similarities between columns of this matrix.\\n\\n        The threshold parameter is a trade-off knob between estimate\\n        quality and computational cost.\\n\\n        The default threshold setting of 0 guarantees deterministically\\n        correct results, but uses the brute-force approach of computing\\n        normalized dot products.\\n\\n        Setting the threshold to positive values uses a sampling\\n        approach and incurs strictly less computational cost than the\\n        brute-force approach. However the similarities computed will\\n        be estimates.\\n\\n        The sampling guarantees relative-error correctness for those\\n        pairs of columns that have similarity greater than the given\\n        similarity threshold.\\n\\n        To describe the guarantee, we set some notation:\\n            * Let A be the smallest in magnitude non-zero element of\\n              this matrix.\\n            * Let B be the largest in magnitude non-zero element of\\n              this matrix.\\n            * Let L be the maximum number of non-zeros per row.\\n\\n        For example, for {0,1} matrices: A=B=1.\\n        Another example, for the Netflix matrix: A=1, B=5\\n\\n        For those column pairs that are above the threshold, the\\n        computed similarity is correct to within 20% relative error\\n        with probability at least 1 - (0.981)^10/B^\\n\\n        The shuffle size is bounded by the *smaller* of the following\\n        two expressions:\\n\\n            * O(n log(n) L / (threshold * A))\\n            * O(m L^2^)\\n\\n        The latter is the cost of the brute-force approach, so for\\n        non-zero thresholds, the cost is always cheaper than the\\n        brute-force approach.\\n\\n        :param: threshold: Set to 0 for deterministic guaranteed\\n                           correctness. Similarities above this\\n                           threshold are estimated with the cost vs\\n                           estimate quality trade-off described above.\\n        :return: An n x n sparse upper-triangular CoordinateMatrix of\\n                 cosine similarities between columns of this matrix.\\n\\n        >>> rows = sc.parallelize([[1, 2], [1, 5]])\\n        >>> mat = RowMatrix(rows)\\n\\n        >>> sims = mat.columnSimilarities()\\n        >>> sims.entries.first().value\\n        0.91914503...', 'output': 'def columnSimilarities(self, threshold=0.0)'}\n",
      "{'input': 'Compute the QR decomposition of this RowMatrix.\\n\\n        The implementation is designed to optimize the QR decomposition\\n        (factorization) for the RowMatrix of a tall and skinny shape.\\n\\n        Reference:\\n         Paul G. Constantine, David F. Gleich. \"Tall and skinny QR\\n         factorizations in MapReduce architectures\"\\n         ([[https://doi.org/10.1145/1996092.1996103]])\\n\\n        :param: computeQ: whether to computeQ\\n        :return: QRDecomposition(Q: RowMatrix, R: Matrix), where\\n                 Q = None if computeQ = false.\\n\\n        >>> rows = sc.parallelize([[3, -6], [4, -8], [0, 1]])\\n        >>> mat = RowMatrix(rows)\\n        >>> decomp = mat.tallSkinnyQR(True)\\n        >>> Q = decomp.Q\\n        >>> R = decomp.R\\n\\n        >>> # Test with absolute values\\n        >>> absQRows = Q.rows.map(lambda row: abs(row.toArray()).tolist())\\n        >>> absQRows.collect()\\n        [[0.6..., 0.0], [0.8..., 0.0], [0.0, 1.0]]\\n\\n        >>> # Test with absolute values\\n        >>> abs(R.toArray()).tolist()\\n        [[5.0, 10.0], [0.0, 1.0]]', 'output': 'def tallSkinnyQR(self, computeQ=False)'}\n",
      "{'input': \"Computes the singular value decomposition of the RowMatrix.\\n\\n        The given row matrix A of dimension (m X n) is decomposed into\\n        U * s * V'T where\\n\\n        * U: (m X k) (left singular vectors) is a RowMatrix whose\\n             columns are the eigenvectors of (A X A')\\n        * s: DenseVector consisting of square root of the eigenvalues\\n             (singular values) in descending order.\\n        * v: (n X k) (right singular vectors) is a Matrix whose columns\\n             are the eigenvectors of (A' X A)\\n\\n        For more specific details on implementation, please refer\\n        the Scala documentation.\\n\\n        :param k: Number of leading singular values to keep (`0 < k <= n`).\\n                  It might return less than k if there are numerically zero singular values\\n                  or there are not enough Ritz values converged before the maximum number of\\n                  Arnoldi update iterations is reached (in case that matrix A is ill-conditioned).\\n        :param computeU: Whether or not to compute U. If set to be\\n                         True, then U is computed by A * V * s^-1\\n        :param rCond: Reciprocal condition number. All singular values\\n                      smaller than rCond * s[0] are treated as zero\\n                      where s[0] is the largest singular value.\\n        :returns: :py:class:`SingularValueDecomposition`\\n\\n        >>> rows = sc.parallelize([[3, 1, 1], [-1, 3, 1]])\\n        >>> rm = RowMatrix(rows)\\n\\n        >>> svd_model = rm.computeSVD(2, True)\\n        >>> svd_model.U.rows.collect()\\n        [DenseVector([-0.7071, 0.7071]), DenseVector([-0.7071, -0.7071])]\\n        >>> svd_model.s\\n        DenseVector([3.4641, 3.1623])\\n        >>> svd_model.V\\n        DenseMatrix(3, 2, [-0.4082, -0.8165, -0.4082, 0.8944, -0.4472, 0.0], 0)\", 'output': 'def computeSVD(self, k, computeU=False, rCond=1e-9)'}\n",
      "{'input': 'Multiply this matrix by a local dense matrix on the right.\\n\\n        :param matrix: a local dense matrix whose number of rows must match the number of columns\\n                       of this matrix\\n        :returns: :py:class:`RowMatrix`\\n\\n        >>> rm = RowMatrix(sc.parallelize([[0, 1], [2, 3]]))\\n        >>> rm.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\\n        [DenseVector([2.0, 3.0]), DenseVector([6.0, 11.0])]', 'output': 'def multiply(self, matrix)'}\n",
      "{'input': 'Returns a distributed matrix whose columns are the left\\n        singular vectors of the SingularValueDecomposition if computeU was set to be True.', 'output': 'def U(self)'}\n",
      "{'input': 'Rows of the IndexedRowMatrix stored as an RDD of IndexedRows.\\n\\n        >>> mat = IndexedRowMatrix(sc.parallelize([IndexedRow(0, [1, 2, 3]),\\n        ...                                        IndexedRow(1, [4, 5, 6])]))\\n        >>> rows = mat.rows\\n        >>> rows.first()\\n        IndexedRow(0, [1.0,2.0,3.0])', 'output': 'def rows(self)'}\n",
      "{'input': 'Convert this matrix to a BlockMatrix.\\n\\n        :param rowsPerBlock: Number of rows that make up each block.\\n                             The blocks forming the final rows are not\\n                             required to have the given number of rows.\\n        :param colsPerBlock: Number of columns that make up each block.\\n                             The blocks forming the final columns are not\\n                             required to have the given number of columns.\\n\\n        >>> rows = sc.parallelize([IndexedRow(0, [1, 2, 3]),\\n        ...                        IndexedRow(6, [4, 5, 6])])\\n        >>> mat = IndexedRowMatrix(rows).toBlockMatrix()\\n\\n        >>> # This IndexedRowMatrix will have 7 effective rows, due to\\n        >>> # the highest row index being 6, and the ensuing\\n        >>> # BlockMatrix will have 7 rows as well.\\n        >>> print(mat.numRows())\\n        7\\n\\n        >>> print(mat.numCols())\\n        3', 'output': 'def toBlockMatrix(self, rowsPerBlock=1024, colsPerBlock=1024)'}\n",
      "{'input': 'Multiply this matrix by a local dense matrix on the right.\\n\\n        :param matrix: a local dense matrix whose number of rows must match the number of columns\\n                       of this matrix\\n        :returns: :py:class:`IndexedRowMatrix`\\n\\n        >>> mat = IndexedRowMatrix(sc.parallelize([(0, (0, 1)), (1, (2, 3))]))\\n        >>> mat.multiply(DenseMatrix(2, 2, [0, 2, 1, 3])).rows.collect()\\n        [IndexedRow(0, [2.0,3.0]), IndexedRow(1, [6.0,11.0])]', 'output': 'def multiply(self, matrix)'}\n",
      "{'input': 'Entries of the CoordinateMatrix stored as an RDD of\\n        MatrixEntries.\\n\\n        >>> mat = CoordinateMatrix(sc.parallelize([MatrixEntry(0, 0, 1.2),\\n        ...                                        MatrixEntry(6, 4, 2.1)]))\\n        >>> entries = mat.entries\\n        >>> entries.first()\\n        MatrixEntry(0, 0, 1.2)', 'output': 'def entries(self)'}\n",
      "{'input': 'The RDD of sub-matrix blocks\\n        ((blockRowIndex, blockColIndex), sub-matrix) that form this\\n        distributed matrix.\\n\\n        >>> mat = BlockMatrix(\\n        ...     sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\\n        ...                     ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))]), 3, 2)\\n        >>> blocks = mat.blocks\\n        >>> blocks.first()\\n        ((0, 0), DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 0))', 'output': 'def blocks(self)'}\n",
      "{'input': 'Persists the underlying RDD with the specified storage level.', 'output': 'def persist(self, storageLevel)'}\n",
      "{'input': 'Adds two block matrices together. The matrices must have the\\n        same size and matching `rowsPerBlock` and `colsPerBlock` values.\\n        If one of the sub matrix blocks that are being added is a\\n        SparseMatrix, the resulting sub matrix block will also be a\\n        SparseMatrix, even if it is being added to a DenseMatrix. If\\n        two dense sub matrix blocks are added, the output block will\\n        also be a DenseMatrix.\\n\\n        >>> dm1 = Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])\\n        >>> dm2 = Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12])\\n        >>> sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 1, 2], [7, 11, 12])\\n        >>> blocks1 = sc.parallelize([((0, 0), dm1), ((1, 0), dm2)])\\n        >>> blocks2 = sc.parallelize([((0, 0), dm1), ((1, 0), dm2)])\\n        >>> blocks3 = sc.parallelize([((0, 0), sm), ((1, 0), dm2)])\\n        >>> mat1 = BlockMatrix(blocks1, 3, 2)\\n        >>> mat2 = BlockMatrix(blocks2, 3, 2)\\n        >>> mat3 = BlockMatrix(blocks3, 3, 2)\\n\\n        >>> mat1.add(mat2).toLocalMatrix()\\n        DenseMatrix(6, 2, [2.0, 4.0, 6.0, 14.0, 16.0, 18.0, 8.0, 10.0, 12.0, 20.0, 22.0, 24.0], 0)\\n\\n        >>> mat1.add(mat3).toLocalMatrix()\\n        DenseMatrix(6, 2, [8.0, 2.0, 3.0, 14.0, 16.0, 18.0, 4.0, 16.0, 18.0, 20.0, 22.0, 24.0], 0)', 'output': 'def add(self, other)'}\n",
      "{'input': 'Transpose this BlockMatrix. Returns a new BlockMatrix\\n        instance sharing the same underlying data. Is a lazy operation.\\n\\n        >>> blocks = sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\\n        ...                          ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))])\\n        >>> mat = BlockMatrix(blocks, 3, 2)\\n\\n        >>> mat_transposed = mat.transpose()\\n        >>> mat_transposed.toLocalMatrix()\\n        DenseMatrix(2, 6, [1.0, 4.0, 2.0, 5.0, 3.0, 6.0, 7.0, 10.0, 8.0, 11.0, 9.0, 12.0], 0)', 'output': 'def transpose(self)'}\n",
      "{'input': \"Returns the size of the vector.\\n\\n    >>> _vector_size([1., 2., 3.])\\n    3\\n    >>> _vector_size((1., 2., 3.))\\n    3\\n    >>> _vector_size(array.array('d', [1., 2., 3.]))\\n    3\\n    >>> _vector_size(np.zeros(3))\\n    3\\n    >>> _vector_size(np.zeros((3, 1)))\\n    3\\n    >>> _vector_size(np.zeros((1, 3)))\\n    Traceback (most recent call last):\\n        ...\\n    ValueError: Cannot treat an ndarray of shape (1, 3) as a vector\", 'output': 'def _vector_size(v)'}\n",
      "{'input': \"Parse string representation back into the DenseVector.\\n\\n        >>> DenseVector.parse(' [ 0.0,1.0,2.0,  3.0]')\\n        DenseVector([0.0, 1.0, 2.0, 3.0])\", 'output': 'def parse(s)'}\n",
      "{'input': \"Compute the dot product of two Vectors. We support\\n        (Numpy array, list, SparseVector, or SciPy sparse)\\n        and a target NumPy array that is either 1- or 2-dimensional.\\n        Equivalent to calling numpy.dot of the two vectors.\\n\\n        >>> dense = DenseVector(array.array('d', [1., 2.]))\\n        >>> dense.dot(dense)\\n        5.0\\n        >>> dense.dot(SparseVector(2, [0, 1], [2., 1.]))\\n        4.0\\n        >>> dense.dot(range(1, 3))\\n        5.0\\n        >>> dense.dot(np.array(range(1, 3)))\\n        5.0\\n        >>> dense.dot([1.,])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> dense.dot(np.reshape([1., 2., 3., 4.], (2, 2), order='F'))\\n        array([  5.,  11.])\\n        >>> dense.dot(np.reshape([1., 2., 3.], (3, 1), order='F'))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\", 'output': 'def dot(self, other)'}\n",
      "{'input': \"Squared distance of two Vectors.\\n\\n        >>> dense1 = DenseVector(array.array('d', [1., 2.]))\\n        >>> dense1.squared_distance(dense1)\\n        0.0\\n        >>> dense2 = np.array([2., 1.])\\n        >>> dense1.squared_distance(dense2)\\n        2.0\\n        >>> dense3 = [2., 1.]\\n        >>> dense1.squared_distance(dense3)\\n        2.0\\n        >>> sparse1 = SparseVector(2, [0, 1], [2., 1.])\\n        >>> dense1.squared_distance(sparse1)\\n        2.0\\n        >>> dense1.squared_distance([1.,])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> dense1.squared_distance(SparseVector(1, [0,], [1.,]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\", 'output': 'def squared_distance(self, other)'}\n",
      "{'input': \"Parse string representation back into the SparseVector.\\n\\n        >>> SparseVector.parse(' (4, [0,1 ],[ 4.0,5.0] )')\\n        SparseVector(4, {0: 4.0, 1: 5.0})\", 'output': 'def parse(s)'}\n",
      "{'input': \"Dot product with a SparseVector or 1- or 2-dimensional Numpy array.\\n\\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\\n        >>> a.dot(a)\\n        25.0\\n        >>> a.dot(array.array('d', [1., 2., 3., 4.]))\\n        22.0\\n        >>> b = SparseVector(4, [2], [1.0])\\n        >>> a.dot(b)\\n        0.0\\n        >>> a.dot(np.array([[1, 1], [2, 2], [3, 3], [4, 4]]))\\n        array([ 22.,  22.])\\n        >>> a.dot([1., 2., 3.])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(np.array([1., 2.]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(DenseVector([1., 2.]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> a.dot(np.zeros((3, 2)))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\", 'output': 'def dot(self, other)'}\n",
      "{'input': \"Squared distance from a SparseVector or 1-dimensional NumPy array.\\n\\n        >>> a = SparseVector(4, [1, 3], [3.0, 4.0])\\n        >>> a.squared_distance(a)\\n        0.0\\n        >>> a.squared_distance(array.array('d', [1., 2., 3., 4.]))\\n        11.0\\n        >>> a.squared_distance(np.array([1., 2., 3., 4.]))\\n        11.0\\n        >>> b = SparseVector(4, [2], [1.0])\\n        >>> a.squared_distance(b)\\n        26.0\\n        >>> b.squared_distance(a)\\n        26.0\\n        >>> b.squared_distance([1., 2.])\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\\n        >>> b.squared_distance(SparseVector(3, [1,], [1.0,]))\\n        Traceback (most recent call last):\\n            ...\\n        AssertionError: dimension mismatch\", 'output': 'def squared_distance(self, other)'}\n",
      "{'input': 'Returns a copy of this SparseVector as a 1-dimensional NumPy array.', 'output': 'def toArray(self)'}\n",
      "{'input': 'Convert this vector to the new mllib-local representation.\\n        This does NOT copy the data; it copies references.\\n\\n        :return: :py:class:`pyspark.ml.linalg.SparseVector`\\n\\n        .. versionadded:: 2.0.0', 'output': 'def asML(self)'}\n",
      "{'input': 'Create a dense vector of 64-bit floats from a Python list or numbers.\\n\\n        >>> Vectors.dense([1, 2, 3])\\n        DenseVector([1.0, 2.0, 3.0])\\n        >>> Vectors.dense(1.0, 2.0)\\n        DenseVector([1.0, 2.0])', 'output': 'def dense(*elements)'}\n",
      "{'input': 'Convert a vector from the new mllib-local representation.\\n        This does NOT copy the data; it copies references.\\n\\n        :param vec: a :py:class:`pyspark.ml.linalg.Vector`\\n        :return: a :py:class:`pyspark.mllib.linalg.Vector`\\n\\n        .. versionadded:: 2.0.0', 'output': 'def fromML(vec)'}\n",
      "{'input': 'Squared distance between two vectors.\\n        a and b can be of type SparseVector, DenseVector, np.ndarray\\n        or array.array.\\n\\n        >>> a = Vectors.sparse(4, [(0, 1), (3, 4)])\\n        >>> b = Vectors.dense([2, 5, 4, 1])\\n        >>> a.squared_distance(b)\\n        51.0', 'output': 'def squared_distance(v1, v2)'}\n",
      "{'input': \"Parse a string representation back into the Vector.\\n\\n        >>> Vectors.parse('[2,1,2 ]')\\n        DenseVector([2.0, 1.0, 2.0])\\n        >>> Vectors.parse(' ( 100,  [0],  [2])')\\n        SparseVector(100, {0: 2.0})\", 'output': 'def parse(s)'}\n",
      "{'input': 'Check equality between sparse/dense vectors,\\n        v1_indices and v2_indices assume to be strictly increasing.', 'output': 'def _equals(v1_indices, v1_values, v2_indices, v2_values)'}\n",
      "{'input': 'Convert Matrix attributes which are array-like or buffer to array.', 'output': 'def _convert_to_array(array_like, dtype)'}\n",
      "{'input': 'Return an numpy.ndarray\\n\\n        >>> m = DenseMatrix(2, 2, range(4))\\n        >>> m.toArray()\\n        array([[ 0.,  2.],\\n               [ 1.,  3.]])', 'output': 'def toArray(self)'}\n",
      "{'input': 'Convert to SparseMatrix', 'output': 'def toSparse(self)'}\n",
      "{'input': 'Convert this matrix to the new mllib-local representation.\\n        This does NOT copy the data; it copies references.\\n\\n        :return: :py:class:`pyspark.ml.linalg.DenseMatrix`\\n\\n        .. versionadded:: 2.0.0', 'output': 'def asML(self)'}\n",
      "{'input': 'Return an numpy.ndarray', 'output': 'def toArray(self)'}\n",
      "{'input': 'Convert this matrix to the new mllib-local representation.\\n        This does NOT copy the data; it copies references.\\n\\n        :return: :py:class:`pyspark.ml.linalg.SparseMatrix`\\n\\n        .. versionadded:: 2.0.0', 'output': 'def asML(self)'}\n",
      "{'input': 'Create a SparseMatrix', 'output': 'def sparse(numRows, numCols, colPtrs, rowIndices, values)'}\n",
      "{'input': 'Convert a matrix from the new mllib-local representation.\\n        This does NOT copy the data; it copies references.\\n\\n        :param mat: a :py:class:`pyspark.ml.linalg.Matrix`\\n        :return: a :py:class:`pyspark.mllib.linalg.Matrix`\\n\\n        .. versionadded:: 2.0.0', 'output': 'def fromML(mat)'}\n",
      "{'input': 'Given a large dataset and an item, approximately find at most k items which have the\\n        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\\n        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\\n        caching of the transformed data when necessary.\\n\\n        .. note:: This method is experimental and will likely change behavior in the next release.\\n\\n        :param dataset: The dataset to search for nearest neighbors of the key.\\n        :param key: Feature vector representing the item to search for.\\n        :param numNearestNeighbors: The maximum number of nearest neighbors.\\n        :param distCol: Output column for storing the distance between each result row and the key.\\n                        Use \"distCol\" as default value if it\\'s not specified.\\n        :return: A dataset containing at most k items closest to the key. A column \"distCol\" is\\n                 added to show the distance between each row and the key.', 'output': 'def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\")'}\n",
      "{'input': 'Join two datasets to approximately find all pairs of rows whose distance are smaller than\\n        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\\n        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\\n        transformed data when necessary.\\n\\n        :param datasetA: One of the datasets to join.\\n        :param datasetB: Another dataset to join.\\n        :param threshold: The threshold for the distance of row pairs.\\n        :param distCol: Output column for storing the distance between each pair of rows. Use\\n                        \"distCol\" as default value if it\\'s not specified.\\n        :return: A joined dataset containing pairs of rows. The original rows are in columns\\n                 \"datasetA\" and \"datasetB\", and a column \"distCol\" is added to show the distance\\n                 between each pair.', 'output': 'def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\")'}\n",
      "{'input': 'Construct the model directly from an array of label strings,\\n        requires an active SparkContext.', 'output': 'def from_labels(cls, labels, inputCol, outputCol=None, handleInvalid=None)'}\n",
      "{'input': 'Construct the model directly from an array of array of label strings,\\n        requires an active SparkContext.', 'output': 'def from_arrays_of_labels(cls, arrayOfLabels, inputCols, outputCols=None,\\n                              handleInvalid=None)'}\n",
      "{'input': 'setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=false, \\\\\\n        locale=None)\\n        Sets params for this StopWordRemover.', 'output': 'def setParams(self, inputCol=None, outputCol=None, stopWords=None, caseSensitive=False,\\n                  locale=None)'}\n",
      "{'input': 'Loads the default stop words for the given language.\\n        Supported languages: danish, dutch, english, finnish, french, german, hungarian,\\n        italian, norwegian, portuguese, russian, spanish, swedish, turkish', 'output': 'def loadDefaultStopWords(language)'}\n",
      "{'input': 'Find \"num\" number of words closest in similarity to \"word\".\\n        word can be a string or vector representation.\\n        Returns a dataframe with two fields word and similarity (which\\n        gives the cosine similarity).', 'output': 'def findSynonyms(self, word, num)'}\n",
      "{'input': 'Find \"num\" number of words closest in similarity to \"word\".\\n        word can be a string or vector representation.\\n        Returns an array with two fields word and similarity (which\\n        gives the cosine similarity).', 'output': 'def findSynonymsArray(self, word, num)'}\n",
      "{'input': \"Hook an exception handler into Py4j, which could capture some SQL exceptions in Java.\\n\\n    When calling Java API, it will call `get_return_value` to parse the returned object.\\n    If any exception happened in JVM, the result will be Java exception object, it raise\\n    py4j.protocol.Py4JJavaError. We replace the original `get_return_value` with one that\\n    could capture the Java exception and throw a Python one (with the same error message).\\n\\n    It's idempotent, could be called multiple times.\", 'output': 'def install_exception_handler()'}\n",
      "{'input': 'Convert python list to java type array\\n    :param gateway: Py4j Gateway\\n    :param jtype: java type of element in array\\n    :param arr: python type list', 'output': 'def toJArray(gateway, jtype, arr)'}\n",
      "{'input': 'Raise ImportError if minimum version of Pandas is not installed', 'output': 'def require_minimum_pandas_version()'}\n",
      "{'input': 'Raise ImportError if minimum version of pyarrow is not installed', 'output': 'def require_minimum_pyarrow_version()'}\n",
      "{'input': 'launch jvm gateway\\n    :param conf: spark configuration passed to spark-submit\\n    :param popen_kwargs: Dictionary of kwargs to pass to Popen when spawning\\n        the py4j JVM. This is a developer feature intended for use in\\n        customizing how pyspark interacts with the py4j JVM (e.g., capturing\\n        stdout/stderr).\\n    :return:', 'output': 'def launch_gateway(conf=None, popen_kwargs=None)'}\n",
      "{'input': \"Performs the authentication protocol defined by the SocketAuthHelper class on the given\\n    file-like object 'conn'.\", 'output': 'def _do_server_auth(conn, auth_secret)'}\n",
      "{'input': 'Connect to local host, authenticate with it, and return a (sockfile,sock) for that connection.\\n    Handles IPV4 & IPV6, does some error handling.\\n    :param port\\n    :param auth_secret\\n    :return: a tuple with (sockfile, sock)', 'output': 'def local_connect_and_auth(port, auth_secret)'}\n",
      "{'input': 'Start callback server if not already started. The callback server is needed if the Java\\n    driver process needs to callback into the Python driver process to execute Python code.', 'output': 'def ensure_callback_server_started(gw)'}\n",
      "{'input': 'Find the SPARK_HOME.', 'output': 'def _find_spark_home()'}\n",
      "{'input': 'Calculates URL contributions to the rank of other URLs.', 'output': 'def computeContribs(urls, rank)'}\n",
      "{'input': 'Gets summary (e.g. cluster assignments, cluster sizes) of the model trained on the\\n        training set. An exception is thrown if no summary exists.', 'output': 'def summary(self)'}\n",
      "{'input': 'Gets summary (e.g. cluster assignments, cluster sizes) of the model trained on the\\n        training set. An exception is thrown if no summary exists.', 'output': 'def summary(self)'}\n",
      "{'input': 'Gets summary (e.g. cluster assignments, cluster sizes) of the model trained on the\\n        training set. An exception is thrown if no summary exists.', 'output': 'def summary(self)'}\n",
      "{'input': 'Returns the image schema.\\n\\n        :return: a :class:`StructType` with a single column of images\\n               named \"image\" (nullable) and having the same type returned by :meth:`columnSchema`.\\n\\n        .. versionadded:: 2.3.0', 'output': 'def imageSchema(self)'}\n",
      "{'input': 'Returns the OpenCV type mapping supported.\\n\\n        :return: a dictionary containing the OpenCV type mapping supported.\\n\\n        .. versionadded:: 2.3.0', 'output': 'def ocvTypes(self)'}\n",
      "{'input': 'Returns the schema for the image column.\\n\\n        :return: a :class:`StructType` for image column,\\n            ``struct<origin:string, height:int, width:int, nChannels:int, mode:int, data:binary>``.\\n\\n        .. versionadded:: 2.4.0', 'output': 'def columnSchema(self)'}\n",
      "{'input': 'Returns field names of image columns.\\n\\n        :return: a list of field names.\\n\\n        .. versionadded:: 2.3.0', 'output': 'def imageFields(self)'}\n",
      "{'input': 'Returns the name of undefined image type for the invalid image.\\n\\n        .. versionadded:: 2.3.0', 'output': 'def undefinedImageType(self)'}\n",
      "{'input': 'Converts an image to an array with metadata.\\n\\n        :param `Row` image: A row that contains the image to be converted. It should\\n            have the attributes specified in `ImageSchema.imageSchema`.\\n        :return: a `numpy.ndarray` that is an image.\\n\\n        .. versionadded:: 2.3.0', 'output': 'def toNDArray(self, image)'}\n",
      "{'input': 'Converts an array with metadata to a two-dimensional image.\\n\\n        :param `numpy.ndarray` array: The array to convert to image.\\n        :param str origin: Path to the image, optional.\\n        :return: a :class:`Row` that is a two dimensional image.\\n\\n        .. versionadded:: 2.3.0', 'output': 'def toImage(self, array, origin=\"\")'}\n",
      "{'input': 'Reads the directory of images from the local or remote source.\\n\\n        .. note:: If multiple jobs are run in parallel with different sampleRatio or recursive flag,\\n            there may be a race condition where one job overwrites the hadoop configs of another.\\n\\n        .. note:: If sample ratio is less than 1, sampling uses a PathFilter that is efficient but\\n            potentially non-deterministic.\\n\\n        .. note:: Deprecated in 2.4.0. Use `spark.read.format(\"image\").load(path)` instead and\\n            this `readImages` will be removed in 3.0.0.\\n\\n        :param str path: Path to the image directory.\\n        :param bool recursive: Recursive search flag.\\n        :param int numPartitions: Number of DataFrame partitions.\\n        :param bool dropImageFailures: Drop the files that are not valid images.\\n        :param float sampleRatio: Fraction of the images loaded.\\n        :param int seed: Random number seed.\\n        :return: a :class:`DataFrame` with a single column of \"images\",\\n               see ImageSchema for details.\\n\\n        >>> df = ImageSchema.readImages(\\'data/mllib/images/origin/kittens\\', recursive=True)\\n        >>> df.count()\\n        5\\n\\n        .. versionadded:: 2.3.0', 'output': 'def readImages(self, path, recursive=False, numPartitions=-1,\\n                   dropImageFailures=False, sampleRatio=1.0, seed=0)'}\n",
      "{'input': 'Construct this object from given Java classname and arguments', 'output': 'def _create_from_java_class(cls, java_class, *args)'}\n",
      "{'input': 'Create a Java array of given java_class type. Useful for\\n        calling a method with a Scala Array from Python with Py4J.\\n        If the param pylist is a 2D array, then a 2D java array will be returned.\\n        The returned 2D java array is a square, non-jagged 2D array that is big\\n        enough for all elements. The empty slots in the inner Java arrays will\\n        be filled with null to make the non-jagged 2D array.\\n\\n        :param pylist:\\n          Python list to convert to a Java Array.\\n        :param java_class:\\n          Java class to specify the type of Array. Should be in the\\n          form of sc._gateway.jvm.* (sc is a valid Spark Context).\\n        :return:\\n          Java Array of converted pylist.\\n\\n        Example primitive Java classes:\\n          - basestring -> sc._gateway.jvm.java.lang.String\\n          - int -> sc._gateway.jvm.java.lang.Integer\\n          - float -> sc._gateway.jvm.java.lang.Double\\n          - bool -> sc._gateway.jvm.java.lang.Boolean', 'output': 'def _new_java_array(pylist, java_class)'}\n",
      "{'input': '>>> _convert_epytext(\"L{A}\")\\n    :class:`A`', 'output': 'def _convert_epytext(line)'}\n",
      "{'input': 'Return string prefix-time(.suffix)\\n\\n    >>> rddToFileName(\"spark\", None, 12345678910)\\n    \\'spark-12345678910\\'\\n    >>> rddToFileName(\"spark\", \"tmp\", 12345678910)\\n    \\'spark-12345678910.tmp\\'', 'output': 'def rddToFileName(prefix, suffix, timestamp)'}\n",
      "{'input': 'Add a profiler for RDD `id`', 'output': 'def add_profiler(self, id, profiler)'}\n",
      "{'input': 'Dump the profile stats into directory `path`', 'output': 'def dump_profiles(self, path)'}\n",
      "{'input': 'Print the profile stats to stdout', 'output': 'def show_profiles(self)'}\n",
      "{'input': 'Print the profile stats to stdout, id is the RDD id', 'output': 'def show(self, id)'}\n",
      "{'input': 'Dump the profile into path, id is the RDD id', 'output': 'def dump(self, id, path)'}\n",
      "{'input': 'Runs and profiles the method to_profile passed in. A profile object is returned.', 'output': 'def profile(self, func)'}\n",
      "{'input': 'Get the existing SQLContext or create a new one with given SparkContext.\\n\\n        :param sc: SparkContext', 'output': 'def getOrCreate(cls, sc)'}\n",
      "{'input': 'Sets the given Spark SQL configuration property.', 'output': 'def setConf(self, key, value)'}\n",
      "{'input': 'Returns the value of Spark SQL configuration property for the given key.\\n\\n        If the key is not set and defaultValue is set, return\\n        defaultValue. If the key is not set and defaultValue is not set, return\\n        the system default value.\\n\\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\\n        u\\'200\\'\\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\\n        u\\'10\\'\\n        >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", u\"50\")\\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\\n        u\\'50\\'', 'output': 'def getConf(self, key, defaultValue=_NoValue)'}\n",
      "{'input': 'Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\\n        ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\\n        step value ``step``.\\n\\n        :param start: the start value\\n        :param end: the end value (exclusive)\\n        :param step: the incremental step (default: 1)\\n        :param numPartitions: the number of partitions of the DataFrame\\n        :return: :class:`DataFrame`\\n\\n        >>> sqlContext.range(1, 7, 2).collect()\\n        [Row(id=1), Row(id=3), Row(id=5)]\\n\\n        If only one argument is specified, it will be used as the end value.\\n\\n        >>> sqlContext.range(3).collect()\\n        [Row(id=0), Row(id=1), Row(id=2)]', 'output': 'def range(self, start, end=None, step=1, numPartitions=None)'}\n",
      "{'input': 'An alias for :func:`spark.udf.register`.\\n        See :meth:`pyspark.sql.UDFRegistration.register`.\\n\\n        .. note:: Deprecated in 2.3.0. Use :func:`spark.udf.register` instead.', 'output': 'def registerFunction(self, name, f, returnType=None)'}\n",
      "{'input': 'An alias for :func:`spark.udf.registerJavaFunction`.\\n        See :meth:`pyspark.sql.UDFRegistration.registerJavaFunction`.\\n\\n        .. note:: Deprecated in 2.3.0. Use :func:`spark.udf.registerJavaFunction` instead.', 'output': 'def registerJavaFunction(self, name, javaClassName, returnType=None)'}\n",
      "{'input': 'Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\\n\\n        When ``schema`` is a list of column names, the type of each column\\n        will be inferred from ``data``.\\n\\n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\\n        from ``data``, which should be an RDD of :class:`Row`,\\n        or :class:`namedtuple`, or :class:`dict`.\\n\\n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string it must match\\n        the real data, or an exception will be thrown at runtime. If the given schema is not\\n        :class:`pyspark.sql.types.StructType`, it will be wrapped into a\\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\\n        each record will also be wrapped into a tuple, which can be converted to row later.\\n\\n        If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\\n        rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\\n\\n        :param data: an RDD of any kind of SQL data representation(e.g. :class:`Row`,\\n            :class:`tuple`, ``int``, ``boolean``, etc.), or :class:`list`, or\\n            :class:`pandas.DataFrame`.\\n        :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\\n            column names, default is None.  The data type string format equals to\\n            :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\\n            omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\\n            ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`.\\n            We can also use ``int`` as a short name for :class:`pyspark.sql.types.IntegerType`.\\n        :param samplingRatio: the sample ratio of rows used for inferring\\n        :param verifySchema: verify data types of every row against schema.\\n        :return: :class:`DataFrame`\\n\\n        .. versionchanged:: 2.0\\n           The ``schema`` parameter can be a :class:`pyspark.sql.types.DataType` or a\\n           datatype string after 2.0.\\n           If it\\'s not a :class:`pyspark.sql.types.StructType`, it will be wrapped into a\\n           :class:`pyspark.sql.types.StructType` and each record will also be wrapped into a tuple.\\n\\n        .. versionchanged:: 2.1\\n           Added verifySchema.\\n\\n        >>> l = [(\\'Alice\\', 1)]\\n        >>> sqlContext.createDataFrame(l).collect()\\n        [Row(_1=u\\'Alice\\', _2=1)]\\n        >>> sqlContext.createDataFrame(l, [\\'name\\', \\'age\\']).collect()\\n        [Row(name=u\\'Alice\\', age=1)]\\n\\n        >>> d = [{\\'name\\': \\'Alice\\', \\'age\\': 1}]\\n        >>> sqlContext.createDataFrame(d).collect()\\n        [Row(age=1, name=u\\'Alice\\')]\\n\\n        >>> rdd = sc.parallelize(l)\\n        >>> sqlContext.createDataFrame(rdd).collect()\\n        [Row(_1=u\\'Alice\\', _2=1)]\\n        >>> df = sqlContext.createDataFrame(rdd, [\\'name\\', \\'age\\'])\\n        >>> df.collect()\\n        [Row(name=u\\'Alice\\', age=1)]\\n\\n        >>> from pyspark.sql import Row\\n        >>> Person = Row(\\'name\\', \\'age\\')\\n        >>> person = rdd.map(lambda r: Person(*r))\\n        >>> df2 = sqlContext.createDataFrame(person)\\n        >>> df2.collect()\\n        [Row(name=u\\'Alice\\', age=1)]\\n\\n        >>> from pyspark.sql.types import *\\n        >>> schema = StructType([\\n        ...    StructField(\"name\", StringType(), True),\\n        ...    StructField(\"age\", IntegerType(), True)])\\n        >>> df3 = sqlContext.createDataFrame(rdd, schema)\\n        >>> df3.collect()\\n        [Row(name=u\\'Alice\\', age=1)]\\n\\n        >>> sqlContext.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\\n        [Row(name=u\\'Alice\\', age=1)]\\n        >>> sqlContext.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\\n        [Row(0=1, 1=2)]\\n\\n        >>> sqlContext.createDataFrame(rdd, \"a: string, b: int\").collect()\\n        [Row(a=u\\'Alice\\', b=1)]\\n        >>> rdd = rdd.map(lambda row: row[1])\\n        >>> sqlContext.createDataFrame(rdd, \"int\").collect()\\n        [Row(value=1)]\\n        >>> sqlContext.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\\n        Traceback (most recent call last):\\n            ...\\n        Py4JJavaError: ...', 'output': 'def createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)'}\n",
      "{'input': 'Creates an external table based on the dataset in a data source.\\n\\n        It returns the DataFrame associated with the external table.\\n\\n        The data source is specified by the ``source`` and a set of ``options``.\\n        If ``source`` is not specified, the default data source configured by\\n        ``spark.sql.sources.default`` will be used.\\n\\n        Optionally, a schema can be provided as the schema of the returned :class:`DataFrame` and\\n        created external table.\\n\\n        :return: :class:`DataFrame`', 'output': 'def createExternalTable(self, tableName, path=None, source=None, schema=None, **options)'}\n",
      "{'input': 'Returns a :class:`DataFrame` containing names of tables in the given database.\\n\\n        If ``dbName`` is not specified, the current database will be used.\\n\\n        The returned DataFrame has two columns: ``tableName`` and ``isTemporary``\\n        (a column with :class:`BooleanType` indicating if a table is a temporary one or not).\\n\\n        :param dbName: string, name of the database to use.\\n        :return: :class:`DataFrame`\\n\\n        >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\\n        >>> df2 = sqlContext.tables()\\n        >>> df2.filter(\"tableName = \\'table1\\'\").first()\\n        Row(database=u\\'\\', tableName=u\\'table1\\', isTemporary=True)', 'output': 'def tables(self, dbName=None)'}\n",
      "{'input': 'Returns a list of names of tables in the database ``dbName``.\\n\\n        :param dbName: string, name of the database to use. Default to the current database.\\n        :return: list of table names, in string\\n\\n        >>> sqlContext.registerDataFrameAsTable(df, \"table1\")\\n        >>> \"table1\" in sqlContext.tableNames()\\n        True\\n        >>> \"table1\" in sqlContext.tableNames(\"default\")\\n        True', 'output': 'def tableNames(self, dbName=None)'}\n",
      "{'input': 'Returns a :class:`StreamingQueryManager` that allows managing all the\\n        :class:`StreamingQuery` StreamingQueries active on `this` context.\\n\\n        .. note:: Evolving.', 'output': 'def streams(self)'}\n",
      "{'input': 'Converts a binary column of avro format into its corresponding catalyst value. The specified\\n    schema must match the read data, otherwise the behavior is undefined: it may fail or return\\n    arbitrary result.\\n\\n    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\\n    application as per the deployment section of \"Apache Avro Data Source Guide\".\\n\\n    :param data: the binary column.\\n    :param jsonFormatSchema: the avro schema in JSON string format.\\n    :param options: options to control how the Avro record is parsed.\\n\\n    >>> from pyspark.sql import Row\\n    >>> from pyspark.sql.avro.functions import from_avro, to_avro\\n    >>> data = [(1, Row(name=\\'Alice\\', age=2))]\\n    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\\n    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\\n    >>> avroDf.collect()\\n    [Row(avro=bytearray(b\\'\\\\\\\\x00\\\\\\\\x00\\\\\\\\x04\\\\\\\\x00\\\\\\\\nAlice\\'))]\\n    >>> jsonFormatSchema = \\'\\'\\'{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\\n    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\\n    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\\n    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}\\'\\'\\'\\n    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\\n    [Row(value=Row(avro=Row(age=2, name=u\\'Alice\\')))]', 'output': 'def from_avro(data, jsonFormatSchema, options={})'}\n",
      "{'input': 'Get the absolute path of a file added through C{SparkContext.addFile()}.', 'output': 'def get(cls, filename)'}\n",
      "{'input': 'Get the root directory that contains files added through\\n        C{SparkContext.addFile()}.', 'output': 'def getRootDirectory(cls)'}\n",
      "{'input': 'Gets summary (e.g. accuracy/precision/recall, objective history, total iterations) of model\\n        trained on the training set. An exception is thrown if `trainingSummary is None`.', 'output': 'def summary(self)'}\n",
      "{'input': 'Evaluates the model on a test dataset.\\n\\n        :param dataset:\\n          Test dataset to evaluate model on, where dataset is an\\n          instance of :py:class:`pyspark.sql.DataFrame`', 'output': 'def evaluate(self, dataset)'}\n",
      "{'input': 'Creates a copy of this instance with a randomly generated uid\\n        and some extra params. This creates a deep copy of the embedded paramMap,\\n        and copies the embedded and extra parameters over.\\n\\n        :param extra: Extra parameters to copy to the new instance\\n        :return: Copy of this instance', 'output': 'def copy(self, extra=None)'}\n",
      "{'input': 'Given a Java OneVsRestModel, create and return a Python wrapper of it.\\n        Used for ML persistence.', 'output': 'def _from_java(cls, java_stage)'}\n",
      "{'input': 'Transfer this instance to a Java OneVsRestModel. Used for ML persistence.\\n\\n        :return: Java object equivalent to this instance.', 'output': 'def _to_java(self)'}\n",
      "{'input': 'Return the message from an exception as either a str or unicode object.  Supports both\\n    Python 2 and Python 3.\\n\\n    >>> msg = \"Exception message\"\\n    >>> excp = Exception(msg)\\n    >>> msg == _exception_message(excp)\\n    True\\n\\n    >>> msg = u\"unicöde\"\\n    >>> excp = Exception(msg)\\n    >>> msg == _exception_message(excp)\\n    True', 'output': 'def _exception_message(excp)'}\n",
      "{'input': 'Get argspec of a function. Supports both Python 2 and Python 3.', 'output': 'def _get_argspec(f)'}\n",
      "{'input': \"Wraps the input function to fail on 'StopIteration' by raising a 'RuntimeError'\\n    prevents silent loss of data when 'f' is used in a for loop in Spark code\", 'output': 'def fail_on_stopiteration(f)'}\n",
      "{'input': 'Given a Spark version string, return the (major version number, minor version number).\\n        E.g., for 2.0.1-SNAPSHOT, return (2, 0).\\n\\n        >>> sparkVersion = \"2.4.0\"\\n        >>> VersionUtils.majorMinorVersion(sparkVersion)\\n        (2, 4)\\n        >>> sparkVersion = \"2.3.0-SNAPSHOT\"\\n        >>> VersionUtils.majorMinorVersion(sparkVersion)\\n        (2, 3)', 'output': 'def majorMinorVersion(sparkVersion)'}\n",
      "{'input': 'Checks whether a SparkContext is initialized or not.\\n        Throws error if a SparkContext is already running.', 'output': 'def _ensure_initialized(cls, instance=None, gateway=None, conf=None)'}\n",
      "{'input': 'Get or instantiate a SparkContext and register it as a singleton object.\\n\\n        :param conf: SparkConf (optional)', 'output': 'def getOrCreate(cls, conf=None)'}\n",
      "{'input': 'Set a Java system property, such as spark.executor.memory. This must\\n        must be invoked before instantiating SparkContext.', 'output': 'def setSystemProperty(cls, key, value)'}\n",
      "{'input': 'Shut down the SparkContext.', 'output': 'def stop(self)'}\n",
      "{'input': \"Create a new RDD of int containing elements from `start` to `end`\\n        (exclusive), increased by `step` every element. Can be called the same\\n        way as python's built-in range() function. If called with a single argument,\\n        the argument is interpreted as `end`, and `start` is set to 0.\\n\\n        :param start: the start value\\n        :param end: the end value (exclusive)\\n        :param step: the incremental step (default: 1)\\n        :param numSlices: the number of partitions of the new RDD\\n        :return: An RDD of int\\n\\n        >>> sc.range(5).collect()\\n        [0, 1, 2, 3, 4]\\n        >>> sc.range(2, 4).collect()\\n        [2, 3]\\n        >>> sc.range(1, 7, 2).collect()\\n        [1, 3, 5]\", 'output': 'def range(self, start, end=None, step=1, numSlices=None)'}\n",
      "{'input': 'Distribute a local Python collection to form an RDD. Using xrange\\n        is recommended if the input represents a range for performance.\\n\\n        >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\\n        [[0], [2], [3], [4], [6]]\\n        >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\\n        [[], [0], [], [2], [4]]', 'output': 'def parallelize(self, c, numSlices=None)'}\n",
      "{'input': 'Using py4j to send a large dataset to the jvm is really slow, so we use either a file\\n        or a socket if we have encryption enabled.\\n        :param data:\\n        :param serializer:\\n        :param reader_func:  A function which takes a filename and reads in the data in the jvm and\\n                returns a JavaRDD. Only used when encryption is disabled.\\n        :param createRDDServer:  A function which creates a PythonRDDServer in the jvm to\\n               accept the serialized data, for use when encryption is enabled.\\n        :return:', 'output': 'def _serialize_to_jvm(self, data, serializer, reader_func, createRDDServer)'}\n",
      "{'input': 'Load an RDD previously saved using L{RDD.saveAsPickleFile} method.\\n\\n        >>> tmpFile = NamedTemporaryFile(delete=True)\\n        >>> tmpFile.close()\\n        >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\\n        >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]', 'output': 'def pickleFile(self, name, minPartitions=None)'}\n",
      "{'input': 'Read a text file from HDFS, a local file system (available on all\\n        nodes), or any Hadoop-supported file system URI, and return it as an\\n        RDD of Strings.\\n        The text files must be encoded as UTF-8.\\n\\n        If use_unicode is False, the strings will be kept as `str` (encoding\\n        as `utf-8`), which is faster and smaller than unicode. (Added in\\n        Spark 1.2)\\n\\n        >>> path = os.path.join(tempdir, \"sample-text.txt\")\\n        >>> with open(path, \"w\") as testFile:\\n        ...    _ = testFile.write(\"Hello world!\")\\n        >>> textFile = sc.textFile(path)\\n        >>> textFile.collect()\\n        [u\\'Hello world!\\']', 'output': 'def textFile(self, name, minPartitions=None, use_unicode=True)'}\n",
      "{'input': 'Read a directory of text files from HDFS, a local file system\\n        (available on all nodes), or any  Hadoop-supported file system\\n        URI. Each file is read as a single record and returned in a\\n        key-value pair, where the key is the path of each file, the\\n        value is the content of each file.\\n        The text files must be encoded as UTF-8.\\n\\n        If use_unicode is False, the strings will be kept as `str` (encoding\\n        as `utf-8`), which is faster and smaller than unicode. (Added in\\n        Spark 1.2)\\n\\n        For example, if you have the following files::\\n\\n          hdfs://a-hdfs-path/part-00000\\n          hdfs://a-hdfs-path/part-00001\\n          ...\\n          hdfs://a-hdfs-path/part-nnnnn\\n\\n        Do C{rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")},\\n        then C{rdd} contains::\\n\\n          (a-hdfs-path/part-00000, its content)\\n          (a-hdfs-path/part-00001, its content)\\n          ...\\n          (a-hdfs-path/part-nnnnn, its content)\\n\\n        .. note:: Small files are preferred, as each file will be loaded\\n            fully in memory.\\n\\n        >>> dirPath = os.path.join(tempdir, \"files\")\\n        >>> os.mkdir(dirPath)\\n        >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\\n        ...    _ = file1.write(\"1\")\\n        >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\\n        ...    _ = file2.write(\"2\")\\n        >>> textFiles = sc.wholeTextFiles(dirPath)\\n        >>> sorted(textFiles.collect())\\n        [(u\\'.../1.txt\\', u\\'1\\'), (u\\'.../2.txt\\', u\\'2\\')]', 'output': 'def wholeTextFiles(self, path, minPartitions=None, use_unicode=True)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Read a directory of binary files from HDFS, a local file system\\n        (available on all nodes), or any Hadoop-supported file system URI\\n        as a byte array. Each file is read as a single record and returned\\n        in a key-value pair, where the key is the path of each file, the\\n        value is the content of each file.\\n\\n        .. note:: Small files are preferred, large file is also allowable, but\\n            may cause bad performance.', 'output': 'def binaryFiles(self, path, minPartitions=None)'}\n",
      "{'input': '.. note:: Experimental\\n\\n        Load data from a flat binary file, assuming each record is a set of numbers\\n        with the specified numerical format (see ByteBuffer), and the number of\\n        bytes per record is constant.\\n\\n        :param path: Directory to the input data files\\n        :param recordLength: The length at which to split the records', 'output': 'def binaryRecords(self, path, recordLength)'}\n",
      "{'input': 'Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\\n        a local file system (available on all nodes), or any Hadoop-supported file system URI.\\n        The mechanism is as follows:\\n\\n            1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\\n               and value Writable classes\\n            2. Serialization is attempted via Pyrolite pickling\\n            3. If this fails, the fallback is to call \\'toString\\' on each key and value\\n            4. C{PickleSerializer} is used to deserialize pickled objects on the Python side\\n\\n        :param path: path to sequncefile\\n        :param keyClass: fully qualified classname of key Writable class\\n               (e.g. \"org.apache.hadoop.io.Text\")\\n        :param valueClass: fully qualified classname of value Writable class\\n               (e.g. \"org.apache.hadoop.io.LongWritable\")\\n        :param keyConverter:\\n        :param valueConverter:\\n        :param minSplits: minimum splits in dataset\\n               (default min(2, sc.defaultParallelism))\\n        :param batchSize: The number of Python objects represented as a single\\n               Java object. (default 0, choose batchSize automatically)', 'output': 'def sequenceFile(self, path, keyClass=None, valueClass=None, keyConverter=None,\\n                     valueConverter=None, minSplits=None, batchSize=0)'}\n",
      "{'input': 'Read a \\'new API\\' Hadoop InputFormat with arbitrary key and value class from HDFS,\\n        a local file system (available on all nodes), or any Hadoop-supported file system URI.\\n        The mechanism is the same as for sc.sequenceFile.\\n\\n        A Hadoop configuration can be passed in as a Python dict. This will be converted into a\\n        Configuration in Java\\n\\n        :param path: path to Hadoop file\\n        :param inputFormatClass: fully qualified classname of Hadoop InputFormat\\n               (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\\n        :param keyClass: fully qualified classname of key Writable class\\n               (e.g. \"org.apache.hadoop.io.Text\")\\n        :param valueClass: fully qualified classname of value Writable class\\n               (e.g. \"org.apache.hadoop.io.LongWritable\")\\n        :param keyConverter: (None by default)\\n        :param valueConverter: (None by default)\\n        :param conf: Hadoop configuration, passed in as a dict\\n               (None by default)\\n        :param batchSize: The number of Python objects represented as a single\\n               Java object. (default 0, choose batchSize automatically)', 'output': 'def newAPIHadoopFile(self, path, inputFormatClass, keyClass, valueClass, keyConverter=None,\\n                         valueConverter=None, conf=None, batchSize=0)'}\n",
      "{'input': 'Build the union of a list of RDDs.\\n\\n        This supports unions() of RDDs with different serialized formats,\\n        although this forces them to be reserialized using the default\\n        serializer:\\n\\n        >>> path = os.path.join(tempdir, \"union-text.txt\")\\n        >>> with open(path, \"w\") as testFile:\\n        ...    _ = testFile.write(\"Hello\")\\n        >>> textFile = sc.textFile(path)\\n        >>> textFile.collect()\\n        [u\\'Hello\\']\\n        >>> parallelized = sc.parallelize([\"World!\"])\\n        >>> sorted(sc.union([textFile, parallelized]).collect())\\n        [u\\'Hello\\', \\'World!\\']', 'output': 'def union(self, rdds)'}\n",
      "{'input': 'Create an L{Accumulator} with the given initial value, using a given\\n        L{AccumulatorParam} helper object to define how to add values of the\\n        data type if provided. Default AccumulatorParams are used for integers\\n        and floating-point numbers if you do not provide one. For other types,\\n        a custom AccumulatorParam can be used.', 'output': 'def accumulator(self, value, accum_param=None)'}\n",
      "{'input': 'Add a file to be downloaded with this Spark job on every node.\\n        The C{path} passed can be either a local file, a file in HDFS\\n        (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\\n        FTP URI.\\n\\n        To access the file in Spark jobs, use\\n        L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the\\n        filename to find its download location.\\n\\n        A directory can be given if the recursive option is set to True.\\n        Currently directories are only supported for Hadoop-supported filesystems.\\n\\n        .. note:: A path can be added only once. Subsequent additions of the same path are ignored.\\n\\n        >>> from pyspark import SparkFiles\\n        >>> path = os.path.join(tempdir, \"test.txt\")\\n        >>> with open(path, \"w\") as testFile:\\n        ...    _ = testFile.write(\"100\")\\n        >>> sc.addFile(path)\\n        >>> def func(iterator):\\n        ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\\n        ...        fileVal = int(testFile.readline())\\n        ...        return [x * fileVal for x in iterator]\\n        >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\\n        [100, 200, 300, 400]', 'output': 'def addFile(self, path, recursive=False)'}\n",
      "{'input': 'Add a .py or .zip dependency for all tasks to be executed on this\\n        SparkContext in the future.  The C{path} passed can be either a local\\n        file, a file in HDFS (or other Hadoop-supported filesystems), or an\\n        HTTP, HTTPS or FTP URI.\\n\\n        .. note:: A path can be added only once. Subsequent additions of the same path are ignored.', 'output': 'def addPyFile(self, path)'}\n",
      "{'input': 'Returns a Java StorageLevel based on a pyspark.StorageLevel.', 'output': 'def _getJavaStorageLevel(self, storageLevel)'}\n",
      "{'input': 'Assigns a group ID to all the jobs started by this thread until the group ID is set to a\\n        different value or cleared.\\n\\n        Often, a unit of execution in an application consists of multiple Spark actions or jobs.\\n        Application programmers can use this method to group all those jobs together and give a\\n        group description. Once set, the Spark web UI will associate such jobs with this group.\\n\\n        The application can use L{SparkContext.cancelJobGroup} to cancel all\\n        running jobs in this group.\\n\\n        >>> import threading\\n        >>> from time import sleep\\n        >>> result = \"Not Set\"\\n        >>> lock = threading.Lock()\\n        >>> def map_func(x):\\n        ...     sleep(100)\\n        ...     raise Exception(\"Task should have been cancelled\")\\n        >>> def start_job(x):\\n        ...     global result\\n        ...     try:\\n        ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\\n        ...         result = sc.parallelize(range(x)).map(map_func).collect()\\n        ...     except Exception as e:\\n        ...         result = \"Cancelled\"\\n        ...     lock.release()\\n        >>> def stop_job():\\n        ...     sleep(5)\\n        ...     sc.cancelJobGroup(\"job_to_cancel\")\\n        >>> suppress = lock.acquire()\\n        >>> suppress = threading.Thread(target=start_job, args=(10,)).start()\\n        >>> suppress = threading.Thread(target=stop_job).start()\\n        >>> suppress = lock.acquire()\\n        >>> print(result)\\n        Cancelled\\n\\n        If interruptOnCancel is set to true for the job group, then job cancellation will result\\n        in Thread.interrupt() being called on the job\\'s executor threads. This is useful to help\\n        ensure that the tasks are actually stopped in a timely manner, but is off by default due\\n        to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.', 'output': 'def setJobGroup(self, groupId, description, interruptOnCancel=False)'}\n",
      "{'input': \"Executes the given partitionFunc on the specified set of partitions,\\n        returning the result as an array of elements.\\n\\n        If 'partitions' is not specified, this will run over all partitions.\\n\\n        >>> myRDD = sc.parallelize(range(6), 3)\\n        >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\\n        [0, 1, 4, 9, 16, 25]\\n\\n        >>> myRDD = sc.parallelize(range(6), 3)\\n        >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\\n        [0, 1, 16, 25]\", 'output': 'def runJob(self, rdd, partitionFunc, partitions=None, allowLocal=False)'}\n",
      "{'input': 'Dump the profile stats into directory `path`', 'output': 'def dump_profiles(self, path)'}\n",
      "{'input': 'Train a matrix factorization model given an RDD of ratings by users\\n        for a subset of products. The ratings matrix is approximated as the\\n        product of two lower-rank matrices of a given rank (number of\\n        features). To solve for these features, ALS is run iteratively with\\n        a configurable level of parallelism.\\n\\n        :param ratings:\\n          RDD of `Rating` or (userID, productID, rating) tuple.\\n        :param rank:\\n          Number of features to use (also referred to as the number of latent factors).\\n        :param iterations:\\n          Number of iterations of ALS.\\n          (default: 5)\\n        :param lambda_:\\n          Regularization parameter.\\n          (default: 0.01)\\n        :param blocks:\\n          Number of blocks used to parallelize the computation. A value\\n          of -1 will use an auto-configured number of blocks.\\n          (default: -1)\\n        :param nonnegative:\\n          A value of True will solve least-squares with nonnegativity\\n          constraints.\\n          (default: False)\\n        :param seed:\\n          Random seed for initial matrix factorization model. A value\\n          of None will use system time as the seed.\\n          (default: None)', 'output': 'def train(cls, ratings, rank, iterations=5, lambda_=0.01, blocks=-1, nonnegative=False,\\n              seed=None)'}\n",
      "{'input': 'Computes an FP-Growth model that contains frequent itemsets.\\n\\n        :param data:\\n          The input data set, each element contains a transaction.\\n        :param minSupport:\\n          The minimal support level.\\n          (default: 0.3)\\n        :param numPartitions:\\n          The number of partitions used by parallel FP-growth. A value\\n          of -1 will use the same number as input data.\\n          (default: -1)', 'output': 'def train(cls, data, minSupport=0.3, numPartitions=-1)'}\n",
      "{'input': 'Finds the complete set of frequent sequential patterns in the\\n        input sequences of itemsets.\\n\\n        :param data:\\n          The input data set, each element contains a sequence of\\n          itemsets.\\n        :param minSupport:\\n          The minimal support level of the sequential pattern, any\\n          pattern that appears more than (minSupport *\\n          size-of-the-dataset) times will be output.\\n          (default: 0.1)\\n        :param maxPatternLength:\\n          The maximal length of the sequential pattern, any pattern\\n          that appears less than maxPatternLength will be output.\\n          (default: 10)\\n        :param maxLocalProjDBSize:\\n          The maximum number of items (including delimiters used in the\\n          internal storage format) allowed in a projected database before\\n          local processing. If a projected database exceeds this size,\\n          another iteration of distributed prefix growth is run.\\n          (default: 32000000)', 'output': 'def train(cls, data, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000)'}\n",
      "{'input': 'Set sample points from the population. Should be a RDD', 'output': 'def setSample(self, sample)'}\n",
      "{'input': 'Estimate the probability density at points', 'output': 'def estimate(self, points)'}\n",
      "{'input': 'Start a TCP server to receive accumulator updates in a daemon thread, and returns it', 'output': 'def _start_update_server(auth_token)'}\n",
      "{'input': \"Adds a term to this accumulator's value\", 'output': 'def add(self, term)'}\n",
      "{'input': 'Compute aggregates and returns the result as a :class:`DataFrame`.\\n\\n        The available aggregate functions can be:\\n\\n        1. built-in aggregation functions, such as `avg`, `max`, `min`, `sum`, `count`\\n\\n        2. group aggregate pandas UDFs, created with :func:`pyspark.sql.functions.pandas_udf`\\n\\n           .. note:: There is no partial aggregation with group aggregate UDFs, i.e.,\\n               a full shuffle is required. Also, all the data of a group will be loaded into\\n               memory, so the user should be aware of the potential OOM risk if data is skewed\\n               and certain groups are too large to fit in memory.\\n\\n           .. seealso:: :func:`pyspark.sql.functions.pandas_udf`\\n\\n        If ``exprs`` is a single :class:`dict` mapping from string to string, then the key\\n        is the column to perform aggregation on, and the value is the aggregate function.\\n\\n        Alternatively, ``exprs`` can also be a list of aggregate :class:`Column` expressions.\\n\\n        .. note:: Built-in aggregation functions and group aggregate pandas UDFs cannot be mixed\\n            in a single call to this function.\\n\\n        :param exprs: a dict mapping from column name (string) to aggregate functions (string),\\n            or a list of :class:`Column`.\\n\\n        >>> gdf = df.groupBy(df.name)\\n        >>> sorted(gdf.agg({\"*\": \"count\"}).collect())\\n        [Row(name=u\\'Alice\\', count(1)=1), Row(name=u\\'Bob\\', count(1)=1)]\\n\\n        >>> from pyspark.sql import functions as F\\n        >>> sorted(gdf.agg(F.min(df.age)).collect())\\n        [Row(name=u\\'Alice\\', min(age)=2), Row(name=u\\'Bob\\', min(age)=5)]\\n\\n        >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n        >>> @pandas_udf(\\'int\\', PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\\n        ... def min_udf(v):\\n        ...     return v.min()\\n        >>> sorted(gdf.agg(min_udf(df.age)).collect())  # doctest: +SKIP\\n        [Row(name=u\\'Alice\\', min_udf(age)=2), Row(name=u\\'Bob\\', min_udf(age)=5)]', 'output': 'def agg(self, *exprs)'}\n",
      "{'input': 'Pivots a column of the current :class:`DataFrame` and perform the specified aggregation.\\n        There are two versions of pivot function: one that requires the caller to specify the list\\n        of distinct values to pivot on, and one that does not. The latter is more concise but less\\n        efficient, because Spark needs to first compute the list of distinct values internally.\\n\\n        :param pivot_col: Name of the column to pivot.\\n        :param values: List of values that will be translated to columns in the output DataFrame.\\n\\n        # Compute the sum of earnings for each year by course with each course as a separate column\\n\\n        >>> df4.groupBy(\"year\").pivot(\"course\", [\"dotNET\", \"Java\"]).sum(\"earnings\").collect()\\n        [Row(year=2012, dotNET=15000, Java=20000), Row(year=2013, dotNET=48000, Java=30000)]\\n\\n        # Or without specifying column values (less efficient)\\n\\n        >>> df4.groupBy(\"year\").pivot(\"course\").sum(\"earnings\").collect()\\n        [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]\\n        >>> df5.groupBy(\"sales.year\").pivot(\"sales.course\").sum(\"sales.earnings\").collect()\\n        [Row(year=2012, Java=20000, dotNET=15000), Row(year=2013, Java=30000, dotNET=48000)]', 'output': 'def pivot(self, pivot_col, values=None)'}\n",
      "{'input': 'Maps each group of the current :class:`DataFrame` using a pandas udf and returns the result\\n        as a `DataFrame`.\\n\\n        The user-defined function should take a `pandas.DataFrame` and return another\\n        `pandas.DataFrame`. For each group, all columns are passed together as a `pandas.DataFrame`\\n        to the user-function and the returned `pandas.DataFrame` are combined as a\\n        :class:`DataFrame`.\\n\\n        The returned `pandas.DataFrame` can be of arbitrary length and its schema must match the\\n        returnType of the pandas udf.\\n\\n        .. note:: This function requires a full shuffle. all the data of a group will be loaded\\n            into memory, so the user should be aware of the potential OOM risk if data is skewed\\n            and certain groups are too large to fit in memory.\\n\\n        .. note:: Experimental\\n\\n        :param udf: a grouped map user-defined function returned by\\n            :func:`pyspark.sql.functions.pandas_udf`.\\n\\n        >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\\n        >>> df = spark.createDataFrame(\\n        ...     [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\\n        ...     (\"id\", \"v\"))\\n        >>> @pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\\n        ... def normalize(pdf):\\n        ...     v = pdf.v\\n        ...     return pdf.assign(v=(v - v.mean()) / v.std())\\n        >>> df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP\\n        +---+-------------------+\\n        | id|                  v|\\n        +---+-------------------+\\n        |  1|-0.7071067811865475|\\n        |  1| 0.7071067811865475|\\n        |  2|-0.8320502943378437|\\n        |  2|-0.2773500981126146|\\n        |  2| 1.1094003924504583|\\n        +---+-------------------+\\n\\n        .. seealso:: :meth:`pyspark.sql.functions.pandas_udf`', 'output': 'def apply(self, udf)'}\n",
      "{'input': 'Creates a :class:`WindowSpec` with the partitioning defined.', 'output': 'def partitionBy(*cols)'}\n",
      "{'input': 'Creates a :class:`WindowSpec` with the frame boundaries defined,\\n        from `start` (inclusive) to `end` (inclusive).\\n\\n        Both `start` and `end` are relative positions from the current row.\\n        For example, \"0\" means \"current row\", while \"-1\" means the row before\\n        the current row, and \"5\" means the fifth row after the current row.\\n\\n        We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\\n        and ``Window.currentRow`` to specify special boundary values, rather than using integral\\n        values directly.\\n\\n        A row based boundary is based on the position of the row within the partition.\\n        An offset indicates the number of rows above or below the current row, the frame for the\\n        current row starts or ends. For instance, given a row based sliding frame with a lower bound\\n        offset of -1 and a upper bound offset of +2. The frame for row with index 5 would range from\\n        index 4 to index 6.\\n\\n        >>> from pyspark.sql import Window\\n        >>> from pyspark.sql import functions as func\\n        >>> from pyspark.sql import SQLContext\\n        >>> sc = SparkContext.getOrCreate()\\n        >>> sqlContext = SQLContext(sc)\\n        >>> tup = [(1, \"a\"), (1, \"a\"), (2, \"a\"), (1, \"b\"), (2, \"b\"), (3, \"b\")]\\n        >>> df = sqlContext.createDataFrame(tup, [\"id\", \"category\"])\\n        >>> window = Window.partitionBy(\"category\").orderBy(\"id\").rowsBetween(Window.currentRow, 1)\\n        >>> df.withColumn(\"sum\", func.sum(\"id\").over(window)).show()\\n        +---+--------+---+\\n        | id|category|sum|\\n        +---+--------+---+\\n        |  1|       b|  3|\\n        |  2|       b|  5|\\n        |  3|       b|  3|\\n        |  1|       a|  2|\\n        |  1|       a|  3|\\n        |  2|       a|  2|\\n        +---+--------+---+\\n\\n        :param start: boundary start, inclusive.\\n                      The frame is unbounded if this is ``Window.unboundedPreceding``, or\\n                      any value less than or equal to -9223372036854775808.\\n        :param end: boundary end, inclusive.\\n                    The frame is unbounded if this is ``Window.unboundedFollowing``, or\\n                    any value greater than or equal to 9223372036854775807.', 'output': 'def rowsBetween(start, end)'}\n",
      "{'input': 'Defines the frame boundaries, from `start` (inclusive) to `end` (inclusive).\\n\\n        Both `start` and `end` are relative positions from the current row.\\n        For example, \"0\" means \"current row\", while \"-1\" means the row before\\n        the current row, and \"5\" means the fifth row after the current row.\\n\\n        We recommend users use ``Window.unboundedPreceding``, ``Window.unboundedFollowing``,\\n        and ``Window.currentRow`` to specify special boundary values, rather than using integral\\n        values directly.\\n\\n        :param start: boundary start, inclusive.\\n                      The frame is unbounded if this is ``Window.unboundedPreceding``, or\\n                      any value less than or equal to max(-sys.maxsize, -9223372036854775808).\\n        :param end: boundary end, inclusive.\\n                    The frame is unbounded if this is ``Window.unboundedFollowing``, or\\n                    any value greater than or equal to min(sys.maxsize, 9223372036854775807).', 'output': 'def rowsBetween(self, start, end)'}\n",
      "{'input': 'Generates an RDD comprised of i.i.d. samples from the\\n        uniform distribution U(0.0, 1.0).\\n\\n        To transform the distribution in the generated RDD from U(0.0, 1.0)\\n        to U(a, b), use\\n        C{RandomRDDs.uniformRDD(sc, n, p, seed)\\\\\\n          .map(lambda v: a + (b - a) * v)}\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param size: Size of the RDD.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of float comprised of i.i.d. samples ~ `U(0.0, 1.0)`.\\n\\n        >>> x = RandomRDDs.uniformRDD(sc, 100).collect()\\n        >>> len(x)\\n        100\\n        >>> max(x) <= 1.0 and min(x) >= 0.0\\n        True\\n        >>> RandomRDDs.uniformRDD(sc, 100, 4).getNumPartitions()\\n        4\\n        >>> parts = RandomRDDs.uniformRDD(sc, 100, seed=4).getNumPartitions()\\n        >>> parts == sc.defaultParallelism\\n        True', 'output': 'def uniformRDD(sc, size, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of i.i.d. samples from the standard normal\\n        distribution.\\n\\n        To transform the distribution in the generated RDD from standard normal\\n        to some other normal N(mean, sigma^2), use\\n        C{RandomRDDs.normal(sc, n, p, seed)\\\\\\n          .map(lambda v: mean + sigma * v)}\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param size: Size of the RDD.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of float comprised of i.i.d. samples ~ N(0.0, 1.0).\\n\\n        >>> x = RandomRDDs.normalRDD(sc, 1000, seed=1)\\n        >>> stats = x.stats()\\n        >>> stats.count()\\n        1000\\n        >>> abs(stats.mean() - 0.0) < 0.1\\n        True\\n        >>> abs(stats.stdev() - 1.0) < 0.1\\n        True', 'output': 'def normalRDD(sc, size, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of i.i.d. samples from the log normal\\n        distribution with the input mean and standard distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param mean: mean for the log Normal distribution\\n        :param std: std for the log Normal distribution\\n        :param size: Size of the RDD.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of float comprised of i.i.d. samples ~ log N(mean, std).\\n\\n        >>> from math import sqrt, exp\\n        >>> mean = 0.0\\n        >>> std = 1.0\\n        >>> expMean = exp(mean + 0.5 * std * std)\\n        >>> expStd = sqrt((exp(std * std) - 1.0) * exp(2.0 * mean + std * std))\\n        >>> x = RandomRDDs.logNormalRDD(sc, mean, std, 1000, seed=2)\\n        >>> stats = x.stats()\\n        >>> stats.count()\\n        1000\\n        >>> abs(stats.mean() - expMean) < 0.5\\n        True\\n        >>> from math import sqrt\\n        >>> abs(stats.stdev() - expStd) < 0.5\\n        True', 'output': 'def logNormalRDD(sc, mean, std, size, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of i.i.d. samples from the Exponential\\n        distribution with the input mean.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param mean: Mean, or 1 / lambda, for the Exponential distribution.\\n        :param size: Size of the RDD.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of float comprised of i.i.d. samples ~ Exp(mean).\\n\\n        >>> mean = 2.0\\n        >>> x = RandomRDDs.exponentialRDD(sc, mean, 1000, seed=2)\\n        >>> stats = x.stats()\\n        >>> stats.count()\\n        1000\\n        >>> abs(stats.mean() - mean) < 0.5\\n        True\\n        >>> from math import sqrt\\n        >>> abs(stats.stdev() - sqrt(mean)) < 0.5\\n        True', 'output': 'def exponentialRDD(sc, mean, size, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of i.i.d. samples from the Gamma\\n        distribution with the input shape and scale.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param shape: shape (> 0) parameter for the Gamma distribution\\n        :param scale: scale (> 0) parameter for the Gamma distribution\\n        :param size: Size of the RDD.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of float comprised of i.i.d. samples ~ Gamma(shape, scale).\\n\\n        >>> from math import sqrt\\n        >>> shape = 1.0\\n        >>> scale = 2.0\\n        >>> expMean = shape * scale\\n        >>> expStd = sqrt(shape * scale * scale)\\n        >>> x = RandomRDDs.gammaRDD(sc, shape, scale, 1000, seed=2)\\n        >>> stats = x.stats()\\n        >>> stats.count()\\n        1000\\n        >>> abs(stats.mean() - expMean) < 0.5\\n        True\\n        >>> abs(stats.stdev() - expStd) < 0.5\\n        True', 'output': 'def gammaRDD(sc, shape, scale, size, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the uniform distribution U(0.0, 1.0).\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD.\\n        :param seed: Seed for the RNG that generates the seed for the generator in each partition.\\n        :return: RDD of Vector with vectors containing i.i.d samples ~ `U(0.0, 1.0)`.\\n\\n        >>> import numpy as np\\n        >>> mat = np.matrix(RandomRDDs.uniformVectorRDD(sc, 10, 10).collect())\\n        >>> mat.shape\\n        (10, 10)\\n        >>> mat.max() <= 1.0 and mat.min() >= 0.0\\n        True\\n        >>> RandomRDDs.uniformVectorRDD(sc, 10, 10, 4).getNumPartitions()\\n        4', 'output': 'def uniformVectorRDD(sc, numRows, numCols, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the standard normal distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ `N(0.0, 1.0)`.\\n\\n        >>> import numpy as np\\n        >>> mat = np.matrix(RandomRDDs.normalVectorRDD(sc, 100, 100, seed=1).collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - 0.0) < 0.1\\n        True\\n        >>> abs(mat.std() - 1.0) < 0.1\\n        True', 'output': 'def normalVectorRDD(sc, numRows, numCols, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the log normal distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param mean: Mean of the log normal distribution\\n        :param std: Standard Deviation of the log normal distribution\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ log `N(mean, std)`.\\n\\n        >>> import numpy as np\\n        >>> from math import sqrt, exp\\n        >>> mean = 0.0\\n        >>> std = 1.0\\n        >>> expMean = exp(mean + 0.5 * std * std)\\n        >>> expStd = sqrt((exp(std * std) - 1.0) * exp(2.0 * mean + std * std))\\n        >>> m = RandomRDDs.logNormalVectorRDD(sc, mean, std, 100, 100, seed=1).collect()\\n        >>> mat = np.matrix(m)\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - expMean) < 0.1\\n        True\\n        >>> abs(mat.std() - expStd) < 0.1\\n        True', 'output': 'def logNormalVectorRDD(sc, mean, std, numRows, numCols, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the Poisson distribution with the input mean.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param mean: Mean, or lambda, for the Poisson distribution.\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`)\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ Pois(mean).\\n\\n        >>> import numpy as np\\n        >>> mean = 100.0\\n        >>> rdd = RandomRDDs.poissonVectorRDD(sc, mean, 100, 100, seed=1)\\n        >>> mat = np.mat(rdd.collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - mean) < 0.5\\n        True\\n        >>> from math import sqrt\\n        >>> abs(mat.std() - sqrt(mean)) < 0.5\\n        True', 'output': 'def poissonVectorRDD(sc, mean, numRows, numCols, numPartitions=None, seed=None)'}\n",
      "{'input': 'Generates an RDD comprised of vectors containing i.i.d. samples drawn\\n        from the Gamma distribution.\\n\\n        :param sc: SparkContext used to create the RDD.\\n        :param shape: Shape (> 0) of the Gamma distribution\\n        :param scale: Scale (> 0) of the Gamma distribution\\n        :param numRows: Number of Vectors in the RDD.\\n        :param numCols: Number of elements in each Vector.\\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\\n        :param seed: Random seed (default: a random long integer).\\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ Gamma(shape, scale).\\n\\n        >>> import numpy as np\\n        >>> from math import sqrt\\n        >>> shape = 1.0\\n        >>> scale = 2.0\\n        >>> expMean = shape * scale\\n        >>> expStd = sqrt(shape * scale * scale)\\n        >>> mat = np.matrix(RandomRDDs.gammaVectorRDD(sc, shape, scale, 100, 100, seed=1).collect())\\n        >>> mat.shape\\n        (100, 100)\\n        >>> abs(mat.mean() - expMean) < 0.1\\n        True\\n        >>> abs(mat.std() - expStd) < 0.1\\n        True', 'output': 'def gammaVectorRDD(sc, shape, scale, numRows, numCols, numPartitions=None, seed=None)'}\n",
      "{'input': 'Returns the active SparkSession for the current thread, returned by the builder.\\n        >>> s = SparkSession.getActiveSession()\\n        >>> l = [(\\'Alice\\', 1)]\\n        >>> rdd = s.sparkContext.parallelize(l)\\n        >>> df = s.createDataFrame(rdd, [\\'name\\', \\'age\\'])\\n        >>> df.select(\"age\").collect()\\n        [Row(age=1)]', 'output': 'def getActiveSession(cls)'}\n",
      "{'input': 'Runtime configuration interface for Spark.\\n\\n        This is the interface through which the user can get and set all Spark and Hadoop\\n        configurations that are relevant to Spark SQL. When getting the value of a config,\\n        this defaults to the value set in the underlying :class:`SparkContext`, if any.', 'output': 'def conf(self)'}\n",
      "{'input': 'Interface through which the user may create, drop, alter or query underlying\\n        databases, tables, functions etc.\\n\\n        :return: :class:`Catalog`', 'output': 'def catalog(self)'}\n",
      "{'input': 'Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\\n        ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\\n        step value ``step``.\\n\\n        :param start: the start value\\n        :param end: the end value (exclusive)\\n        :param step: the incremental step (default: 1)\\n        :param numPartitions: the number of partitions of the DataFrame\\n        :return: :class:`DataFrame`\\n\\n        >>> spark.range(1, 7, 2).collect()\\n        [Row(id=1), Row(id=3), Row(id=5)]\\n\\n        If only one argument is specified, it will be used as the end value.\\n\\n        >>> spark.range(3).collect()\\n        [Row(id=0), Row(id=1), Row(id=2)]', 'output': 'def range(self, start, end=None, step=1, numPartitions=None)'}\n",
      "{'input': 'Infer schema from list of Row or tuple.\\n\\n        :param data: list of Row or tuple\\n        :param names: list of column names\\n        :return: :class:`pyspark.sql.types.StructType`', 'output': 'def _inferSchemaFromList(self, data, names=None)'}\n",
      "{'input': 'Infer schema from an RDD of Row or tuple.\\n\\n        :param rdd: an RDD of Row or tuple\\n        :param samplingRatio: sampling ratio, or no sampling (default)\\n        :return: :class:`pyspark.sql.types.StructType`', 'output': 'def _inferSchema(self, rdd, samplingRatio=None, names=None)'}\n",
      "{'input': 'Create an RDD for DataFrame from an existing RDD, returns the RDD and schema.', 'output': 'def _createFromRDD(self, rdd, schema, samplingRatio)'}\n",
      "{'input': 'Create an RDD for DataFrame from a list or pandas.DataFrame, returns\\n        the RDD and schema.', 'output': 'def _createFromLocal(self, data, schema)'}\n",
      "{'input': 'Used when converting a pandas.DataFrame to Spark using to_records(), this will correct\\n        the dtypes of fields in a record so they can be properly loaded into Spark.\\n        :param rec: a numpy record to check field dtypes\\n        :return corrected dtype for a numpy.record or None if no correction needed', 'output': 'def _get_numpy_record_dtype(self, rec)'}\n",
      "{'input': 'Convert a pandas.DataFrame to list of records that can be used to make a DataFrame\\n         :return list of records', 'output': 'def _convert_from_pandas(self, pdf, schema, timezone)'}\n",
      "{'input': 'Create a DataFrame from a given pandas.DataFrame by slicing it into partitions, converting\\n        to Arrow data, then sending to the JVM to parallelize. If a schema is passed in, the\\n        data types will be used to coerce the data in Pandas to Arrow conversion.', 'output': 'def _create_from_pandas_with_arrow(self, pdf, schema, timezone)'}\n",
      "{'input': 'Initialize a SparkSession for a pyspark shell session. This is called from shell.py\\n        to make error handling simpler without needing to declare local variables in that\\n        script, which would expose those to users.', 'output': 'def _create_shell_session()'}\n",
      "{'input': 'Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\\n\\n        When ``schema`` is a list of column names, the type of each column\\n        will be inferred from ``data``.\\n\\n        When ``schema`` is ``None``, it will try to infer the schema (column names and types)\\n        from ``data``, which should be an RDD of :class:`Row`,\\n        or :class:`namedtuple`, or :class:`dict`.\\n\\n        When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\\n        the real data, or an exception will be thrown at runtime. If the given schema is not\\n        :class:`pyspark.sql.types.StructType`, it will be wrapped into a\\n        :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\\n        each record will also be wrapped into a tuple, which can be converted to row later.\\n\\n        If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\\n        rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\\n\\n        :param data: an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean,\\n            etc.), or :class:`list`, or :class:`pandas.DataFrame`.\\n        :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\\n            column names, default is ``None``.  The data type string format equals to\\n            :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\\n            omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\\n            ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`. We can also use\\n            ``int`` as a short name for ``IntegerType``.\\n        :param samplingRatio: the sample ratio of rows used for inferring\\n        :param verifySchema: verify data types of every row against schema.\\n        :return: :class:`DataFrame`\\n\\n        .. versionchanged:: 2.1\\n           Added verifySchema.\\n\\n        .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\\n\\n        >>> l = [(\\'Alice\\', 1)]\\n        >>> spark.createDataFrame(l).collect()\\n        [Row(_1=u\\'Alice\\', _2=1)]\\n        >>> spark.createDataFrame(l, [\\'name\\', \\'age\\']).collect()\\n        [Row(name=u\\'Alice\\', age=1)]\\n\\n        >>> d = [{\\'name\\': \\'Alice\\', \\'age\\': 1}]\\n        >>> spark.createDataFrame(d).collect()\\n        [Row(age=1, name=u\\'Alice\\')]\\n\\n        >>> rdd = sc.parallelize(l)\\n        >>> spark.createDataFrame(rdd).collect()\\n        [Row(_1=u\\'Alice\\', _2=1)]\\n        >>> df = spark.createDataFrame(rdd, [\\'name\\', \\'age\\'])\\n        >>> df.collect()\\n        [Row(name=u\\'Alice\\', age=1)]\\n\\n        >>> from pyspark.sql import Row\\n        >>> Person = Row(\\'name\\', \\'age\\')\\n        >>> person = rdd.map(lambda r: Person(*r))\\n        >>> df2 = spark.createDataFrame(person)\\n        >>> df2.collect()\\n        [Row(name=u\\'Alice\\', age=1)]\\n\\n        >>> from pyspark.sql.types import *\\n        >>> schema = StructType([\\n        ...    StructField(\"name\", StringType(), True),\\n        ...    StructField(\"age\", IntegerType(), True)])\\n        >>> df3 = spark.createDataFrame(rdd, schema)\\n        >>> df3.collect()\\n        [Row(name=u\\'Alice\\', age=1)]\\n\\n        >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\\n        [Row(name=u\\'Alice\\', age=1)]\\n        >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\\n        [Row(0=1, 1=2)]\\n\\n        >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\\n        [Row(a=u\\'Alice\\', b=1)]\\n        >>> rdd = rdd.map(lambda row: row[1])\\n        >>> spark.createDataFrame(rdd, \"int\").collect()\\n        [Row(value=1)]\\n        >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\\n        Traceback (most recent call last):\\n            ...\\n        Py4JJavaError: ...', 'output': 'def createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)'}\n",
      "{'input': 'Returns a :class:`DataFrame` representing the result of the given query.\\n\\n        :return: :class:`DataFrame`\\n\\n        >>> df.createOrReplaceTempView(\"table1\")\\n        >>> df2 = spark.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\\n        >>> df2.collect()\\n        [Row(f1=1, f2=u\\'row1\\'), Row(f1=2, f2=u\\'row2\\'), Row(f1=3, f2=u\\'row3\\')]', 'output': 'def sql(self, sqlQuery)'}\n",
      "{'input': 'Returns the specified table as a :class:`DataFrame`.\\n\\n        :return: :class:`DataFrame`\\n\\n        >>> df.createOrReplaceTempView(\"table1\")\\n        >>> df2 = spark.table(\"table1\")\\n        >>> sorted(df.collect()) == sorted(df2.collect())\\n        True', 'output': 'def table(self, tableName)'}\n",
      "{'input': 'Returns a :class:`StreamingQueryManager` that allows managing all the\\n        :class:`StreamingQuery` StreamingQueries active on `this` context.\\n\\n        .. note:: Evolving.\\n\\n        :return: :class:`StreamingQueryManager`', 'output': 'def streams(self)'}\n",
      "{'input': 'Stop the underlying :class:`SparkContext`.', 'output': 'def stop(self)'}\n",
      "{'input': 'Returns a :class:`SparkJobInfo` object, or None if the job info\\n        could not be found or was garbage collected.', 'output': 'def getJobInfo(self, jobId)'}\n",
      "{'input': 'Returns a :class:`SparkStageInfo` object, or None if the stage\\n        info could not be found or was garbage collected.', 'output': 'def getStageInfo(self, stageId)'}\n",
      "{'input': 'Restore an object of namedtuple', 'output': 'def _restore(name, fields, value)'}\n",
      "{'input': 'Make class generated by namedtuple picklable', 'output': 'def _hack_namedtuple(cls)'}\n",
      "{'input': 'Hack namedtuple() to make it picklable', 'output': 'def _hijack_namedtuple()'}\n",
      "{'input': 'Load a stream of un-ordered Arrow RecordBatches, where the last iteration yields\\n        a list of indices that can be used to put the RecordBatches in the correct order.', 'output': 'def load_stream(self, stream)'}\n",
      "{'input': 'Create an Arrow record batch from the given pandas.Series or list of Series,\\n        with optional type.\\n\\n        :param series: A single pandas.Series, list of Series, or list of (series, arrow_type)\\n        :return: Arrow RecordBatch', 'output': 'def _create_batch(self, series)'}\n",
      "{'input': 'Make ArrowRecordBatches from Pandas Series and serialize. Input is a single series or\\n        a list of series accompanied by an optional pyarrow type to coerce the data to.', 'output': 'def dump_stream(self, iterator, stream)'}\n",
      "{'input': 'Deserialize ArrowRecordBatches to an Arrow table and return as a list of pandas.Series.', 'output': 'def load_stream(self, stream)'}\n",
      "{'input': 'Override because Pandas UDFs require a START_ARROW_STREAM before the Arrow stream is sent.\\n        This should be sent after creating the first record batch so in case of an error, it can\\n        be sent back to the JVM before the Arrow stream starts.', 'output': 'def dump_stream(self, iterator, stream)'}\n",
      "{'input': 'Waits for the termination of `this` query, either by :func:`query.stop()` or by an\\n        exception. If the query has terminated with an exception, then the exception will be thrown.\\n        If `timeout` is set, it returns whether the query has terminated or not within the\\n        `timeout` seconds.\\n\\n        If the query has terminated, then all subsequent calls to this method will either return\\n        immediately (if the query was terminated by :func:`stop()`), or throw the exception\\n        immediately (if the query has terminated with exception).\\n\\n        throws :class:`StreamingQueryException`, if `this` query has terminated with an exception', 'output': 'def awaitTermination(self, timeout=None)'}\n",
      "{'input': 'Returns an array of the most recent [[StreamingQueryProgress]] updates for this query.\\n        The number of progress updates retained for each stream is configured by Spark session\\n        configuration `spark.sql.streaming.numRecentProgressUpdates`.', 'output': 'def recentProgress(self)'}\n",
      "{'input': 'Returns the most recent :class:`StreamingQueryProgress` update of this streaming query or\\n        None if there were no progress updates\\n        :return: a map', 'output': 'def lastProgress(self)'}\n",
      "{'input': ':return: the StreamingQueryException if the query was terminated by an exception, or None.', 'output': 'def exception(self)'}\n",
      "{'input': 'Wait until any of the queries on the associated SQLContext has terminated since the\\n        creation of the context, or since :func:`resetTerminated()` was called. If any query was\\n        terminated with an exception, then the exception will be thrown.\\n        If `timeout` is set, it returns whether the query has terminated or not within the\\n        `timeout` seconds.\\n\\n        If a query has terminated, then subsequent calls to :func:`awaitAnyTermination()` will\\n        either return immediately (if the query was terminated by :func:`query.stop()`),\\n        or throw the exception immediately (if the query was terminated with exception). Use\\n        :func:`resetTerminated()` to clear past terminations and wait for new terminations.\\n\\n        In the case where multiple queries have terminated since :func:`resetTermination()`\\n        was called, if any query has terminated with exception, then :func:`awaitAnyTermination()`\\n        will throw any of the exception. For correctly documenting exceptions across multiple\\n        queries, users need to stop all of them after any of them terminates with exception, and\\n        then check the `query.exception()` for each query.\\n\\n        throws :class:`StreamingQueryException`, if `this` query has terminated with an exception', 'output': 'def awaitAnyTermination(self, timeout=None)'}\n",
      "{'input': 'Loads a data stream from a data source and returns it as a :class`DataFrame`.\\n\\n        .. note:: Evolving.\\n\\n        :param path: optional string for file-system backed data sources.\\n        :param format: optional string for format of the data source. Default to \\'parquet\\'.\\n        :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema\\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\\n        :param options: all other string options\\n\\n        >>> json_sdf = spark.readStream.format(\"json\") \\\\\\\\\\n        ...     .schema(sdf_schema) \\\\\\\\\\n        ...     .load(tempfile.mkdtemp())\\n        >>> json_sdf.isStreaming\\n        True\\n        >>> json_sdf.schema == sdf_schema\\n        True', 'output': 'def load(self, path=None, format=None, schema=None, **options)'}\n",
      "{'input': \"Loads a JSON file stream and returns the results as a :class:`DataFrame`.\\n\\n        `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\\n        For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\\n\\n        If the ``schema`` parameter is not specified, this function goes\\n        through the input once to determine the input schema.\\n\\n        .. note:: Evolving.\\n\\n        :param path: string represents path to the JSON dataset,\\n                     or RDD of Strings storing JSON objects.\\n        :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\\n        :param primitivesAsString: infers all primitive values as a string type. If None is set,\\n                                   it uses the default value, ``false``.\\n        :param prefersDecimal: infers all floating-point values as a decimal type. If the values\\n                               do not fit in decimal, then it infers them as doubles. If None is\\n                               set, it uses the default value, ``false``.\\n        :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\\n                              it uses the default value, ``false``.\\n        :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\\n                                        it uses the default value, ``false``.\\n        :param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\\n                                        set, it uses the default value, ``true``.\\n        :param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\\n                                        set, it uses the default value, ``false``.\\n        :param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\\n                                                   using backslash quoting mechanism. If None is\\n                                                   set, it uses the default value, ``false``.\\n        :param mode: allows a mode for dealing with corrupt records during parsing. If None is\\n                     set, it uses the default value, ``PERMISSIVE``.\\n\\n                * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\\\\n                  into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\\\\n                  fields to ``null``. To keep corrupt records, an user can set a string type \\\\\\n                  field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\\\\n                  schema does not have the field, it drops corrupt records during parsing. \\\\\\n                  When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord`` \\\\\\n                  field in an output schema.\\n                *  ``DROPMALFORMED`` : ignores the whole corrupted records.\\n                *  ``FAILFAST`` : throws an exception when it meets corrupted records.\\n\\n        :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\\n                                          created by ``PERMISSIVE`` mode. This overrides\\n                                          ``spark.sql.columnNameOfCorruptRecord``. If None is set,\\n                                          it uses the value specified in\\n                                          ``spark.sql.columnNameOfCorruptRecord``.\\n        :param dateFormat: sets the string that indicates a date format. Custom date formats\\n                           follow the formats at ``java.time.format.DateTimeFormatter``. This\\n                           applies to date type. If None is set, it uses the\\n                           default value, ``yyyy-MM-dd``.\\n        :param timestampFormat: sets the string that indicates a timestamp format.\\n                                Custom date formats follow the formats at\\n                                ``java.time.format.DateTimeFormatter``.\\n                                This applies to timestamp type. If None is set, it uses the\\n                                default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\\n        :param multiLine: parse one record, which may span multiple lines, per file. If None is\\n                          set, it uses the default value, ``false``.\\n        :param allowUnquotedControlChars: allows JSON Strings to contain unquoted control\\n                                          characters (ASCII characters with value less than 32,\\n                                          including tab and line feed characters) or not.\\n        :param lineSep: defines the line separator that should be used for parsing. If None is\\n                        set, it covers all ``\\\\\\\\r``, ``\\\\\\\\r\\\\\\\\n`` and ``\\\\\\\\n``.\\n        :param locale: sets a locale as language tag in IETF BCP 47 format. If None is set,\\n                       it uses the default value, ``en-US``. For instance, ``locale`` is used while\\n                       parsing dates and timestamps.\\n        :param dropFieldIfAllNull: whether to ignore column of all null values or empty\\n                                   array/struct during schema inference. If None is set, it\\n                                   uses the default value, ``false``.\\n        :param encoding: allows to forcibly set one of standard basic or extended encoding for\\n                         the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\\n                         the encoding of input JSON will be detected automatically\\n                         when the multiLine option is set to ``true``.\\n\\n        >>> json_sdf = spark.readStream.json(tempfile.mkdtemp(), schema = sdf_schema)\\n        >>> json_sdf.isStreaming\\n        True\\n        >>> json_sdf.schema == sdf_schema\\n        True\", 'output': 'def json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None,\\n             allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None,\\n             allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None,\\n             mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None,\\n             multiLine=None,  allowUnquotedControlChars=None, lineSep=None, locale=None,\\n             dropFieldIfAllNull=None, encoding=None)'}\n",
      "{'input': 'Loads a ORC file stream, returning the result as a :class:`DataFrame`.\\n\\n        .. note:: Evolving.\\n\\n        >>> orc_sdf = spark.readStream.schema(sdf_schema).orc(tempfile.mkdtemp())\\n        >>> orc_sdf.isStreaming\\n        True\\n        >>> orc_sdf.schema == sdf_schema\\n        True', 'output': 'def orc(self, path)'}\n",
      "{'input': 'Loads a Parquet file stream, returning the result as a :class:`DataFrame`.\\n\\n        You can set the following Parquet-specific option(s) for reading Parquet files:\\n            * ``mergeSchema``: sets whether we should merge schemas collected from all \\\\\\n                Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``. \\\\\\n                The default value is specified in ``spark.sql.parquet.mergeSchema``.\\n\\n        .. note:: Evolving.\\n\\n        >>> parquet_sdf = spark.readStream.schema(sdf_schema).parquet(tempfile.mkdtemp())\\n        >>> parquet_sdf.isStreaming\\n        True\\n        >>> parquet_sdf.schema == sdf_schema\\n        True', 'output': 'def parquet(self, path)'}\n",
      "{'input': 'Loads a text file stream and returns a :class:`DataFrame` whose schema starts with a\\n        string column named \"value\", and followed by partitioned columns if there\\n        are any.\\n        The text files must be encoded as UTF-8.\\n\\n        By default, each line in the text file is a new row in the resulting DataFrame.\\n\\n        .. note:: Evolving.\\n\\n        :param paths: string, or list of strings, for input path(s).\\n        :param wholetext: if true, read each file from input path(s) as a single row.\\n        :param lineSep: defines the line separator that should be used for parsing. If None is\\n                        set, it covers all ``\\\\\\\\r``, ``\\\\\\\\r\\\\\\\\n`` and ``\\\\\\\\n``.\\n\\n        >>> text_sdf = spark.readStream.text(tempfile.mkdtemp())\\n        >>> text_sdf.isStreaming\\n        True\\n        >>> \"value\" in str(text_sdf.schema)\\n        True', 'output': 'def text(self, path, wholetext=False, lineSep=None)'}\n",
      "{'input': 'r\"\"\"Loads a CSV file stream and returns the result as a :class:`DataFrame`.\\n\\n        This function will go through the input once to determine the input schema if\\n        ``inferSchema`` is enabled. To avoid going through the entire data once, disable\\n        ``inferSchema`` option or specify the schema explicitly using ``schema``.\\n\\n        .. note:: Evolving.\\n\\n        :param path: string, or list of strings, for input path(s).\\n        :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\\n                       or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\\n        :param sep: sets a single character as a separator for each field and value.\\n                    If None is set, it uses the default value, ``,``.\\n        :param encoding: decodes the CSV files by the given encoding type. If None is set,\\n                         it uses the default value, ``UTF-8``.\\n        :param quote: sets a single character used for escaping quoted values where the\\n                      separator can be part of the value. If None is set, it uses the default\\n                      value, ``\"``. If you would like to turn off quotations, you need to set an\\n                      empty string.\\n        :param escape: sets a single character used for escaping quotes inside an already\\n                       quoted value. If None is set, it uses the default value, ``\\\\``.\\n        :param comment: sets a single character used for skipping lines beginning with this\\n                        character. By default (None), it is disabled.\\n        :param header: uses the first line as names of columns. If None is set, it uses the\\n                       default value, ``false``.\\n        :param inferSchema: infers the input schema automatically from data. It requires one extra\\n                       pass over the data. If None is set, it uses the default value, ``false``.\\n        :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\\n                              forcibly applied to datasource files, and headers in CSV files will be\\n                              ignored. If the option is set to ``false``, the schema will be\\n                              validated against all headers in CSV files or the first header in RDD\\n                              if the ``header`` option is set to ``true``. Field names in the schema\\n                              and column names in CSV headers are checked by their positions\\n                              taking into account ``spark.sql.caseSensitive``. If None is set,\\n                              ``true`` is used by default. Though the default value is ``true``,\\n                              it is recommended to disable the ``enforceSchema`` option\\n                              to avoid incorrect results.\\n        :param ignoreLeadingWhiteSpace: a flag indicating whether or not leading whitespaces from\\n                                        values being read should be skipped. If None is set, it\\n                                        uses the default value, ``false``.\\n        :param ignoreTrailingWhiteSpace: a flag indicating whether or not trailing whitespaces from\\n                                         values being read should be skipped. If None is set, it\\n                                         uses the default value, ``false``.\\n        :param nullValue: sets the string representation of a null value. If None is set, it uses\\n                          the default value, empty string. Since 2.0.1, this ``nullValue`` param\\n                          applies to all supported types including the string type.\\n        :param nanValue: sets the string representation of a non-number value. If None is set, it\\n                         uses the default value, ``NaN``.\\n        :param positiveInf: sets the string representation of a positive infinity value. If None\\n                            is set, it uses the default value, ``Inf``.\\n        :param negativeInf: sets the string representation of a negative infinity value. If None\\n                            is set, it uses the default value, ``Inf``.\\n        :param dateFormat: sets the string that indicates a date format. Custom date formats\\n                           follow the formats at ``java.time.format.DateTimeFormatter``. This\\n                           applies to date type. If None is set, it uses the\\n                           default value, ``yyyy-MM-dd``.\\n        :param timestampFormat: sets the string that indicates a timestamp format.\\n                                Custom date formats follow the formats at\\n                                ``java.time.format.DateTimeFormatter``.\\n                                This applies to timestamp type. If None is set, it uses the\\n                                default value, ``yyyy-MM-dd\\'T\\'HH:mm:ss.SSSXXX``.\\n        :param maxColumns: defines a hard limit of how many columns a record can have. If None is\\n                           set, it uses the default value, ``20480``.\\n        :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\\n                                  value being read. If None is set, it uses the default value,\\n                                  ``-1`` meaning unlimited length.\\n        :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\\n                                            If specified, it is ignored.\\n        :param mode: allows a mode for dealing with corrupt records during parsing. If None is\\n                     set, it uses the default value, ``PERMISSIVE``.\\n\\n                * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\\\\n                  into a field configured by ``columnNameOfCorruptRecord``, and sets malformed \\\\\\n                  fields to ``null``. To keep corrupt records, an user can set a string type \\\\\\n                  field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\\\\n                  schema does not have the field, it drops corrupt records during parsing. \\\\\\n                  A record with less/more tokens than schema is not a corrupted record to CSV. \\\\\\n                  When it meets a record having fewer tokens than the length of the schema, \\\\\\n                  sets ``null`` to extra fields. When the record has more tokens than the \\\\\\n                  length of the schema, it drops extra tokens.\\n                * ``DROPMALFORMED`` : ignores the whole corrupted records.\\n                * ``FAILFAST`` : throws an exception when it meets corrupted records.\\n\\n        :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\\n                                          created by ``PERMISSIVE`` mode. This overrides\\n                                          ``spark.sql.columnNameOfCorruptRecord``. If None is set,\\n                                          it uses the value specified in\\n                                          ``spark.sql.columnNameOfCorruptRecord``.\\n        :param multiLine: parse one record, which may span multiple lines. If None is\\n                          set, it uses the default value, ``false``.\\n        :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\\n                                          the quote character. If None is set, the default value is\\n                                          escape character when escape and quote characters are\\n                                          different, ``\\\\0`` otherwise..\\n        :param emptyValue: sets the string representation of an empty value. If None is set, it uses\\n                           the default value, empty string.\\n        :param locale: sets a locale as language tag in IETF BCP 47 format. If None is set,\\n                       it uses the default value, ``en-US``. For instance, ``locale`` is used while\\n                       parsing dates and timestamps.\\n        :param lineSep: defines the line separator that should be used for parsing. If None is\\n                        set, it covers all ``\\\\\\\\r``, ``\\\\\\\\r\\\\\\\\n`` and ``\\\\\\\\n``.\\n                        Maximum length is 1 character.\\n\\n        >>> csv_sdf = spark.readStream.csv(tempfile.mkdtemp(), schema = sdf_schema)\\n        >>> csv_sdf.isStreaming\\n        True\\n        >>> csv_sdf.schema == sdf_schema\\n        True', 'output': 'def csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None,\\n            comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None,\\n            ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None,\\n            negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None,\\n            maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None,\\n            columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None,\\n            enforceSchema=None, emptyValue=None, locale=None, lineSep=None)'}\n",
      "{'input': \"Specifies how data of a streaming DataFrame/Dataset is written to a streaming sink.\\n\\n        Options include:\\n\\n        * `append`:Only the new rows in the streaming DataFrame/Dataset will be written to\\n           the sink\\n        * `complete`:All the rows in the streaming DataFrame/Dataset will be written to the sink\\n           every time these is some updates\\n        * `update`:only the rows that were updated in the streaming DataFrame/Dataset will be\\n           written to the sink every time there are some updates. If the query doesn't contain\\n           aggregations, it will be equivalent to `append` mode.\\n\\n       .. note:: Evolving.\\n\\n        >>> writer = sdf.writeStream.outputMode('append')\", 'output': 'def outputMode(self, outputMode)'}\n",
      "{'input': \"Specifies the name of the :class:`StreamingQuery` that can be started with\\n        :func:`start`. This name must be unique among all the currently active queries\\n        in the associated SparkSession.\\n\\n        .. note:: Evolving.\\n\\n        :param queryName: unique name for the query\\n\\n        >>> writer = sdf.writeStream.queryName('streaming_query')\", 'output': 'def queryName(self, queryName)'}\n",
      "{'input': \"Set the trigger for the stream query. If this is not set it will run the query as fast\\n        as possible, which is equivalent to setting the trigger to ``processingTime='0 seconds'``.\\n\\n        .. note:: Evolving.\\n\\n        :param processingTime: a processing time interval as a string, e.g. '5 seconds', '1 minute'.\\n                               Set a trigger that runs a query periodically based on the processing\\n                               time. Only one trigger can be set.\\n        :param once: if set to True, set a trigger that processes only one batch of data in a\\n                     streaming query then terminates the query. Only one trigger can be set.\\n\\n        >>> # trigger the query for execution every 5 seconds\\n        >>> writer = sdf.writeStream.trigger(processingTime='5 seconds')\\n        >>> # trigger the query for just once batch of data\\n        >>> writer = sdf.writeStream.trigger(once=True)\\n        >>> # trigger the query for execution every 5 seconds\\n        >>> writer = sdf.writeStream.trigger(continuous='5 seconds')\", 'output': 'def trigger(self, processingTime=None, once=None, continuous=None)'}\n",
      "{'input': 'Sets the output of the streaming query to be processed using the provided writer ``f``.\\n        This is often used to write the output of a streaming query to arbitrary storage systems.\\n        The processing logic can be specified in two ways.\\n\\n        #. A **function** that takes a row as input.\\n            This is a simple way to express your processing logic. Note that this does\\n            not allow you to deduplicate generated data when failures cause reprocessing of\\n            some input data. That would require you to specify the processing logic in the next\\n            way.\\n\\n        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\\n            The object can have the following methods.\\n\\n            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\\n                (for example, open a connection, start a transaction, etc). Additionally, you can\\n                use the `partition_id` and `epoch_id` to deduplicate regenerated data\\n                (discussed later).\\n\\n            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\\n\\n            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\\n                close connection, commit transaction, etc.) after all rows have been processed.\\n\\n            The object will be used by Spark in the following way.\\n\\n            * A single copy of this object is responsible of all the data generated by a\\n                single task in a query. In other words, one instance is responsible for\\n                processing one partition of the data generated in a distributed manner.\\n\\n            * This object must be serializable because each task will get a fresh\\n                serialized-deserialized copy of the provided object. Hence, it is strongly\\n                recommended that any initialization for writing data (e.g. opening a\\n                connection or starting a transaction) is done after the `open(...)`\\n                method has been called, which signifies that the task is ready to generate data.\\n\\n            * The lifecycle of the methods are as follows.\\n\\n                For each partition with ``partition_id``:\\n\\n                ... For each batch/epoch of streaming data with ``epoch_id``:\\n\\n                ....... Method ``open(partitionId, epochId)`` is called.\\n\\n                ....... If ``open(...)`` returns true, for each row in the partition and\\n                        batch/epoch, method ``process(row)`` is called.\\n\\n                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\\n                        processing rows.\\n\\n            Important points to note:\\n\\n            * The `partitionId` and `epochId` can be used to deduplicate generated data when\\n                failures cause reprocessing of some input data. This depends on the execution\\n                mode of the query. If the streaming query is being executed in the micro-batch\\n                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\\n                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\\n                to deduplicate and/or transactionally commit data and achieve exactly-once\\n                guarantees. However, if the streaming query is being executed in the continuous\\n                mode, then this guarantee does not hold and therefore should not be used for\\n                deduplication.\\n\\n            * The ``close()`` method (if exists) will be called if `open()` method exists and\\n                returns successfully (irrespective of the return value), except if the Python\\n                crashes in the middle.\\n\\n        .. note:: Evolving.\\n\\n        >>> # Print every row using a function\\n        >>> def print_row(row):\\n        ...     print(row)\\n        ...\\n        >>> writer = sdf.writeStream.foreach(print_row)\\n        >>> # Print every row using a object with process() method\\n        >>> class RowPrinter:\\n        ...     def open(self, partition_id, epoch_id):\\n        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\\n        ...         return True\\n        ...     def process(self, row):\\n        ...         print(row)\\n        ...     def close(self, error):\\n        ...         print(\"Closed with error: %s\" % str(error))\\n        ...\\n        >>> writer = sdf.writeStream.foreach(RowPrinter())', 'output': 'def foreach(self, f)'}\n",
      "{'input': 'Sets the output of the streaming query to be processed using the provided\\n        function. This is supported only the in the micro-batch execution modes (that is, when the\\n        trigger is not continuous). In every micro-batch, the provided function will be called in\\n        every micro-batch with (i) the output rows as a DataFrame and (ii) the batch identifier.\\n        The batchId can be used deduplicate and transactionally write the output\\n        (that is, the provided Dataset) to external systems. The output DataFrame is guaranteed\\n        to exactly same for the same batchId (assuming all operations are deterministic in the\\n        query).\\n\\n        .. note:: Evolving.\\n\\n        >>> def func(batch_df, batch_id):\\n        ...     batch_df.collect()\\n        ...\\n        >>> writer = sdf.writeStream.foreach(func)', 'output': 'def foreachBatch(self, func)'}\n",
      "{'input': 'Streams the contents of the :class:`DataFrame` to a data source.\\n\\n        The data source is specified by the ``format`` and a set of ``options``.\\n        If ``format`` is not specified, the default data source configured by\\n        ``spark.sql.sources.default`` will be used.\\n\\n        .. note:: Evolving.\\n\\n        :param path: the path in a Hadoop supported file system\\n        :param format: the format used to save\\n        :param outputMode: specifies how data of a streaming DataFrame/Dataset is written to a\\n                           streaming sink.\\n\\n            * `append`:Only the new rows in the streaming DataFrame/Dataset will be written to the\\n              sink\\n            * `complete`:All the rows in the streaming DataFrame/Dataset will be written to the sink\\n               every time these is some updates\\n            * `update`:only the rows that were updated in the streaming DataFrame/Dataset will be\\n              written to the sink every time there are some updates. If the query doesn\\'t contain\\n              aggregations, it will be equivalent to `append` mode.\\n        :param partitionBy: names of partitioning columns\\n        :param queryName: unique name for the query\\n        :param options: All other string options. You may want to provide a `checkpointLocation`\\n                        for most streams, however it is not required for a `memory` stream.\\n\\n        >>> sq = sdf.writeStream.format(\\'memory\\').queryName(\\'this_query\\').start()\\n        >>> sq.isActive\\n        True\\n        >>> sq.name\\n        u\\'this_query\\'\\n        >>> sq.stop()\\n        >>> sq.isActive\\n        False\\n        >>> sq = sdf.writeStream.trigger(processingTime=\\'5 seconds\\').start(\\n        ...     queryName=\\'that_query\\', outputMode=\"append\", format=\\'memory\\')\\n        >>> sq.name\\n        u\\'that_query\\'\\n        >>> sq.isActive\\n        True\\n        >>> sq.stop()', 'output': 'def start(self, path=None, format=None, outputMode=None, partitionBy=None, queryName=None,\\n              **options)'}\n",
      "{'input': \"Get the Python compiler to emit LOAD_FAST(arg); STORE_DEREF\\n\\n    Notes\\n    -----\\n    In Python 3, we could use an easier function:\\n\\n    .. code-block:: python\\n\\n       def f():\\n           cell = None\\n\\n           def _stub(value):\\n               nonlocal cell\\n               cell = value\\n\\n           return _stub\\n\\n        _cell_set_template_code = f().__code__\\n\\n    This function is _only_ a LOAD_FAST(arg); STORE_DEREF, but that is\\n    invalid syntax on Python 2. If we use this function we also don't need\\n    to do the weird freevars/cellvars swap below\", 'output': 'def _make_cell_set_template_code()'}\n",
      "{'input': 'Return whether *func* is a Tornado coroutine function.\\n    Running coroutines are not supported.', 'output': 'def is_tornado_coroutine(func)'}\n",
      "{'input': 'Serialize obj as bytes streamed into file\\n\\n    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to\\n    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication speed\\n    between processes running the same Python version.\\n\\n    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure\\n    compatibility with older versions of Python.', 'output': 'def dump(obj, file, protocol=None)'}\n",
      "{'input': 'Serialize obj as a string of bytes allocated in memory\\n\\n    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to\\n    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication speed\\n    between processes running the same Python version.\\n\\n    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure\\n    compatibility with older versions of Python.', 'output': 'def dumps(obj, protocol=None)'}\n",
      "{'input': 'Fills in the rest of function data into the skeleton function object\\n\\n    The skeleton itself is create by _make_skel_func().', 'output': 'def _fill_function(*args)'}\n",
      "{'input': 'Put attributes from `class_dict` back on `skeleton_class`.\\n\\n    See CloudPickler.save_dynamic_class for more info.', 'output': 'def _rehydrate_skeleton_class(skeleton_class, class_dict)'}\n",
      "{'input': 'Return True if the module is special module that cannot be imported by its\\n    name.', 'output': 'def _is_dynamic(module)'}\n",
      "{'input': 'Save a code object', 'output': 'def save_codeobject(self, obj)'}\n",
      "{'input': 'Registered with the dispatch to handle all function types.\\n\\n        Determines what kind of function obj is (e.g. lambda, defined at\\n        interactive prompt, etc) and handles the pickling appropriately.', 'output': 'def save_function(self, obj, name=None)'}\n",
      "{'input': \"Save a class that can't be stored as module global.\\n\\n        This method is used to serialize classes that are defined inside\\n        functions, or that otherwise can't be serialized as attribute lookups\\n        from global modules.\", 'output': 'def save_dynamic_class(self, obj)'}\n",
      "{'input': \"Pickles an actual func object.\\n\\n        A func comprises: code, globals, defaults, closure, and dict.  We\\n        extract and save these, injecting reducing functions at certain points\\n        to recreate the func object.  Keep in mind that some of these pieces\\n        can contain a ref to the func itself.  Thus, a naive save on these\\n        pieces could trigger an infinite loop of save's.  To get around that,\\n        we first create a skeleton func object using just the code (this is\\n        safe, since this won't contain a ref to the func), and memoize it as\\n        soon as it's created.  The other stuff can then be filled in later.\", 'output': 'def save_function_tuple(self, func)'}\n",
      "{'input': 'Save a \"global\".\\n\\n        The name of this method is somewhat misleading: all types get\\n        dispatched here.', 'output': 'def save_global(self, obj, name=None, pack=struct.pack)'}\n",
      "{'input': 'Inner logic to save instance. Based off pickle.save_inst', 'output': 'def save_inst(self, obj)'}\n",
      "{'input': 'itemgetter serializer (needed for namedtuple support)', 'output': 'def save_itemgetter(self, obj)'}\n",
      "{'input': 'attrgetter serializer', 'output': 'def save_attrgetter(self, obj)'}\n",
      "{'input': 'Copy the current param to a new parent, must be a dummy param.', 'output': 'def _copy_new_parent(self, parent)'}\n",
      "{'input': 'Convert a value to a list, if possible.', 'output': 'def toList(value)'}\n",
      "{'input': 'Convert a value to list of floats, if possible.', 'output': 'def toListFloat(value)'}\n",
      "{'input': 'Convert a value to list of ints, if possible.', 'output': 'def toListInt(value)'}\n",
      "{'input': 'Convert a value to list of strings, if possible.', 'output': 'def toListString(value)'}\n",
      "{'input': 'Convert a value to a MLlib Vector, if possible.', 'output': 'def toVector(value)'}\n",
      "{'input': 'Convert a value to a string, if possible.', 'output': 'def toString(value)'}\n",
      "{'input': 'Copy all params defined on the class to current object.', 'output': 'def _copy_params(self)'}\n",
      "{'input': 'Returns all params ordered by name. The default implementation\\n        uses :py:func:`dir` to get all attributes of type\\n        :py:class:`Param`.', 'output': 'def params(self)'}\n",
      "{'input': 'Explains a single param and returns its name, doc, and optional\\n        default value and user-supplied value in a string.', 'output': 'def explainParam(self, param)'}\n",
      "{'input': 'Gets a param by its name.', 'output': 'def getParam(self, paramName)'}\n",
      "{'input': 'Checks whether a param is explicitly set by user.', 'output': 'def isSet(self, param)'}\n",
      "{'input': 'Checks whether a param has a default value.', 'output': 'def hasDefault(self, param)'}\n",
      "{'input': 'Tests whether this instance contains a param with a given\\n        (string) name.', 'output': 'def hasParam(self, paramName)'}\n",
      "{'input': 'Gets the value of a param in the user-supplied param map or its\\n        default value. Raises an error if neither is set.', 'output': 'def getOrDefault(self, param)'}\n",
      "{'input': 'Extracts the embedded default param values and user-supplied\\n        values, and then merges them with extra values from input into\\n        a flat param map, where the latter value is used if there exist\\n        conflicts, i.e., with ordering: default param values <\\n        user-supplied values < extra.\\n\\n        :param extra: extra param values\\n        :return: merged param map', 'output': 'def extractParamMap(self, extra=None)'}\n",
      "{'input': 'Creates a copy of this instance with the same uid and some\\n        extra params. The default implementation creates a\\n        shallow copy using :py:func:`copy.copy`, and then copies the\\n        embedded and extra parameters over and returns the copy.\\n        Subclasses should override this method if the default approach\\n        is not sufficient.\\n\\n        :param extra: Extra parameters to copy to the new instance\\n        :return: Copy of this instance', 'output': 'def copy(self, extra=None)'}\n",
      "{'input': 'Sets a parameter in the embedded param map.', 'output': 'def set(self, param, value)'}\n",
      "{'input': 'Validates that the input param belongs to this Params instance.', 'output': 'def _shouldOwn(self, param)'}\n",
      "{'input': 'Resolves a param and validates the ownership.\\n\\n        :param param: param name or the param instance, which must\\n                      belong to this Params instance\\n        :return: resolved param instance', 'output': 'def _resolveParam(self, param)'}\n",
      "{'input': 'Sets user-supplied params.', 'output': 'def _set(self, **kwargs)'}\n",
      "{'input': 'Sets default params.', 'output': 'def _setDefault(self, **kwargs)'}\n",
      "{'input': 'Copies param values from this instance to another instance for\\n        params shared by them.\\n\\n        :param to: the target instance\\n        :param extra: extra params to be copied\\n        :return: the target instance with param values copied', 'output': 'def _copyValues(self, to, extra=None)'}\n",
      "{'input': 'Changes the uid of this instance. This updates both\\n        the stored uid and the parent uid of params and param maps.\\n        This is used by persistence (loading).\\n        :param newUid: new uid to use, which is converted to unicode\\n        :return: same instance, but with the uid and Param.parent values\\n                 updated, including within param maps', 'output': 'def _resetUid(self, newUid)'}\n",
      "{'input': 'Return an JavaRDD of Object by unpickling\\n\\n    It will convert each Python object into Java object by Pyrolite, whenever the\\n    RDD is serialized in batch or not.', 'output': 'def _to_java_object_rdd(rdd)'}\n",
      "{'input': 'Return the broadcasted value', 'output': 'def value(self)'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 680/412178 [00:00<02:12, 3109.30 examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Apply preprocessing to the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_data))\n\u001b[0;32m      4\u001b[0m train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# Remove invalid samples\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sarbu\\miniconda3\\envs\\machine_learning_py38\\lib\\site-packages\\datasets\\arrow_dataset.py:602\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sarbu\\miniconda3\\envs\\machine_learning_py38\\lib\\site-packages\\datasets\\arrow_dataset.py:567\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    563\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    565\u001b[0m }\n\u001b[0;32m    566\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    569\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sarbu\\miniconda3\\envs\\machine_learning_py38\\lib\\site-packages\\datasets\\arrow_dataset.py:3156\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3151\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3152\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3153\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3154\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3155\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3156\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3157\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3158\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\sarbu\\miniconda3\\envs\\machine_learning_py38\\lib\\site-packages\\datasets\\arrow_dataset.py:3516\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batched:\n\u001b[0;32m   3515\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m-> 3516\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[0;32m   3517\u001b[0m         example \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(example, i, offset\u001b[38;5;241m=\u001b[39moffset)\n\u001b[0;32m   3518\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m update_data:\n",
      "File \u001b[1;32mc:\\Users\\sarbu\\miniconda3\\envs\\machine_learning_py38\\lib\\site-packages\\datasets\\arrow_dataset.py:2437\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2435\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2436\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mARROW_READER_BATCH_SIZE_IN_DATASET_ITER\n\u001b[1;32m-> 2437\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pa_subtable \u001b[38;5;129;01min\u001b[39;00m table_iter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m   2438\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[0;32m   2439\u001b[0m         pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sarbu\\miniconda3\\envs\\machine_learning_py38\\lib\\site-packages\\datasets\\table.py:2397\u001b[0m, in \u001b[0;36mtable_iter\u001b[1;34m(table, batch_size, drop_last_batch)\u001b[0m\n\u001b[0;32m   2395\u001b[0m chunks_buffer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   2396\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mto_reader(max_chunksize\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[1;32m-> 2397\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2398\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   2399\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m chunks_buffer_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m<\u001b[39m batch_size:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply preprocessing to the dataset\n",
    "train_data = dataset['train'].map(preprocess_function, batched=False)\n",
    "print(len(train_data))\n",
    "train_data = train_data.filter(lambda x: len(x[\"input\"])==0 or len(x[\"output\"])==0) # Remove invalid samples\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from preprocessed dataset: \n"
     ]
    }
   ],
   "source": [
    "# Print one example from the preprocessed dataset\n",
    "print(\"Example from preprocessed dataset:\", train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the T5 tokenizer and model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization function for the dataset\n",
    "def tokenize_function(example):\n",
    "    inputs = tokenizer(example['input'], truncation=True, padding='max_length', max_length=128)\n",
    "    labels = tokenizer(example['output'], truncation=True, padding='max_length', max_length=128)\n",
    "    return {\n",
    "        \"input_ids\": inputs['input_ids'],\n",
    "        \"attention_mask\": inputs['attention_mask'],\n",
    "        \"labels\": labels['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenization to the train data\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./finetuned_codet5\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500\n",
    ")\n",
    "\n",
    "# Create a Trainer object for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./finetuned_codet5\")\n",
    "tokenizer.save_pretrained(\"./finetuned_codet5\")\n",
    "print(\"Fine-tuning complete! Model saved at ./finetuned_codet5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate function headers with syntax restrictions\n",
    "def generate_function_header(description):\n",
    "    # Tokenize the input description\n",
    "    input_ids = tokenizer.encode(description, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "\n",
    "    # Generate output with a maximum length and stopping criteria\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,  # Limit to a typical function header length\n",
    "        num_beams=5,    # Beam search for better results\n",
    "        early_stopping=True,\n",
    "        eos_token_id=tokenizer.encode(\")\")[0]  # Stop generation after the closing parenthesis (end of function header)\n",
    "    )\n",
    "\n",
    "    # Decode the output tokens\n",
    "    generated_header = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure that the generated output ends with ')'\n",
    "    if not generated_header.strip().endswith(\")\"):\n",
    "        generated_header += \")\"\n",
    "\n",
    "    # Add ':' and a newline after the function header\n",
    "    generated_header += \":\\n\"\n",
    "\n",
    "    # Validate the generated function header\n",
    "    try:\n",
    "        ast.parse(generated_header)\n",
    "        valid = True\n",
    "    except SyntaxError:\n",
    "        valid = False\n",
    "\n",
    "    return generated_header, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "description = \"Calculate the factorial of a number.\"\n",
    "header, is_valid = generate_function_header(description)\n",
    "print(\"Generated Function Header:\", header)\n",
    "print(\"Is the header valid?:\", is_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Fine-tuning complete! Model saved at ./finetuned_codet5\")\n",
    "\n",
    "# Define a function to generate function headers with syntax restrictions\n",
    "def generate_function_header(description):\n",
    "    # Tokenize the input description\n",
    "    input_ids = tokenizer.encode(description, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "\n",
    "    # Generate output with a maximum length and stopping criteria\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=50,  # Limit to a typical function header length\n",
    "        num_beams=5,    # Beam search for better results\n",
    "        early_stopping=True,\n",
    "        eos_token_id=tokenizer.encode(\")\")[0]  # Stop generation after the closing parenthesis (end of function header)\n",
    "    )\n",
    "\n",
    "    # Decode the output tokens\n",
    "    generated_header = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure that the generated output ends with ')'\n",
    "    if not generated_header.strip().endswith(\")\"):\n",
    "        generated_header += \")\"\n",
    "\n",
    "    # Add ':' and a newline after the function header\n",
    "    generated_header += \":\\n\"\n",
    "\n",
    "    # Validate the generated function header\n",
    "    try:\n",
    "        ast.parse(generated_header)\n",
    "        valid = True\n",
    "    except SyntaxError:\n",
    "        valid = False\n",
    "\n",
    "    return generated_header, valid\n",
    "\n",
    "# Example usage\n",
    "description = \"Calculate the factorial of a number.\"\n",
    "header, is_valid = generate_function_header(description)\n",
    "print(\"Generated Function Header:\", header)\n",
    "print(\"Is the header valid?:\", is_valid)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
